# Chapter 32. Messaging platform

---

# Introduction

Messaging platforms are among the most demanding distributed systems to design and operate. I've spent years building and scaling messaging infrastructure at Google, and I can tell you: the fundamental challenge isn't sending a message from A to Bâ€”any undergraduate can build that. The challenge is doing it reliably for billions of users, with sub-second latency, perfect ordering, exactly-once delivery semantics, and graceful degradation when (not if) things fail.

This chapter covers messaging platform design as Staff Engineers practice it: with deep understanding of the consistency-availability trade-offs unique to messaging, awareness of the ordering problems that seem simple but aren't, and judgment about when to sacrifice guarantees for user experience.

**The Staff Engineer's First Law of Messaging**: Users care about three things in order: (1) Did my message get delivered? (2) Did it arrive quickly? (3) Is the conversation in the right order? Everything elseâ€”read receipts, typing indicators, reactionsâ€”is polish. Get the fundamentals wrong, and no amount of features saves you.

---

## Quick Visual: Messaging Platform at a Glance

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  MESSAGING PLATFORM: THE STAFF ENGINEER VIEW                â”‚
â”‚                                                                             â”‚
â”‚   WRONG Framing: "Send messages between users"                              â”‚
â”‚   RIGHT Framing: "Deliver messages reliably with ordering guarantees,       â”‚
â”‚                   sub-second latency, and graceful offline handling"        â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Before designing, understand:                                      â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚  1. 1:1 messaging or group messaging or both?                       â”‚   â”‚
â”‚   â”‚  2. What ordering guarantees? (Per-sender? Per-conversation?)       â”‚   â”‚
â”‚   â”‚  3. How long can users be offline? (Minutes? Days? Months?)         â”‚   â”‚
â”‚   â”‚  4. What's the message size distribution? (Text? Media? Files?)     â”‚   â”‚
â”‚   â”‚  5. Synchronous delivery (online) or async (offline)?               â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚   THE UNCOMFORTABLE TRUTH:                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Perfect ordering + perfect availability + low latency is           â”‚   â”‚
â”‚   â”‚  impossible across regions. You must choose which to sacrifice.     â”‚   â”‚
â”‚   â”‚  Most messaging apps sacrifice global ordering for availability     â”‚   â”‚
â”‚   â”‚  and use sender-ordering + local clocks for "good enough" UX.       â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## L5 vs L6 Messaging Platform Decisions

| Scenario | L5 Approach | L6 Approach |
|----------|-------------|-------------|
| **Message ordering** | "Use a single message queue for total ordering" | "Total ordering doesn't scale. Use per-conversation ordering with Lamport timestamps. Users only see their own conversation anyway." |
| **Delivery guarantees** | "Use exactly-once delivery with 2PC" | "Exactly-once is expensive. Use at-least-once with idempotent receivers. Duplicate detection at client is cheaper than prevention." |
| **Offline message storage** | "Store all messages forever in hot storage" | "Hot storage for 30 days, warm for 1 year, cold archive beyond. 99% of reads are last 24 hours." |
| **Group messaging** | "Fan-out on write for fast reads" | "Fan-out on read for small groups, fan-out on write for large groups. Threshold at ~50 members." |
| **Presence (online status)** | "Real-time presence with WebSocket heartbeats" | "Presence is eventually consistent. 30-second staleness is acceptable. Don't make presence a scaling bottleneck." |

**Key Difference**: L6 engineers recognize that messaging systems have fundamentally different requirements than traditional request-response systems. They design for the unique challenges of long-lived connections, offline users, and conversation-local consistency.

---

# Part 1: Foundations â€” What a Messaging Platform Is and Why It Exists

## What Is a Messaging Platform?

A messaging platform enables asynchronous communication between users through discrete messages. Unlike email (store-and-forward, minutes-to-hours delivery) or phone calls (synchronous, real-time), messaging platforms occupy a middle ground: near-real-time when users are online, reliably stored when they're offline.

### The Simplest Mental Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  MESSAGING: THE POSTAL SYSTEM ANALOGY                       â”‚
â”‚                                                                             â”‚
â”‚   Think of messaging as a modern postal system:                             â”‚
â”‚                                                                             â”‚
â”‚   SENDER writes a letter (message)                                          â”‚
â”‚   â†“                                                                         â”‚
â”‚   POST OFFICE (message service) receives it                                 â”‚
â”‚   â†“                                                                         â”‚
â”‚   If RECIPIENT is home (online): Deliver immediately                        â”‚
â”‚   If RECIPIENT is away (offline): Hold at post office, deliver on return   â”‚
â”‚   â†“                                                                         â”‚
â”‚   RECIPIENT receives letter, sends acknowledgment (read receipt)            â”‚
â”‚                                                                             â”‚
â”‚   COMPLICATIONS AT SCALE:                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  What if there are millions of post offices (distributed system)?  â”‚   â”‚
â”‚   â”‚  â†’ Need coordination for which office holds which letters          â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚  What if recipient has multiple homes (multiple devices)?          â”‚   â”‚
â”‚   â”‚  â†’ Need to track delivery to ALL devices, not just one             â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚  What if letters arrive out of order?                               â”‚   â”‚
â”‚   â”‚  â†’ Need sequence numbers to reorder at recipient                   â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚  What if sender sends to 1000 recipients (group chat)?             â”‚   â”‚
â”‚   â”‚  â†’ Need efficient fan-out, not 1000 individual deliveries          â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Why Messaging Platforms Exist

### 1. Asynchronous Communication at Human Speed

Email is too slow for conversations. Phone calls require both parties simultaneously. Messaging bridges the gap:

```
Scenario: Coordinating dinner plans

EMAIL:
T+0min:   Alice sends "Dinner at 7?"
T+5min:   Bob checks email, replies "Sure, where?"
T+12min:  Alice checks email, replies "That Italian place"
T+20min:  Bob confirms
TOTAL: 20+ minutes for simple coordination

PHONE:
T+0min:   Alice calls Bob
          Bob doesn't answer (in meeting)
T+30min:  Bob calls back
          Alice doesn't answer (now in meeting)
T+60min:  Finally connect, 2-minute conversation
TOTAL: 60+ minutes wall clock time

MESSAGING:
T+0min:   Alice sends "Dinner at 7?"
T+0min:   Bob sees notification, replies "Sure, where?"
T+1min:   Alice replies "That Italian place"
T+1min:   Bob sends ğŸ‘
TOTAL: 1-2 minutes, no synchronization required
```

### 2. Persistent Conversation History

Unlike phone calls, messages create a record:

```
Benefits:
â€¢ Search old conversations for decisions/agreements
â€¢ Onboard new group members with context
â€¢ Reference shared links, files, addresses
â€¢ Legal/compliance record-keeping
â€¢ Memory augmentation ("What did we decide last week?")
```

### 3. Multi-Device, Multi-Platform Access

Modern messaging works across all devices simultaneously:

```
User's devices:
â”œâ”€â”€ Phone (primary, always connected)
â”œâ”€â”€ Tablet (occasional use)
â”œâ”€â”€ Laptop (work hours)
â””â”€â”€ Desktop (home)

Message arrives:
â†’ Delivered to ALL devices
â†’ Read status synced across devices
â†’ User can respond from any device
â†’ Conversation continues seamlessly
```

### 4. Rich Communication Beyond Text

Modern platforms support:

```
Content types:
â”œâ”€â”€ Text (with emoji, formatting)
â”œâ”€â”€ Images (with compression, thumbnails)
â”œâ”€â”€ Videos (with streaming, transcoding)
â”œâ”€â”€ Voice messages (with waveform preview)
â”œâ”€â”€ Files/documents
â”œâ”€â”€ Location sharing
â”œâ”€â”€ Contact cards
â”œâ”€â”€ Polls and interactive elements
â””â”€â”€ Reactions and replies
```

## What Happens If a Messaging Platform Does NOT Exist (or Fails)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  MESSAGING PLATFORM FAILURES                                â”‚
â”‚                                                                             â”‚
â”‚   FAILURE MODE 1: MESSAGE LOSS                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  User sends critical message â†’ Message disappears â†’ No notification â”‚   â”‚
â”‚   â”‚  â†’ User believes message was delivered â†’ Relationship damaged       â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚  Real example: "I told you I couldn't make it!" / "I never got it" â”‚   â”‚
â”‚   â”‚  This is THE cardinal sin of messaging platforms                    â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚   FAILURE MODE 2: MESSAGE REORDERING                                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Messages arrive out of order â†’ Conversation is confusing          â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚  Alice: "Should we go to dinner?"                                   â”‚   â”‚
â”‚   â”‚  Bob: "Yes"                                                         â”‚   â”‚
â”‚   â”‚  Alice: "Or maybe just drinks?"                                     â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚  Bob sees: "Or maybe just drinks?" then "Should we go to dinner?"  â”‚   â”‚
â”‚   â”‚  Bob's "Yes" now makes no sense                                    â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚   FAILURE MODE 3: DUPLICATE MESSAGES                                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Same message delivered multiple times â†’ User confusion/annoyance  â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚  Less severe than loss, but still breaks trust                     â”‚   â”‚
â”‚   â”‚  "Why did you send that 5 times?"                                  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚   FAILURE MODE 4: PRESENCE LIES                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Shows "online" when offline or vice versa                         â”‚   â”‚
â”‚   â”‚  â†’ Social friction ("Why aren't you responding? You're online!")   â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚  Staff insight: This is why many apps show "last seen" instead     â”‚   â”‚
â”‚   â”‚  of "online now"â€”it's more forgiving of staleness                  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚   FAILURE MODE 5: SYNC DIVERGENCE                                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Different devices show different message history                  â”‚   â”‚
â”‚   â”‚  â†’ User confusion about what was actually said                     â”‚   â”‚
â”‚   â”‚  â†’ "I can see it on my phone but not my laptop"                    â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Part 2: Functional Requirements

## Core Use Cases

### 1. Send a Message (1:1 Conversation)

```
Use Case: Alice sends "Hello" to Bob

Input: 
  - Sender: Alice
  - Recipient: Bob  
  - Content: "Hello"
  - Timestamp: 2024-01-15T10:30:00Z

Process:
  1. Authenticate Alice
  2. Validate message (size, content policy)
  3. Generate message ID (globally unique)
  4. Store message durably
  5. If Bob online: Push to Bob's device(s)
  6. If Bob offline: Queue for later delivery
  7. Acknowledge to Alice (single checkmark)
  8. When Bob receives: Update to delivered (double checkmark)
  9. When Bob reads: Update to read (blue checkmarks)

Output:
  - Message ID for reference
  - Delivery status updates
```

### 2. Send a Message (Group Conversation)

```
Use Case: Alice sends "Meeting at 3" to group "Project Team" (50 members)

Input:
  - Sender: Alice
  - Group: project_team_123
  - Content: "Meeting at 3"

Process:
  1. Authenticate Alice
  2. Verify Alice is group member
  3. Generate message ID
  4. Store message (once, not 50 times)
  5. Fan-out delivery to 50 members
  6. Track per-member delivery status

Complications:
  - Some members online, some offline
  - Some members have notifications muted
  - Some members blocked Alice
  - Members in different timezones
```

### 3. Receive Messages (Real-time)

```
Use Case: Bob receives messages while app is open

Mechanism: Long-lived WebSocket connection

Process:
  1. Bob's device establishes WebSocket to message gateway
  2. Gateway registers Bob's connection in presence service
  3. When message arrives for Bob:
     a. Look up Bob's active connections
     b. Push message through WebSocket
     c. Wait for ACK from device
     d. Mark message as delivered
  4. If connection drops: Re-establish with resume token

Latency target: < 500ms from send to receive
```

### 4. Receive Messages (Offline Sync)

```
Use Case: Bob opens app after 3 days offline

Process:
  1. Bob's device connects with last_sync_timestamp
  2. Server queries: "All messages for Bob since timestamp"
  3. Return messages in batches (pagination)
  4. For each conversation: Return messages in order
  5. Client merges with local state
  6. Update sync checkpoint

Complications:
  - Bob might have 10,000 unread messages
  - Some messages might be deleted by sender
  - Media might need re-download
  - Group memberships might have changed
```

### 5. Typing Indicators

```
Use Case: Alice sees "Bob is typing..."

Mechanism: Ephemeral, unreliable signaling

Process:
  1. Bob types â†’ Client sends "typing" signal
  2. Signal routed to Alice (if online)
  3. Alice displays indicator
  4. Indicator auto-expires after 5 seconds
  5. No persistence, no retry, no guarantee

Staff insight: Typing indicators are cosmetic.
- Lost typing indicators = no harm
- Stale typing indicators = minor annoyance
- Don't optimize for this; focus on message delivery
```

### 6. Read Receipts

```
Use Case: Alice sees that Bob read her message

Process:
  1. Bob's device renders message on screen
  2. Client sends read_receipt(message_id, timestamp)
  3. Receipt stored and forwarded to Alice
  4. Alice's UI updates to show "read"

Privacy considerations:
  - Some users disable read receipts
  - Group read receipts: Show count, not names
  - Read != comprehended (user may have scrolled past)
```

### 7. Message Editing and Deletion

```
Use Case: Alice edits typo / deletes embarrassing message

Edit process:
  1. Alice sends edit(message_id, new_content)
  2. Validate: Alice is author, within edit window
  3. Update message content
  4. Push edit to all recipients
  5. UI shows "edited" indicator

Delete process:
  1. Alice sends delete(message_id)
  2. Options:
     a. Delete for self only (just hide in Alice's view)
     b. Delete for everyone (remove content, show "deleted")
  3. Delete window: Typically 1-48 hours

Staff insight: "Delete for everyone" is contentious.
- Recipient may have already seen/screenshotted
- Creates strange conversation gaps
- Some platforms show "Alice deleted a message"
```

## Read Paths

```
// Get conversation history (hot path)
FUNCTION get_messages(conversation_id, user_id, cursor, limit):
    // Verify access
    IF NOT is_member(user_id, conversation_id):
        RETURN ERROR("Access denied")
    
    // Fetch messages
    messages = query_messages(
        conversation_id, 
        before_cursor=cursor, 
        limit=limit
    )
    
    // Fetch media URLs (signed, time-limited)
    FOR message IN messages:
        IF message.has_media:
            message.media_url = generate_signed_url(message.media_id)
    
    RETURN {
        messages: messages,
        next_cursor: messages.last().id,
        has_more: count > limit
    }

// Get conversation list
FUNCTION get_conversations(user_id, cursor, limit):
    conversations = query_user_conversations(
        user_id,
        order_by=last_message_time DESC,
        limit=limit
    )
    
    // Include snippet of last message
    FOR conv IN conversations:
        conv.last_message = get_last_message(conv.id)
        conv.unread_count = get_unread_count(conv.id, user_id)
    
    RETURN conversations
```

## Write Paths

```
// Send message (critical path)
FUNCTION send_message(sender_id, conversation_id, content, media):
    // Validate
    IF NOT is_member(sender_id, conversation_id):
        RETURN ERROR("Not a member")
    
    IF length(content) > MAX_MESSAGE_SIZE:
        RETURN ERROR("Message too long")
    
    // Generate IDs
    message_id = generate_snowflake_id()
    
    // Handle media upload (async)
    IF media:
        media_id = upload_media_async(media)
    
    // Create message
    message = {
        id: message_id,
        conversation_id: conversation_id,
        sender_id: sender_id,
        content: content,
        media_id: media_id,
        created_at: now(),
        sequence: get_next_sequence(conversation_id)
    }
    
    // Persist (must succeed before ACK)
    store_message(message)
    
    // Update conversation metadata
    update_conversation_last_message(conversation_id, message)
    
    // Trigger delivery (async)
    enqueue_delivery(message)
    
    RETURN {
        message_id: message_id,
        status: "sent",
        timestamp: message.created_at
    }
```

## Control / Admin Paths

```
// Create group conversation
FUNCTION create_group(creator_id, name, member_ids):
    IF len(member_ids) > MAX_GROUP_SIZE:
        RETURN ERROR("Group too large")
    
    conversation_id = generate_id()
    
    create_conversation({
        id: conversation_id,
        type: "group",
        name: name,
        created_by: creator_id,
        created_at: now()
    })
    
    // Add members (including creator)
    FOR member_id IN [creator_id] + member_ids:
        add_member(conversation_id, member_id, role="member")
    
    set_admin(conversation_id, creator_id)
    
    // Send system message
    send_system_message(conversation_id, "Group created")
    
    RETURN conversation_id

// Block user
FUNCTION block_user(blocker_id, blocked_id):
    create_block(blocker_id, blocked_id)
    
    // Prevent future messages
    // Hide existing conversations (don't delete)
    // Remove from each other's contact lists
    
    RETURN SUCCESS

// Report content
FUNCTION report_message(reporter_id, message_id, reason):
    message = get_message(message_id)
    
    create_report({
        reporter_id: reporter_id,
        message_id: message_id,
        conversation_id: message.conversation_id,
        content_snapshot: message.content,
        reason: reason,
        created_at: now()
    })
    
    // Trigger async review workflow
    enqueue_moderation_review(report)
    
    RETURN SUCCESS
```

## Edge Cases

### Edge Case 1: Sending to Blocked User

```
Alice blocked Bob. Bob tries to send message to Alice.

Options:
A) Error: "Cannot send to this user" (reveals block)
B) Silent success: Message appears sent but never delivered
C) Delayed failure: "Message could not be delivered"

Staff approach: Option B (silent success)
- Prevents harassment escalation
- Blocks are private information
- Bob sees sent status, Alice never sees message
```

### Edge Case 2: Group Message to Mixed Online/Offline

```
Group has 100 members:
- 40 online
- 30 offline (will be back today)
- 20 offline (haven't opened app in months)
- 10 have uninstalled app

Message delivery:
1. Immediate push to 40 online members
2. Store for 30 recently-active offline members
3. Store but don't push-notify 20 inactive members
4. Detect uninstalled via push failure, mark for cleanup
```

### Edge Case 3: Message During Network Partition

```
Alice sends message, but network fails before ACK

Client behavior:
1. Show "sending..." indicator
2. Retry with exponential backoff
3. If server ACKs: Update to "sent"
4. If server rejects as duplicate: Already delivered, update status
5. If timeout after N retries: Show "failed to send, tap to retry"

Server behavior:
- Use client-generated message ID for idempotency
- If same ID received twice, return success (already stored)
```

### Edge Case 4: Device Clock Far in Future

```
Alice's phone clock is 1 year ahead.
Message timestamp: 2025-01-15 (actual date: 2024-01-15)

Problems:
- Message sorts to wrong position
- "Last seen" shows future date
- Message expiry confused

Staff approach:
- Server assigns authoritative timestamp
- Client timestamp used only for offline sorting
- On sync, server timestamp wins
```

## What Is Intentionally OUT of Scope

| Excluded | Why |
|----------|-----|
| Voice/video calling | Different infrastructure (WebRTC, media servers) |
| Stories/status updates | Different content model (broadcast, ephemeral) |
| End-to-end encryption | Massive complexity, covered separately |
| Content moderation ML | Separate system, messaging just provides data |
| Payment/commerce | Different compliance, security requirements |
| Bots/integrations platform | API layer on top of core messaging |

---

# Part 3: Non-Functional Requirements

## Latency Expectations

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LATENCY REQUIREMENTS                                     â”‚
â”‚                                                                             â”‚
â”‚   MESSAGE SEND (User presses send â†’ sees "sent"):                           â”‚
â”‚   â€¢ P50: < 100ms (feels instant)                                            â”‚
â”‚   â€¢ P95: < 300ms (acceptable)                                               â”‚
â”‚   â€¢ P99: < 1000ms (user notices but tolerates)                              â”‚
â”‚   â€¢ > 2000ms: User retries, potential duplicate                             â”‚
â”‚                                                                             â”‚
â”‚   MESSAGE DELIVERY (Send â†’ recipient sees):                                 â”‚
â”‚   â€¢ P50: < 500ms (real-time feel)                                           â”‚
â”‚   â€¢ P95: < 2000ms (still responsive)                                        â”‚
â”‚   â€¢ P99: < 5000ms (degraded but working)                                    â”‚
â”‚                                                                             â”‚
â”‚   CONVERSATION LOAD (Open chat â†’ see messages):                             â”‚
â”‚   â€¢ P50: < 200ms (instant)                                                  â”‚
â”‚   â€¢ P99: < 500ms (smooth)                                                   â”‚
â”‚                                                                             â”‚
â”‚   TYPING INDICATORS:                                                        â”‚
â”‚   â€¢ < 300ms to appear (feels real-time)                                     â”‚
â”‚   â€¢ Can drop indicators under load (cosmetic)                               â”‚
â”‚                                                                             â”‚
â”‚   STAFF INSIGHT:                                                            â”‚
â”‚   Message send latency is sacredâ€”it's the user's signal that the           â”‚
â”‚   system is working. Everything else can degrade.                          â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Availability Expectations

```
Target: 99.99% availability (52 minutes downtime/year)

What "available" means for messaging:
1. Users can send messages (even if delivery is delayed)
2. Users can read existing messages
3. New messages eventually arrive

Acceptable degradations during partial outage:
â€¢ Delayed delivery (minutes, not hours)
â€¢ Missing typing indicators
â€¢ Stale presence information
â€¢ Slow media upload/download
â€¢ Missing read receipts

Unacceptable during any outage:
â€¢ Message loss
â€¢ Message corruption
â€¢ Unauthorized access to messages
```

## Consistency Needs

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONSISTENCY MODEL                                        â”‚
â”‚                                                                             â”‚
â”‚   STRONG CONSISTENCY REQUIRED:                                              â”‚
â”‚   â€¢ Message ordering within conversation (per-sender at minimum)            â”‚
â”‚   â€¢ Membership changes (who can see which messages)                         â”‚
â”‚   â€¢ Block/unblock (must take effect immediately)                            â”‚
â”‚                                                                             â”‚
â”‚   EVENTUAL CONSISTENCY ACCEPTABLE:                                          â”‚
â”‚   â€¢ Read receipts (can lag by seconds)                                      â”‚
â”‚   â€¢ Typing indicators (ephemeral anyway)                                    â”‚
â”‚   â€¢ Presence status (30-second staleness OK)                                â”‚
â”‚   â€¢ Unread counts (can lag)                                                 â”‚
â”‚   â€¢ Cross-device sync (seconds of delay OK)                                 â”‚
â”‚                                                                             â”‚
â”‚   THE FUNDAMENTAL TRADE-OFF:                                                â”‚
â”‚   Global total ordering of messages is impossible at scale.                â”‚
â”‚   We use conversation-local ordering with Lamport-style timestamps.        â”‚
â”‚                                                                             â”‚
â”‚   Within single conversation: Messages appear in consistent order           â”‚
â”‚   Across conversations: No global ordering guarantee                        â”‚
â”‚   Across regions: May see messages from other region with lag               â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Durability

```
Message durability requirements:
â€¢ Messages must NEVER be lost after ACK to sender
â€¢ Durability: 99.999999999% (11 nines) for message content
â€¢ This requires synchronous replication before ACK

Strategy:
1. Write to primary storage (with sync replication)
2. Only ACK to sender after durable write confirmed
3. Async replication to secondary regions for disaster recovery

Media durability:
â€¢ Same 11 nines for media files
â€¢ Store in blob storage with cross-region replication
â€¢ Keep metadata even if media expires (show "media expired")
```

## Correctness vs User Experience Trade-offs

```
CORRECTNESS:                           USER EXPERIENCE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Wait for full sync before showing      Show cached data immediately,
conversation                           sync in background
                                       â†’ Choose UX (stale > slow)

Block all operations during            Allow reads, queue writes
network issues                         â†’ Choose UX (partial > nothing)

Show precise message timestamps        Show "2 minutes ago"
(10:42:37.123)                         â†’ Choose UX (human-readable)

Enforce strict ordering                Allow out-of-order display,
(wait for missing messages)            reorder when gap fills
                                       â†’ Depends: strict for 1:1,
                                         relaxed for busy groups

Verify read receipt delivery           Best-effort read receipts
before showing "read"                  â†’ Choose UX (fast > accurate)
```

## Security Implications

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SECURITY REQUIREMENTS                                    â”‚
â”‚                                                                             â”‚
â”‚   CONFIDENTIALITY:                                                          â”‚
â”‚   â€¢ Messages visible only to conversation participants                      â”‚
â”‚   â€¢ No leakage through errors, logs, or side channels                       â”‚
â”‚   â€¢ Encryption in transit (TLS)                                             â”‚
â”‚   â€¢ Encryption at rest (storage encryption)                                 â”‚
â”‚   â€¢ Optional: End-to-end encryption (E2EE)                                  â”‚
â”‚                                                                             â”‚
â”‚   INTEGRITY:                                                                â”‚
â”‚   â€¢ Messages cannot be modified in transit                                  â”‚
â”‚   â€¢ Message history cannot be tampered with                                 â”‚
â”‚   â€¢ Sender identity is authentic                                            â”‚
â”‚                                                                             â”‚
â”‚   ACCESS CONTROL:                                                           â”‚
â”‚   â€¢ Only authenticated users can send/receive                               â”‚
â”‚   â€¢ Group membership enforced                                               â”‚
â”‚   â€¢ Block lists enforced                                                    â”‚
â”‚   â€¢ Admin actions properly authorized                                       â”‚
â”‚                                                                             â”‚
â”‚   STAFF INSIGHT:                                                            â”‚
â”‚   Messaging has strict privacy expectations. A single bug that              â”‚
â”‚   leaks messages to wrong recipient is a PR disaster and potential          â”‚
â”‚   legal liability. Defense in depth is essential.                           â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Part 4: Scale & Load Modeling

## User Scale

```
Reference: WhatsApp-scale messaging platform

Users:
â€¢ Total registered users: 2 billion
â€¢ Daily active users (DAU): 500 million
â€¢ Monthly active users (MAU): 1.5 billion
â€¢ Concurrent connections: 100 million (peak)

Conversations:
â€¢ Average conversations per user: 20
â€¢ Total 1:1 conversations: 10 billion
â€¢ Total group conversations: 500 million
â€¢ Average group size: 10 members
â€¢ Large groups (>100 members): 5 million
```

## Message Volume

```
Messages per day:
â€¢ Total messages: 100 billion/day
â€¢ Average per DAU: 200 messages sent+received
â€¢ Peak hour: 10 billion messages (10x average)

Message breakdown:
â€¢ Text only: 70% (avg 100 bytes)
â€¢ Images: 20% (avg 200KB compressed)
â€¢ Video: 5% (avg 5MB)
â€¢ Voice: 3% (avg 100KB)
â€¢ Other (files, location): 2%
```

## QPS Calculations

```
STEADY STATE:
Messages: 100B/day Ã· 86,400 sec = 1.15M messages/sec
Peak: ~10M messages/sec

Per message, multiple operations:
â€¢ 1 write (store message)
â€¢ N reads (N recipients, plus sender confirmation)
â€¢ 1-3 push notifications
â€¢ 1 unread count update

Effective operations:
â€¢ Write QPS: 10M/sec (peak)
â€¢ Read QPS: 50M/sec (peak) - 5 reads per message avg
â€¢ Push QPS: 30M/sec (peak)

CONNECTION MANAGEMENT:
â€¢ 100M concurrent WebSocket connections
â€¢ Each connection: heartbeat every 30 seconds
â€¢ Heartbeat QPS: 3.3M/sec
```

## Storage Requirements

```
MESSAGE STORAGE:
Daily text: 70B messages Ã— 100 bytes = 7TB/day
Daily images: 20B Ã— 200KB = 4PB/day
Daily video: 5B Ã— 5MB = 25PB/day
Daily total: ~30PB/day

With retention:
â€¢ Hot storage (30 days): 900PB
â€¢ Warm storage (1 year): 10EB
â€¢ Cold storage (archive): âˆ

METADATA STORAGE:
â€¢ Per message: ~500 bytes metadata
â€¢ 100B messages Ã— 500 bytes = 50TB/day
â€¢ Indexes: 2x metadata = 100TB/day

CONVERSATION METADATA:
â€¢ 10B conversations Ã— 1KB = 10TB
â€¢ Relatively static, heavily cached
```

## Burst Behavior

```
PREDICTABLE BURSTS:
â€¢ New Year's midnight: 100x normal (by timezone)
â€¢ Major sporting events: 10-50x
â€¢ Celebrity deaths/announcements: 20x
â€¢ Religious holidays: 5-10x

UNPREDICTABLE BURSTS:
â€¢ Breaking news: 10-20x
â€¢ Viral content: Localized 50x spikes
â€¢ Platform outage recovery: 10x (message backlog)

SCALING IMPLICATIONS:
â€¢ Must handle 100x burst without data loss
â€¢ Acceptable to delay delivery during extreme bursts
â€¢ Must shed load gracefully (typing indicators first)
```

## What Breaks First

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SCALING BOTTLENECKS (In Order)                           â”‚
â”‚                                                                             â”‚
â”‚   1. CONNECTION SERVERS (First to break)                                    â”‚
â”‚      â€¢ 100M WebSocket connections = massive memory                          â”‚
â”‚      â€¢ Connection storms after outage                                       â”‚
â”‚      â€¢ Solution: Horizontal scaling, connection limits per user             â”‚
â”‚                                                                             â”‚
â”‚   2. MESSAGE FANOUT FOR LARGE GROUPS                                        â”‚
â”‚      â€¢ 1000-member group = 1000 deliveries per message                      â”‚
â”‚      â€¢ Active 1000-member group = 1M deliveries/day                        â”‚
â”‚      â€¢ Solution: Read fanout for large groups, lazy delivery               â”‚
â”‚                                                                             â”‚
â”‚   3. PRESENCE SERVICE                                                       â”‚
â”‚      â€¢ 100M users each updating presence                                    â”‚
â”‚      â€¢ Naive approach: NÂ² presence checks                                   â”‚
â”‚      â€¢ Solution: Approximate presence, subscription limits                  â”‚
â”‚                                                                             â”‚
â”‚   4. HOT CONVERSATIONS                                                      â”‚
â”‚      â€¢ Celebrity/influencer conversations                                   â”‚
â”‚      â€¢ Millions of fans messaging same person                               â”‚
â”‚      â€¢ Solution: Queue and rate-limit inbound, special handling             â”‚
â”‚                                                                             â”‚
â”‚   5. MEDIA STORAGE COSTS                                                    â”‚
â”‚      â€¢ 30PB/day is $$$                                                      â”‚
â”‚      â€¢ Solution: Aggressive compression, expiry, tiered storage             â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Dangerous Assumptions in Capacity Planning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           DANGEROUS ASSUMPTIONS THAT KILL SYSTEMS                           â”‚
â”‚                                                                             â”‚
â”‚   ASSUMPTION 1: "Message size averages 100 bytes"                           â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚   DANGER: Ignores distribution. Media messages are 1000x larger.            â”‚
â”‚   REALITY:                                                                  â”‚
â”‚   â€¢ 70% text (100 bytes avg)                                                â”‚
â”‚   â€¢ 20% images (200KB avg) â†’ These dominate bandwidth                       â”‚
â”‚   â€¢ 10% other (variable)                                                    â”‚
â”‚   IMPACT: Sizing bandwidth on "average" underestimates by 100x              â”‚
â”‚   FIX: Size for P99 message size, not average                               â”‚
â”‚                                                                             â”‚
â”‚   ASSUMPTION 2: "Users are evenly distributed across time"                  â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚   DANGER: Peak is 10x average for messaging (evening hours + events)        â”‚
â”‚   REALITY:                                                                  â”‚
â”‚   â€¢ Steady state: 1M msg/sec                                                â”‚
â”‚   â€¢ Peak hour: 10M msg/sec                                                  â”‚
â”‚   â€¢ New Year's midnight: 100M msg/sec (by timezone)                         â”‚
â”‚   IMPACT: Sizing for average = 10x underprovisioned at peak                 â”‚
â”‚   FIX: Size for peak, with 2x headroom for burst                            â”‚
â”‚                                                                             â”‚
â”‚   ASSUMPTION 3: "Connection churn is negligible"                            â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚   DANGER: Mobile networks cause constant reconnections                      â”‚
â”‚   REALITY:                                                                  â”‚
â”‚   â€¢ Average connection lifetime: 10 minutes (not hours)                     â”‚
â”‚   â€¢ 100M connections Ã— 6 reconnects/hour = 600M connect events/hour         â”‚
â”‚   â€¢ That's 170K connect/sec just for maintenance                            â”‚
â”‚   IMPACT: Connection handling dominates CPU if not optimized                â”‚
â”‚   FIX: Budget 30% of gateway CPU for connection lifecycle                   â”‚
â”‚                                                                             â”‚
â”‚   ASSUMPTION 4: "Read:Write ratio is 10:1 like web apps"                    â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚   DANGER: Messaging is closer to 2:1 (conversational pattern)               â”‚
â”‚   REALITY:                                                                  â”‚
â”‚   â€¢ Each message: 1 write + 1-N reads (recipients)                          â”‚
â”‚   â€¢ Average recipients: 2 (mostly 1:1 conversations)                        â”‚
â”‚   â€¢ Actual ratio: 1:2 for message operations                                â”‚
â”‚   IMPACT: Write path needs more capacity than typical web app               â”‚
â”‚   FIX: Optimize write path as much as read path                             â”‚
â”‚                                                                             â”‚
â”‚   ASSUMPTION 5: "Users read all their messages"                             â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚   DANGER: Many messages are never read (muted groups, spam)                 â”‚
â”‚   REALITY:                                                                  â”‚
â”‚   â€¢ 1:1 messages: 95% read within 24 hours                                  â”‚
â”‚   â€¢ Small group: 70% read                                                   â”‚
â”‚   â€¢ Large group: 30% read                                                   â”‚
â”‚   IMPACT: Aggressive push for all messages wastes resources                 â”‚
â”‚   FIX: Tiered notification strategy based on likelihood of engagement       â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STAFF INSIGHT:
The most dangerous assumption is the one you don't know you're making.
Every capacity estimate should list its assumptions explicitly.
When those assumptions are violated, the system fails in predictable ways.
```

---

# Part 5: High-Level Architecture

## Core Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MESSAGING PLATFORM ARCHITECTURE                          â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                         CLIENTS                                     â”‚   â”‚
â”‚   â”‚         Mobile Apps  â”‚  Web App  â”‚  Desktop Apps                    â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                â”‚                                            â”‚
â”‚                                â–¼                                            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                      EDGE / API LAYER                               â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚   â”‚
â”‚   â”‚  â”‚   API       â”‚  â”‚  WebSocket  â”‚  â”‚   Push      â”‚                  â”‚   â”‚
â”‚   â”‚  â”‚   Gateway   â”‚  â”‚  Gateway    â”‚  â”‚   Gateway   â”‚                  â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                â”‚                                            â”‚
â”‚                                â–¼                                            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                      CORE SERVICES                                  â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚   â”‚
â”‚   â”‚  â”‚   Message   â”‚  â”‚  Delivery   â”‚  â”‚  Presence   â”‚                  â”‚   â”‚
â”‚   â”‚  â”‚   Service   â”‚  â”‚  Service    â”‚  â”‚  Service    â”‚                  â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚   â”‚
â”‚   â”‚  â”‚   Sync      â”‚  â”‚  Group      â”‚  â”‚  User       â”‚                  â”‚   â”‚
â”‚   â”‚  â”‚   Service   â”‚  â”‚  Service    â”‚  â”‚  Service    â”‚                  â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                â”‚                                            â”‚
â”‚                                â–¼                                            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                      DATA LAYER                                     â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚   â”‚
â”‚   â”‚  â”‚   Message   â”‚  â”‚   User      â”‚  â”‚   Media     â”‚                  â”‚   â”‚
â”‚   â”‚  â”‚   Store     â”‚  â”‚   Store     â”‚  â”‚   Store     â”‚                  â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚   â”‚
â”‚   â”‚  â”‚   Cache     â”‚  â”‚   Message   â”‚                                   â”‚   â”‚
â”‚   â”‚  â”‚   Layer     â”‚  â”‚   Queue     â”‚                                   â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Component Responsibilities

### API Gateway
- HTTP REST endpoints for non-real-time operations
- Authentication and authorization
- Rate limiting
- Request routing

### WebSocket Gateway
- Maintains long-lived connections with clients
- Routes real-time messages to correct connections
- Handles connection lifecycle (connect, heartbeat, disconnect)
- Stateful: knows which users are connected

### Push Gateway
- Sends push notifications to offline users
- Integrates with APNs (iOS), FCM (Android)
- Batches and prioritizes notifications
- Handles token management and failures

### Message Service
- Core message CRUD operations
- Message validation and storage
- Sequence number assignment
- Handles edits and deletes

### Delivery Service
- Routes messages to recipients
- Manages online vs offline delivery
- Tracks delivery and read status
- Handles retries and failures

### Presence Service
- Tracks online/offline status
- Manages "last seen" timestamps
- Subscription model for presence updates
- Handles privacy settings

### Sync Service
- Provides message history for offline clients
- Handles multi-device synchronization
- Manages sync cursors and checkpoints
- Resolves conflicts

### Group Service
- Group CRUD operations
- Membership management
- Admin functions
- Group metadata

### User Service
- User profiles and settings
- Contact lists
- Block lists
- Preferences

## Stateless vs Stateful Decisions

```
STATELESS SERVICES:
â€¢ Message Service: Processes requests, stores in DB
â€¢ Group Service: CRUD operations, DB-backed
â€¢ User Service: Profile operations, DB-backed
â€¢ Sync Service: Query and return, no local state

STATEFUL SERVICES:
â€¢ WebSocket Gateway: Holds active connections
  - State: user_id â†’ connection mapping
  - Failure: connections drop, clients reconnect
  
â€¢ Presence Service: Tracks who's online
  - State: online users, last activity
  - Failure: presence becomes stale, self-heals
  
â€¢ Delivery Service: In-flight message tracking
  - State: pending deliveries
  - Failure: re-query from message store, retry

STAFF INSIGHT:
Stateful services are harder to scale and recover.
We minimize state and make it reconstructible.
WebSocket state is client-recoverable (reconnect).
Presence state is ephemeral (rebuild on restart).
Delivery state is checkpointed (resume from checkpoint).
```

## Data Flow: Send Message

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SEND MESSAGE FLOW                                        â”‚
â”‚                                                                             â”‚
â”‚   1. SENDER DEVICE                                                          â”‚
â”‚      â”‚                                                                      â”‚
â”‚      â–¼                                                                      â”‚
â”‚   2. API GATEWAY                                                            â”‚
â”‚      â€¢ Authenticate request                                                 â”‚
â”‚      â€¢ Rate limit check                                                     â”‚
â”‚      â”‚                                                                      â”‚
â”‚      â–¼                                                                      â”‚
â”‚   3. MESSAGE SERVICE                                                        â”‚
â”‚      â€¢ Validate message                                                     â”‚
â”‚      â€¢ Generate message ID + sequence number                                â”‚
â”‚      â€¢ Store message in MESSAGE STORE (sync write)                          â”‚
â”‚      â€¢ Return ACK to sender                                                 â”‚
â”‚      â”‚                                                                      â”‚
â”‚      â–¼                                                                      â”‚
â”‚   4. DELIVERY SERVICE (async)                                               â”‚
â”‚      â€¢ Look up conversation members                                         â”‚
â”‚      â€¢ For each recipient:                                                  â”‚
â”‚        â”‚                                                                    â”‚
â”‚        â”œâ”€â–º ONLINE: Query PRESENCE SERVICE                                   â”‚
â”‚        â”‚     â””â”€â–º Send via WEBSOCKET GATEWAY                                 â”‚
â”‚        â”‚                                                                    â”‚
â”‚        â””â”€â–º OFFLINE: Push notification via PUSH GATEWAY                      â”‚
â”‚                                                                             â”‚
â”‚   5. RECIPIENT DEVICE(S)                                                    â”‚
â”‚      â€¢ Receive message via WebSocket or push                                â”‚
â”‚      â€¢ Send delivery ACK                                                    â”‚
â”‚      â”‚                                                                      â”‚
â”‚      â–¼                                                                      â”‚
â”‚   6. DELIVERY SERVICE                                                       â”‚
â”‚      â€¢ Update delivery status                                               â”‚
â”‚      â€¢ Notify sender of delivery                                            â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Data Flow: Receive Messages (Sync)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SYNC MESSAGE FLOW                                        â”‚
â”‚                                                                             â”‚
â”‚   1. CLIENT connects after being offline                                    â”‚
â”‚      â€¢ Sends: last_sync_id = "msg_12345"                                    â”‚
â”‚      â”‚                                                                      â”‚
â”‚      â–¼                                                                      â”‚
â”‚   2. SYNC SERVICE                                                           â”‚
â”‚      â€¢ Query: all messages for user since msg_12345                         â”‚
â”‚      â€¢ Group by conversation                                                â”‚
â”‚      â”‚                                                                      â”‚
â”‚      â–¼                                                                      â”‚
â”‚   3. MESSAGE STORE                                                          â”‚
â”‚      â€¢ Index: (user_id, message_id) for inbox                               â”‚
â”‚      â€¢ Returns paginated results                                            â”‚
â”‚      â”‚                                                                      â”‚
â”‚      â–¼                                                                      â”‚
â”‚   4. SYNC SERVICE                                                           â”‚
â”‚      â€¢ Enrich with conversation metadata                                    â”‚
â”‚      â€¢ Fetch sender info from cache                                         â”‚
â”‚      â€¢ Return to client with new sync cursor                                â”‚
â”‚      â”‚                                                                      â”‚
â”‚      â–¼                                                                      â”‚
â”‚   5. CLIENT                                                                 â”‚
â”‚      â€¢ Merge with local message store                                       â”‚
â”‚      â€¢ Update UI                                                            â”‚
â”‚      â€¢ Store new sync cursor                                                â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Part 6: Deep Component Design

## WebSocket Gateway

### Purpose
Maintains persistent connections with clients for real-time message delivery.

### Internal Data Structures

```
// Per-server connection registry
ConnectionRegistry:
    connections: Map<connection_id, Connection>
    user_connections: Map<user_id, Set<connection_id>>
    
Connection:
    connection_id: string
    user_id: string
    device_id: string
    socket: WebSocket
    connected_at: timestamp
    last_activity: timestamp
    resume_token: string

// Distributed user location
UserLocationService:
    user_locations: Map<user_id, List<ServerAddress>>
    // Backed by Redis with TTL
```

### Connection Lifecycle

```
CONNECT:
1. Client initiates WebSocket handshake
2. Gateway validates auth token
3. Generate connection_id and resume_token
4. Register in local ConnectionRegistry
5. Publish to UserLocationService: "user X on server Y"
6. Send connection ACK with resume_token

HEARTBEAT:
1. Client sends ping every 30 seconds
2. Server responds pong
3. Update last_activity timestamp
4. If no heartbeat for 90 seconds: consider dead

DISCONNECT:
1. Client closes or connection times out
2. Remove from local ConnectionRegistry
3. Remove from UserLocationService
4. Keep resume_token valid for 5 minutes (reconnect window)

RESUME:
1. Client reconnects with resume_token
2. Validate token (not expired, correct user)
3. Replay any messages since disconnect
4. Issue new resume_token
```

### Message Routing

```
FUNCTION route_message_to_user(user_id, message):
    // Find all connections for this user
    locations = UserLocationService.get(user_id)
    
    IF locations is empty:
        RETURN NOT_CONNECTED
    
    FOR location IN locations:
        IF location.server == self:
            // Local delivery
            connections = ConnectionRegistry.get_user_connections(user_id)
            FOR conn IN connections:
                conn.socket.send(message)
        ELSE:
            // Remote delivery via internal RPC
            send_to_server(location.server, user_id, message)
    
    RETURN DELIVERED
```

### Failure Behavior

```
SERVER CRASH:
â€¢ All connections on that server drop
â€¢ Clients detect via missed heartbeat (30-90 seconds)
â€¢ Clients reconnect to different server
â€¢ Messages during gap: delivered on reconnect via sync

NETWORK PARTITION:
â€¢ Connections appear alive but unresponsive
â€¢ Messages queue up, eventually timeout
â€¢ Client-side timeout triggers reconnect

OVERLOAD:
â€¢ New connections rejected with backoff header
â€¢ Existing connections maintained
â€¢ Message delivery prioritized over new connections
```

### Thundering Herd on Reconnection: Quantified Mitigation

```
SCENARIO: WebSocket server with 100K connections crashes

NAIVE BEHAVIOR:
T+0:     Server dies
T+30s:   100K clients detect via heartbeat timeout
T+30s:   100K clients immediately reconnect
T+31s:   Load balancer distributes 100K connect requests across 10 servers
T+31s:   10K connections per server simultaneously = connection storm
T+32s:   TLS handshake CPU spikes to 100%
T+33s:   Some servers OOM due to connection state allocation
T+34s:   More servers crash â†’ cascade

PROBLEM QUANTIFICATION:
â€¢ 100K TLS handshakes Ã— 50ms each = 5M ms of CPU work
â€¢ 10 servers Ã— 8 cores Ã— 1000ms/sec = 80K ms capacity/sec
â€¢ Overload factor: 5M / 80K = 62x capacity
â€¢ Result: All servers become unresponsive

MITIGATION: Client-Side Jitter with Backoff

FUNCTION client_reconnect_with_jitter():
    base_delay = 1_SECOND
    max_delay = 30_SECONDS
    
    FOR attempt IN 1..10:
        // Jittered exponential backoff
        delay = base_delay * (2 ^ (attempt - 1))
        delay = min(delay, max_delay)
        jitter = random(0, delay)
        
        SLEEP(jitter)
        
        TRY:
            connect()
            RETURN SUCCESS
        CATCH connection_refused:
            CONTINUE  // Server still overloaded
        CATCH timeout:
            CONTINUE  // Server still slow
    
    RETURN FAILURE  // Give up after 10 attempts

JITTER DISTRIBUTION EFFECT:
â€¢ 100K clients with 30-second jitter window
â€¢ Average: 100K / 30 sec = 3,333 connections/sec
â€¢ Each server: 333 connections/sec (manageable)
â€¢ Peak (random clumping): ~2x average = 666/sec (still OK)

SERVER-SIDE PROTECTION:

FUNCTION handle_connection_attempt(client):
    current_rate = connection_rate_counter.get()
    
    IF current_rate > MAX_CONNECTIONS_PER_SECOND:
        // Reject with Retry-After header
        RETURN 503 with Retry-After: random(5, 30)
    
    IF pending_connections > MAX_PENDING:
        // Backpressure: don't accept until existing complete
        RETURN 503 with Retry-After: random(1, 5)
    
    // Accept connection
    connection_rate_counter.increment()
    process_connection(client)

ROLLING DEPLOYMENT STRATEGY FOR WEBSOCKET SERVERS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Challenge: Deploying new version to 100 WebSocket servers with 1M connections each

WRONG: Blue-green (instant switch)
â€¢ All connections drop simultaneously
â€¢ 100M reconnections in seconds
â€¢ Guaranteed thundering herd

RIGHT: Rolling drain
1. Mark server for drain (stop accepting new connections)
2. Wait for existing connections to gracefully close (heartbeat cycle)
3. After 5 minutes: Forcefully close remaining connections (5% stragglers)
4. Deploy new version
5. Add back to load balancer pool
6. Repeat for next server

DEPLOYMENT RATE:
â€¢ 100 servers, 5 minutes per server = 8+ hours for full deployment
â€¢ But each server's users reconnect to already-updated servers
â€¢ No thundering herd because only 1% of connections drop at a time

BATCH OPTIMIZATION:
â€¢ Deploy in batches of 5 servers (5% of fleet)
â€¢ Parallel drain â†’ parallel deploy â†’ parallel restore
â€¢ Full deployment: 100 servers / 5 per batch Ã— 5 min = 100 minutes
â€¢ Max simultaneous reconnection: 5M (5% of 100M) spread over 5 min = 17K/sec
```

### Why Simpler Alternatives Fail

```
Alternative: HTTP Long Polling
Problems:
â€¢ Higher latency (connection setup per message)
â€¢ More server load (new connection per poll)
â€¢ Harder to maintain ordering guarantees
â€¢ More complex client-side logic

Alternative: Server-Sent Events (SSE)
Problems:
â€¢ Unidirectional (serverâ†’client only)
â€¢ Need separate channel for clientâ†’server
â€¢ Less efficient for bidirectional chat

Alternative: Single WebSocket server
Problems:
â€¢ Single point of failure
â€¢ Cannot scale beyond one machine
â€¢ All eggs in one basket
```

## Message Service

### Purpose
Handles message creation, storage, and retrieval.

### Data Structures

```
Message:
    message_id: snowflake_id          // Globally unique, time-ordered
    conversation_id: string
    sender_id: string
    content: encrypted_bytes
    content_type: enum(text, image, video, ...)
    media_id: string (optional)
    reply_to: message_id (optional)
    sequence: int64                   // Per-conversation sequence number
    created_at: timestamp
    updated_at: timestamp
    status: enum(active, edited, deleted)
    
ConversationSequence:
    conversation_id: string
    next_sequence: atomic_int64
```

### Sequence Number Assignment

```
// Ensures message ordering within conversation
FUNCTION assign_sequence(conversation_id):
    // Atomic increment
    sequence = INCR conversation_sequences:{conversation_id}
    RETURN sequence

// Why this matters:
// - Messages might arrive at different servers
// - Network delays might reorder messages
// - Sequence provides total order within conversation
```

### Message Storage Strategy

```
WRITE PATH:
1. Validate message
2. Assign sequence number (atomic)
3. Write to primary message store (sync)
4. Replicate to secondary (async)
5. ACK to sender
6. Enqueue for delivery (async)

READ PATH:
1. Query by (conversation_id, sequence_range)
2. Cache recent messages per conversation
3. Return in sequence order

INDEXES:
â€¢ Primary: (conversation_id, sequence) â†’ message
â€¢ Secondary: (user_id, created_at) â†’ message (for inbox)
â€¢ Media: (media_id) â†’ message (for media access control)
```

### Failure Behavior

```
WRITE FAILURE:
â€¢ Retry with idempotency key (client-generated message_id)
â€¢ Duplicate writes detected and deduplicated
â€¢ Client shows "retry" if all retries fail

READ FAILURE:
â€¢ Serve from cache if available
â€¢ Return partial results with "more available" flag
â€¢ Client can retry failed ranges
```

## Delivery Service

### Purpose
Routes messages to recipients, handling online/offline cases.

### Data Structures

```
DeliveryTask:
    message_id: string
    conversation_id: string
    recipients: List<RecipientStatus>
    created_at: timestamp
    attempts: int
    next_retry_at: timestamp
    
RecipientStatus:
    user_id: string
    status: enum(pending, delivered, read, failed)
    delivered_at: timestamp (optional)
    device_deliveries: Map<device_id, DeliveryStatus>
```

### Delivery Algorithm

```
FUNCTION deliver_message(message):
    recipients = get_conversation_members(message.conversation_id)
    recipients.remove(message.sender_id)  // Don't deliver to sender
    
    FOR recipient IN recipients:
        // Check blocks
        IF is_blocked(recipient, message.sender_id):
            CONTINUE
        
        // Check online status
        IF is_online(recipient):
            TRY:
                push_via_websocket(recipient, message)
                mark_delivered(message.id, recipient)
            CATCH:
                enqueue_for_retry(message.id, recipient)
        ELSE:
            // Offline: send push notification
            send_push_notification(recipient, message)
            // Message will be fetched on next sync
            
    RETURN delivery_status
```

### Retry Strategy

```
RETRY POLICY:
â€¢ Immediate retry: 1 attempt
â€¢ After 1 second: 2nd attempt
â€¢ After 5 seconds: 3rd attempt
â€¢ After 30 seconds: 4th attempt
â€¢ After 5 minutes: 5th attempt
â€¢ After 1 hour: mark as "will deliver on sync"

RETRY QUEUE:
â€¢ Priority queue ordered by next_retry_at
â€¢ Separate queues per priority (high: 1:1, low: large groups)
â€¢ Dead letter queue for permanent failures
```

## Presence Service

### Purpose
Tracks user online/offline status with acceptable staleness.

### Design Philosophy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PRESENCE ANTI-PATTERNS (What NOT to do)                                   â”‚
â”‚                                                                             â”‚
â”‚   DON'T: Synchronous presence check on every message                        â”‚
â”‚   WHY: Adds latency to critical path                                        â”‚
â”‚                                                                             â”‚
â”‚   DON'T: Broadcast presence changes to all contacts                         â”‚
â”‚   WHY: O(NÂ²) problemâ€”1M users Ã— 100 contacts = 100M updates                 â”‚
â”‚                                                                             â”‚
â”‚   DON'T: Store presence in main database                                    â”‚
â”‚   WHY: Too high write volume, kills database                                â”‚
â”‚                                                                             â”‚
â”‚   DO: Lazy presence with subscription model                                 â”‚
â”‚   DO: Accept 30-60 second staleness                                         â”‚
â”‚   DO: Use in-memory store with TTL                                          â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Structures

```
PresenceEntry:
    user_id: string
    status: enum(online, away, offline)
    last_seen: timestamp
    ttl: 120 seconds  // Auto-expire if not refreshed

PresenceSubscription:
    subscriber_id: string
    subscribed_to: Set<user_id>
    // Limited to ~100 subscriptions per user
```

### Presence Update Flow

```
USER COMES ONLINE:
1. WebSocket Gateway notifies Presence Service
2. Update: presence:{user_id} = {status: online, last_seen: now}
3. Set TTL: 120 seconds
4. Find subscribers watching this user
5. Push presence update to subscribers (async, best-effort)

HEARTBEAT REFRESH:
1. Every 60 seconds, client activity refreshes presence
2. Update last_seen, reset TTL
3. No broadcast (status unchanged)

USER GOES OFFLINE:
1. Explicit: Client sends "going offline" â†’ immediate update
2. Implicit: TTL expires (120 seconds no refresh)
3. Status becomes "offline" or "last seen X ago"
```

### Why This Design

```
PROBLEM: Naive presence is O(NÂ²)
â€¢ 100M online users
â€¢ Average 100 contacts each
â€¢ Presence change â†’ notify 100 people
â€¢ 100M users Ã— 100 contacts = 10B presence events

SOLUTION: Subscription with limits
â€¢ Only track presence for active conversations
â€¢ Limit subscriptions to ~100 users
â€¢ Only push to users with open app
â€¢ Reduces events by 1000x

ACCEPTABLE STALENESS:
â€¢ "Online" might be 30-60 seconds stale
â€¢ "Last seen 2 hours ago" is fine for messaging
â€¢ Typing indicators fill the real-time gap
```

---

# Part 7: Data Model & Storage Decisions

## Core Entities

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA MODEL                                               â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  USERS                                                              â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ user_id (PK)                                                   â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ phone_number / email (unique, indexed)                         â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ display_name                                                   â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ avatar_url                                                     â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ created_at                                                     â”‚   â”‚
â”‚   â”‚  â””â”€â”€ settings (JSON: privacy, notifications, etc.)                  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  CONVERSATIONS                                                      â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ conversation_id (PK)                                           â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ type (1:1 | group)                                             â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ name (for groups)                                              â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ created_at                                                     â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ created_by                                                     â”‚   â”‚
â”‚   â”‚  â””â”€â”€ metadata (JSON: settings, last_activity, etc.)                 â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  CONVERSATION_MEMBERS                                               â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ conversation_id (PK)                                           â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ user_id (PK)                                                   â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ role (member | admin)                                          â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ joined_at                                                      â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ last_read_sequence (for unread count)                          â”‚   â”‚
â”‚   â”‚  â””â”€â”€ muted_until                                                    â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  MESSAGES                                                           â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ message_id (PK, snowflake)                                     â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ conversation_id (indexed)                                      â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ sequence (per-conversation order)                              â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ sender_id                                                      â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ content (encrypted)                                            â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ content_type                                                   â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ media_id (FK to media)                                         â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ reply_to (FK to message)                                       â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ created_at                                                     â”‚   â”‚
â”‚   â”‚  â””â”€â”€ status (active | edited | deleted)                             â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  USER_INBOX (Denormalized for sync performance)                     â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ user_id (partition key)                                        â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ message_id (sort key)                                          â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ conversation_id                                                â”‚   â”‚
â”‚   â”‚  â””â”€â”€ created_at                                                     â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Storage Technology Choices

```
MESSAGE STORE: Distributed Wide-Column Store (Cassandra/Bigtable)
WHY:
â€¢ Handles massive write throughput (10M/sec)
â€¢ Excellent for time-series data (messages ordered by time)
â€¢ Good partition strategy: conversation_id
â€¢ Easy horizontal scaling
â€¢ Tunable consistency (per-query)

USER/CONVERSATION METADATA: Relational Database (Sharded MySQL/PostgreSQL)
WHY:
â€¢ Complex queries (find conversation by members)
â€¢ Strong consistency needed for membership
â€¢ Relatively low write volume
â€¢ ACID transactions for group operations

PRESENCE: In-Memory Store (Redis Cluster)
WHY:
â€¢ Extremely fast reads/writes
â€¢ Built-in TTL for auto-expiry
â€¢ Pub/sub for presence updates
â€¢ Ephemeral data (can lose and rebuild)

MEDIA: Object Storage (S3/GCS)
WHY:
â€¢ Unlimited scale
â€¢ Cost-effective for large files
â€¢ Built-in CDN integration
â€¢ Lifecycle policies for archival

CACHE: Distributed Cache (Redis/Memcached)
WHY:
â€¢ Hot conversation metadata
â€¢ Recent message cache
â€¢ User profile cache
â€¢ Session/auth token cache
```

## Partitioning Strategy

```
MESSAGES:
Partition key: conversation_id
Sort key: sequence

Why:
â€¢ All messages in conversation are co-located
â€¢ Reads are always within single partition
â€¢ Writes distributed across conversations
â€¢ Hot conversations don't affect others

POTENTIAL ISSUE: Hot conversation (celebrity with millions)
Solution: Sub-partition large conversations by time bucket

USER_INBOX:
Partition key: user_id
Sort key: message_id

Why:
â€¢ Sync query is always for single user
â€¢ Message_id is time-ordered (snowflake)
â€¢ Efficient range queries for "messages since X"
```

## Retention Policies

```
MESSAGE CONTENT:
â€¢ Hot storage (fast SSD): 30 days
â€¢ Warm storage (HDD): 1 year
â€¢ Cold storage (archive): 7 years (legal/compliance)
â€¢ User-deleted: Remove from all tiers

MEDIA:
â€¢ Original: 30 days
â€¢ Thumbnails: 1 year
â€¢ After expiry: Show "media expired" placeholder
â€¢ User can re-upload if they have local copy

METADATA:
â€¢ Conversation metadata: Forever (small)
â€¢ Message metadata: Same as content
â€¢ Delivery receipts: 30 days
â€¢ Read receipts: 7 days

PRESENCE/TYPING:
â€¢ No persistence (in-memory only)
â€¢ Reconstruct on service restart
```

## Schema Evolution

```
CHALLENGE: Billions of messages, can't do ALTER TABLE

STRATEGY 1: New columns are nullable with defaults
â€¢ Add new column with default value
â€¢ Old messages have default
â€¢ New messages have explicit value
â€¢ Backfill async if needed

STRATEGY 2: Schema versioning
â€¢ Each message has schema_version field
â€¢ Reader handles multiple versions
â€¢ Writer always uses latest version
â€¢ Gradual migration over time

STRATEGY 3: Flexible schema
â€¢ Content stored as JSON/protobuf
â€¢ Schema defined by content_type
â€¢ New types don't require DB changes

EXAMPLE: Adding reactions
V1: No reactions
V2: reactions: Map<emoji, count>
V3: reactions: List<{emoji, user_id, timestamp}>

Migration:
â€¢ V1 messages: reactions = null (no reactions existed)
â€¢ V2 messages: Migrate to V3 format on read
â€¢ V3 messages: Full data
```

### Sequence Gap Handling When Assignment Fails

```
SCENARIO: Redis sequence assignment fails mid-operation

Timeline:
T+0: Message A gets sequence 100 (success)
T+1: Message B tries to get sequence (Redis timeout)
T+2: Message C gets sequence 101 (success)
T+3: Message B retries, gets sequence 102 (success)

Result: Sequences [100, 101, 102] but arrival order was [A, C, B]

PROBLEM:
â€¢ Client expects sequences to reflect send order
â€¢ Gap detection (101 missing) triggers sync
â€¢ Reordering confuses conversation flow

SOLUTION: Sender-Side Sequence + Server Reconciliation

FUNCTION assign_message_sequence(conversation_id, sender_id, client_seq):
    // Client provides its own sequence number per-sender
    // Server assigns global sequence but respects sender ordering
    
    server_seq = INCR conversation_sequences:{conversation_id}
    
    message.client_sequence = client_seq    // Sender's local order
    message.server_sequence = server_seq    // Global storage order
    message.sender_id = sender_id
    
    RETURN message

CLIENT RECONSTRUCTION:
FUNCTION order_messages(messages):
    // Group by sender, sort by client_sequence within sender
    // Then merge based on server_sequence for cross-sender ordering
    
    by_sender = group_by(messages, m => m.sender_id)
    
    FOR sender_id, sender_msgs IN by_sender:
        sender_msgs.sort(key=client_sequence)  // Per-sender strict order
    
    // Merge maintaining causal order
    result = merge_sorted(by_sender.values(), 
                          key=server_sequence,
                          preserve_sender_order=TRUE)
    
    RETURN result

GAP HANDLING:
â€¢ Gaps in server_sequence are normal (failed transactions)
â€¢ Gaps in client_sequence indicate lost messages (trigger resync)
â€¢ Client tracks: last_seen_client_sequence per sender
â€¢ If gap: Request messages in range from server

WHY THIS MATTERS:
â€¢ Pure server-side sequence breaks on partial failures
â€¢ Pure client-side sequence doesn't order across senders
â€¢ Hybrid approach gives best of both:
  - Sender ordering is always correct (client-side)
  - Cross-sender ordering is best-effort (server-side)
```

### Conversation Tombstone Handling and Cleanup

```
SCENARIO: User deletes conversation with 10,000 messages

NAIVE APPROACH (DON'T DO THIS):
DELETE FROM messages WHERE conversation_id = 'conv_123'
// Takes 30 seconds, blocks other writes, causes timeout

STAFF APPROACH: Tombstone with Lazy Cleanup

FUNCTION delete_conversation(user_id, conversation_id):
    // Step 1: Mark as deleted (instant)
    UPDATE conversation_members
    SET deleted_at = now(), visible = FALSE
    WHERE user_id = user_id AND conversation_id = conversation_id
    
    // User immediately sees conversation gone
    RETURN SUCCESS

FUNCTION background_cleanup():
    // Step 2: Async cleanup job (runs hourly)
    
    deleted_convos = SELECT conversation_id 
                     FROM conversation_members
                     WHERE deleted_at < now() - 24_HOURS
                     AND all_members_deleted = TRUE
    
    FOR conv_id IN deleted_convos:
        // Delete in batches to avoid load spikes
        WHILE messages_exist(conv_id):
            DELETE FROM messages 
            WHERE conversation_id = conv_id 
            LIMIT 1000
            
            SLEEP 100ms  // Rate limit to avoid DB overload

RETENTION COMPLICATIONS:
â€¢ Legal hold: Some users under litigation, can't delete
â€¢ Cross-user: Alice deletes, Bob hasn't â†’ hide for Alice, keep for Bob
â€¢ Media: Media might be referenced by multiple messages
â€¢ Audit: May need to retain metadata even if content deleted

TOMBSTONE STATE MACHINE:
[visible] â†’ [hidden_for_user] â†’ [hidden_all_members] â†’ [content_deleted] â†’ [fully_purged]
     â†“              â†“                    â†“                    â†“
   Active      User action          Grace period         Background job

STAFF INSIGHT:
Deletion is a product feature, not a database operation.
The data lifecycle has legal, compliance, and UX implications.
"Delete" usually means "hide and schedule for eventual removal."
```

## Why Other Data Models Were Rejected

```
REJECTED: Single MySQL for everything
Problems:
â€¢ Cannot handle 10M writes/sec
â€¢ Joins become bottleneck
â€¢ Sharding MySQL is complex
â€¢ No built-in time-series optimization

REJECTED: MongoDB for messages
Problems:
â€¢ Document model doesn't fit conversation pattern
â€¢ Ordering guarantees weaker
â€¢ Operational complexity at scale
â€¢ Better options exist for this workload

REJECTED: Storing messages in S3 directly
Problems:
â€¢ No efficient range queries
â€¢ No real-time writes
â€¢ High latency for recent messages
â€¢ Works for archive, not hot path
```

---

# Part 8: Consistency, Concurrency & Ordering

## The Core Ordering Challenge

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MESSAGE ORDERING: THE HARD PROBLEM                       â”‚
â”‚                                                                             â”‚
â”‚   SCENARIO: Alice and Bob messaging simultaneously                          â”‚
â”‚                                                                             â”‚
â”‚   Alice's phone           Network              Bob's phone                  â”‚
â”‚   10:00:00.001            â”€â”€â”€â”€â”€â”€â”€â”€â–º            10:00:00.100                 â”‚
â”‚   "Hello"                                      "Hi there"                   â”‚
â”‚                           â—„â”€â”€â”€â”€â”€â”€â”€â”€                                         â”‚
â”‚   10:00:00.150                                 10:00:00.050                 â”‚
â”‚   "How are you?"                               "What's up?"                 â”‚
â”‚                                                                             â”‚
â”‚   PROBLEM: Whose "Hello" comes first?                                       â”‚
â”‚   â€¢ Alice's clock: Alice first (10:00:00.001 vs 10:00:00.100)               â”‚
â”‚   â€¢ Bob's clock: Bob first (10:00:00.050 vs 10:00:00.150)                   â”‚
â”‚   â€¢ Server receipt: Depends on network latency                              â”‚
â”‚                                                                             â”‚
â”‚   THERE IS NO "CORRECT" ANSWER                                              â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Ordering Guarantees We Provide

```
GUARANTEE 1: Per-sender ordering (STRICT)
â€¢ Messages from same sender appear in send order
â€¢ Implemented via per-sender sequence numbers
â€¢ Client cannot send message N+1 until N is ACKed

GUARANTEE 2: Per-conversation causal ordering (STRONG)
â€¢ If Alice reads Bob's message then replies, reply comes after
â€¢ Implemented via Lamport-style logical clocks
â€¢ Each message carries "I have seen message X" metadata

GUARANTEE 3: Cross-sender ordering (BEST EFFORT)
â€¢ Use server-assigned timestamps as tiebreaker
â€¢ Within same millisecond: arbitrary but consistent
â€¢ Users rarely notice cross-sender ordering issues

NO GUARANTEE: Cross-conversation ordering
â€¢ Message to Group A and message to Friend B have no ordering
â€¢ Different conversations are independent
â€¢ This is fineâ€”users don't compare across conversations
```

## Implementation: Logical Clocks

```
LAMPORT TIMESTAMP PER CONVERSATION:

Message:
    message_id: string
    logical_time: int64
    sender_sequence: int64
    causal_dependencies: List<message_id>

SEND MESSAGE:
1. Client tracks: max_seen_logical_time
2. new_logical_time = max_seen_logical_time + 1
3. Include: causal_dependencies = [last_message_I_saw]
4. Server validates: new_time > all dependency times
5. If conflict: Server assigns higher time

RECEIVE MESSAGE:
1. Update: max_seen_logical_time = max(current, received)
2. Sort messages by logical_time
3. If gap in sequence: Wait or fetch missing

CONFLICT RESOLUTION:
Same logical time? Use (logical_time, sender_id) as total order
Sender_id comparison is arbitrary but consistent
```

## Race Conditions

### Race 1: Simultaneous Send to Same Conversation

```
Alice and Bob both send at T=0

Server A receives Alice's message
Server B receives Bob's message

WITHOUT COORDINATION:
â€¢ Both assigned sequence=1
â€¢ Conflict when syncing

SOLUTION: Centralized sequence assignment
â€¢ Conversation sequence stored in Redis
â€¢ INCR is atomic
â€¢ Both servers get unique sequence
â€¢ May have gaps if one fails, but ordering preserved
```

### Race 2: Edit During Delivery

```
Alice sends "Hello"
Message in transit to Bob
Alice edits to "Hello!"
Edit arrives before original

Bob's view:
1. Receives edit for message he doesn't have
2. What to do?

SOLUTION: Idempotent operations with version
â€¢ Each message has version number
â€¢ Edit: version++ with full content
â€¢ Client applies edits to latest known version
â€¢ If base message arrives after edit: Apply edit
```

### Race 3: Group Membership Change During Send

```
Alice sends to group of 10 members
During fanout, Carol is removed from group

Question: Should Carol receive the message?

SOLUTION: Snapshot membership at send time
â€¢ When message stored, snapshot member list
â€¢ Fanout uses snapshot, not current membership
â€¢ Carol receives (was member at send time)
â€¢ Future messages: Carol not included

Alternative: Use current membership
â€¢ Carol doesn't receive
â€¢ But this creates strange gaps in Carol's history
â€¢ Staff choice: Snapshot (more intuitive for users)
```

## Idempotency

```
CLIENT-GENERATED MESSAGE ID:

PROBLEM: 
Client sends message, network fails before ACK
Client retriesâ€”is this a duplicate or new message?

SOLUTION:
Client generates unique message_id before first attempt
Server uses message_id for deduplication

FUNCTION receive_message(message):
    existing = get_message_by_id(message.id)
    
    IF existing:
        // Duplicate - return existing
        RETURN existing
    ELSE:
        // New message - store and process
        store(message)
        RETURN message

IDEMPOTENCY WINDOW:
â€¢ Keep message_id lookup for 24 hours
â€¢ After 24 hours: Unlikely duplicate (user would notice)
â€¢ Very late retry: Accept as new (rare edge case)
```

## Clock Assumptions

```
WHAT WE ASSUME:
â€¢ Server clocks are synchronized via NTP
â€¢ Server clock skew < 100ms (acceptable)
â€¢ Client clocks can be arbitrarily wrong
â€¢ Network latency is unbounded

IMPLICATIONS:
â€¢ Never trust client timestamps for ordering
â€¢ Client timestamp for display: "Sent at 10:42am"
â€¢ Server timestamp for ordering and sync
â€¢ Logical clocks for causal ordering

WHEN CLOCKS GO WRONG:
Client 1 year ahead:
â€¢ Server rejects timestamp, uses server time
â€¢ Message sorts correctly
â€¢ Display time might be wrong briefly

Server 1 hour behind:
â€¢ Messages from this server have old timestamps
â€¢ Sort among themselves correctly
â€¢ May sort before messages from other servers
â€¢ Self-heals when NTP corrects

STAFF INSIGHT:
Clock issues are inevitable at scale. Design
so that clock problems cause cosmetic issues
(wrong display time) not correctness issues
(wrong message order).
```

---

# Part 9: Failure Modes & Degradation

## Failure Categories

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FAILURE SEVERITY LEVELS                                  â”‚
â”‚                                                                             â”‚
â”‚   LEVEL 1: INVISIBLE TO USERS                                               â”‚
â”‚   â€¢ Single server failure with quick failover                               â”‚
â”‚   â€¢ Cache miss (falls back to database)                                     â”‚
â”‚   â€¢ Replica lag (reads slightly stale data)                                 â”‚
â”‚                                                                             â”‚
â”‚   LEVEL 2: COSMETIC DEGRADATION                                             â”‚
â”‚   â€¢ Typing indicators stop working                                          â”‚
â”‚   â€¢ Presence shows stale data                                               â”‚
â”‚   â€¢ Read receipts delayed                                                   â”‚
â”‚   â€¢ Push notifications delayed                                              â”‚
â”‚                                                                             â”‚
â”‚   LEVEL 3: NOTICEABLE DEGRADATION                                           â”‚
â”‚   â€¢ Message delivery delayed (seconds to minutes)                           â”‚
â”‚   â€¢ Slow conversation loading                                               â”‚
â”‚   â€¢ Media upload/download failing                                           â”‚
â”‚                                                                             â”‚
â”‚   LEVEL 4: SIGNIFICANT IMPACT                                               â”‚
â”‚   â€¢ Messages not delivering (but stored)                                    â”‚
â”‚   â€¢ Cannot send new messages                                                â”‚
â”‚   â€¢ Cannot load conversations                                               â”‚
â”‚                                                                             â”‚
â”‚   LEVEL 5: CATASTROPHIC                                                     â”‚
â”‚   â€¢ Message loss (data corruption)                                          â”‚
â”‚   â€¢ Messages delivered to wrong users                                       â”‚
â”‚   â€¢ Extended complete outage                                                â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Partial Failures

### WebSocket Gateway Failure

```
FAILURE: 10% of WebSocket gateways crash

IMPACT:
â€¢ 10% of users disconnected
â€¢ Messages to those users queue up
â€¢ Reconnections storm to remaining servers

MITIGATION:
â€¢ Clients auto-reconnect with exponential backoff
â€¢ Load balancer removes dead servers
â€¢ Remaining servers handle increased connections
â€¢ Message queue drains when users reconnect

DEGRADATION:
â€¢ 10% of users: 10-30 second delay
â€¢ 90% of users: Unaffected
â€¢ No message loss

RECOVERY TIME: 30 seconds to 2 minutes
```

### Message Store Failure

```
FAILURE: Primary message store unavailable

IMPACT:
â€¢ Cannot write new messages
â€¢ Cannot read recent messages (not in cache)

MITIGATION:
â€¢ Switch to secondary replica (async, may be behind)
â€¢ Queue writes in memory/local disk
â€¢ Return "sending..." to client, complete async
â€¢ Serve cached conversations for reads

DEGRADATION:
â€¢ Writes: Delayed, client shows "sending..."
â€¢ Reads: Recent messages might be missing
â€¢ No data loss if primary recovers

RECOVERY TIME: Depends on primary repair
```

### Presence Service Failure

```
FAILURE: Presence service completely down

IMPACT:
â€¢ Online status not updating
â€¢ "Last seen" stuck at failure time
â€¢ Delivery routing guesses online/offline

MITIGATION:
â€¢ Cache last-known presence (stale)
â€¢ Assume everyone potentially online
â€¢ Send both push and WebSocket (wasteful but works)

DEGRADATION:
â€¢ Presence shows stale data
â€¢ Some unnecessary push notifications
â€¢ Core messaging unaffected

RECOVERY TIME: Immediate once service restarts
```

## Slow Dependencies

```
MESSAGE STORE SLOW (P99 > 1 second):

Detection:
â€¢ Latency metrics alert
â€¢ Queue depth growing
â€¢ Timeout rate increasing

Response:
â€¢ Shed non-critical operations (analytics events)
â€¢ Increase timeout for critical writes
â€¢ Serve reads from cache more aggressively
â€¢ Consider circuit breaker if persistent

PUSH NOTIFICATION SERVICE SLOW:

Detection:
â€¢ Push delivery latency increasing
â€¢ Push queue growing

Response:
â€¢ Push is non-blocking anyway
â€¢ Increase queue capacity
â€¢ Drop low-priority pushes (read receipts)
â€¢ Batch more aggressively
```

## Retry Storms

```
SCENARIO: Message store recovers after 5-minute outage

PROBLEM:
â€¢ 5 minutes of messages queued
â€¢ All messages try to deliver at once
â€¢ Store overwhelmed again

PREVENTION:
â€¢ Jitter in retry timing
â€¢ Gradual ramp-up after recovery
â€¢ Priority queue (1:1 before groups, recent before old)
â€¢ Rate limit writes per second

IMPLEMENTATION:
FUNCTION calculate_retry_delay(attempt, base_delay):
    delay = base_delay * (2 ^ attempt)
    jitter = random(0, delay * 0.1)
    RETURN delay + jitter

// Retry at 1s, 2s, 4s, 8s... with 10% jitter
// Spreads retries over time window
```

### Retry Storm Quantification and Blast Radius Analysis

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              RETRY STORM: QUANTIFIED IMPACT ANALYSIS                        â”‚
â”‚                                                                             â”‚
â”‚   SCENARIO: 5-minute outage at 10M messages/sec capacity                    â”‚
â”‚                                                                             â”‚
â”‚   QUEUED BACKLOG:                                                           â”‚
â”‚   5 min Ã— 60 sec Ã— 10M msg/sec = 3 BILLION messages queued                  â”‚
â”‚                                                                             â”‚
â”‚   NAIVE RECOVERY (all retry at once):                                       â”‚
â”‚   â€¢ 3B messages Ã— 3 retries each = 9B requests in first minute              â”‚
â”‚   â€¢ System capacity: 10M/sec Ã— 60 = 600M/minute                             â”‚
â”‚   â€¢ Overload factor: 9B / 600M = 15x capacity                               â”‚
â”‚   â€¢ Result: IMMEDIATE RE-FAILURE                                            â”‚
â”‚                                                                             â”‚
â”‚   CONTROLLED RECOVERY (with admission control):                             â”‚
â”‚   â€¢ Ramp: 10% â†’ 25% â†’ 50% â†’ 75% â†’ 100% capacity over 10 minutes             â”‚
â”‚   â€¢ Drain time: 3B messages / (avg 50% Ã— 10M/sec) = 10 minutes              â”‚
â”‚   â€¢ Max delay for any message: 10 min outage + 10 min drain = 20 min        â”‚
â”‚                                                                             â”‚
â”‚   PRIORITY DURING RECOVERY:                                                 â”‚
â”‚   1. 1:1 messages < 1 min old (hot)                                         â”‚
â”‚   2. Group messages < 1 min old                                             â”‚
â”‚   3. 1:1 messages < 5 min old                                               â”‚
â”‚   4. All remaining messages (FIFO within tier)                              â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STAFF-LEVEL IMPLEMENTATION:

FUNCTION recovery_admission_controller(time_since_recovery):
    // Ramp capacity gradually to prevent re-failure
    
    IF time_since_recovery < 1_MINUTE:
        capacity_percent = 10
    ELSE IF time_since_recovery < 3_MINUTES:
        capacity_percent = 25
    ELSE IF time_since_recovery < 5_MINUTES:
        capacity_percent = 50
    ELSE IF time_since_recovery < 8_MINUTES:
        capacity_percent = 75
    ELSE:
        capacity_percent = 100
    
    RETURN capacity_percent

FUNCTION dequeue_with_priority(queue, capacity_budget):
    // Process highest priority messages first
    
    priorities = [
        (filter: age < 1min AND type = "1:1", weight: 4),
        (filter: age < 1min AND type = "group", weight: 3),
        (filter: age < 5min AND type = "1:1", weight: 2),
        (filter: TRUE, weight: 1)  // catch-all
    ]
    
    messages_to_process = []
    remaining_budget = capacity_budget
    
    FOR priority IN priorities:
        eligible = queue.filter(priority.filter)
        take_count = min(eligible.count, remaining_budget * priority.weight / sum(weights))
        messages_to_process.extend(eligible.take(take_count))
        remaining_budget -= take_count
    
    RETURN messages_to_process

WHY THIS MATTERS AT L6:
â€¢ Naive retry kills systems after outages
â€¢ The outage is often shorter than the retry storm aftermath
â€¢ L5 focuses on the outage; L6 focuses on recovery amplification
â€¢ Real-world: WhatsApp's 2021 outage had 6-hour recovery due to storm

BLAST RADIUS CONTAINMENT:
â€¢ Retry storms from one cell MUST NOT affect other cells
â€¢ Per-cell rate limiters at ingress prevent cross-cell cascade
â€¢ If cell A storms, cell B's users are unaffected
â€¢ Monitoring: alert if inter-cell traffic exceeds baseline by 5x
```

## Data Corruption

```
SCENARIO: Bug causes messages stored with wrong conversation_id

DETECTION:
â€¢ Users report seeing wrong messages
â€¢ Message count mismatch in conversations
â€¢ Anomaly detection on message patterns

IMMEDIATE RESPONSE:
1. Identify scope (how many messages affected)
2. Stop the bleeding (fix or rollback bad code)
3. Quarantine affected data
4. Communicate to affected users

RECOVERY:
â€¢ If recoverable: Repair conversation_id from metadata
â€¢ If not: Mark messages as "recovered" with caveats
â€¢ Worst case: Inform users of data loss

PREVENTION:
â€¢ Strong typing on conversation_id
â€¢ Foreign key constraints (where possible)
â€¢ Write-time validation
â€¢ Audit log for forensics
```

## Control-Plane Failures

```
CONFIG SERVICE DOWN:

Impact:
â€¢ Cannot update rate limits
â€¢ Cannot change feature flags
â€¢ Cannot modify routing rules

Mitigation:
â€¢ Servers cache last-known config
â€¢ Stale config is usually fine
â€¢ Critical configs have long TTL

GROUP MANAGEMENT SERVICE DOWN:

Impact:
â€¢ Cannot create new groups
â€¢ Cannot add/remove members
â€¢ Existing groups work fine

Mitigation:
â€¢ Queue group operations
â€¢ Notify users of delay
â€¢ Retry on recovery
```

## Failure Timeline Walkthrough

```
T+0:00 - Primary message database becomes unresponsive
â”‚
T+0:05 - Health checks fail, alerts fire
â”‚
T+0:10 - Automatic failover to secondary begins
â”‚        â€¢ Writes queue in memory
â”‚        â€¢ Reads served from cache
â”‚        â€¢ Clients see "sending..." stuck
â”‚
T+0:30 - Failover complete, secondary promoted
â”‚        â€¢ New writes go to new primary
â”‚        â€¢ Queued writes drain (FIFO)
â”‚        â€¢ Some writes during transition may retry
â”‚
T+1:00 - Queue drained, normal operation
â”‚        â€¢ Some messages delayed by 30-60 seconds
â”‚        â€¢ Order preserved within conversations
â”‚
T+5:00 - Old primary recovered, becomes secondary
â”‚        â€¢ Catches up via replication
â”‚        â€¢ No user impact
â”‚
POST-MORTEM:
â€¢ 60 seconds of elevated latency
â€¢ 0 messages lost
â€¢ 0.1% of users experienced noticeable delay
â€¢ Root cause: Disk I/O saturation from runaway query
```

## Graceful Degradation Strategy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DEGRADATION PRIORITY                                     â”‚
â”‚                                                                             â”‚
â”‚   PROTECT AT ALL COSTS:                                                     â”‚
â”‚   1. Message storage (never lose a message)                                 â”‚
â”‚   2. Message delivery (delay OK, lose not OK)                               â”‚
â”‚   3. Message ordering (within conversation)                                 â”‚
â”‚                                                                             â”‚
â”‚   DEGRADE FIRST (SHED LOAD):                                                â”‚
â”‚   1. Typing indicators (cosmetic)                                           â”‚
â”‚   2. Read receipts (cosmetic)                                               â”‚
â”‚   3. Presence updates (stale is fine)                                       â”‚
â”‚   4. Media thumbnails (show placeholder)                                    â”‚
â”‚   5. Search (can fail gracefully)                                           â”‚
â”‚                                                                             â”‚
â”‚   DEGRADATION TOGGLES:                                                      â”‚
â”‚   â€¢ disable_typing_indicators: true                                         â”‚
â”‚   â€¢ disable_read_receipts: true                                             â”‚
â”‚   â€¢ presence_staleness_ok: 300 (seconds)                                    â”‚
â”‚   â€¢ disable_media_preview: true                                             â”‚
â”‚   â€¢ message_delivery_best_effort: true                                      â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Cascading Failure Timeline: Complete Analysis

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CASCADING FAILURE: REDIS PRESENCE CLUSTER DEGRADATION               â”‚
â”‚                                                                             â”‚
â”‚   This timeline shows how a single component failure cascades through       â”‚
â”‚   the system, and how containment prevents catastrophic outage.             â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TRIGGER: Redis Presence cluster primary node OOM kills

T+0:00   INITIAL FAILURE
         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         â€¢ Redis primary: Memory exhaustion, process killed
         â€¢ Impact: Write path to presence unavailable
         â€¢ Blast radius: All presence updates fail
         â€¢ User impact: None yet (cached presence still valid)

T+0:05   DETECTION
         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         â€¢ Redis sentinel detects primary down
         â€¢ Failover initiated to replica
         â€¢ Delivery Service: Presence lookups timeout (100ms)
         â€¢ Delivery Service: Falls back to "assume online, try WebSocket"
         
T+0:15   FAILOVER ATTEMPT FAILS
         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         â€¢ Replica also under memory pressure (same traffic pattern)
         â€¢ Replica promoted but immediately starts swapping
         â€¢ Latency: 100ms â†’ 2 seconds for presence operations
         
T+0:30   PROPAGATION TO DELIVERY SERVICE
         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         â€¢ Delivery Service thread pool saturates waiting on Redis
         â€¢ Default pool: 200 threads Ã— 2 second wait = 400 req/sec capacity
         â€¢ Actual load: 10,000 req/sec
         â€¢ Queue depth grows: 10K, 20K, 50K...
         â€¢ Memory pressure on Delivery Service begins
         
T+1:00   USER IMPACT BEGINS
         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         â€¢ Delivery latency: <1 sec â†’ 30 seconds
         â€¢ Push notifications delayed
         â€¢ Users see: Messages "stuck at sending" then delivered late
         â€¢ Support tickets: Starting to appear
         
T+1:30   CIRCUIT BREAKER TRIGGERS
         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         â€¢ Delivery Service circuit breaker on Presence: OPEN
         â€¢ All presence calls fail-fast (5ms instead of 2sec)
         â€¢ Fallback: Treat all users as "potentially online"
         â€¢ Delivery: Try WebSocket first, push notification as backup
         â€¢ This is CORRECT degradation behavior
         
T+2:00   AMPLIFICATION: PUSH NOTIFICATION SURGE
         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         â€¢ Without presence, push sent to ALL users (even online ones)
         â€¢ Push volume: 2M/min â†’ 20M/min (10x)
         â€¢ Push gateway: Queue grows
         â€¢ Third-party (APNs/FCM): Rate limits hit
         
T+3:00   SECONDARY DEGRADATION
         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         â€¢ Push gateway: Dropping low-priority pushes
         â€¢ Read receipts, typing: Not being pushed
         â€¢ Message delivery push: Still flowing (high priority)
         â€¢ User impact: "My friend isn't getting typing indicators"
         
T+5:00   REDIS CLUSTER RECOVERY
         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         â€¢ On-call scales Redis cluster (adds memory)
         â€¢ New primary stabilizes
         â€¢ Latency: 2 sec â†’ 100ms
         
T+5:30   CIRCUIT BREAKER: HALF-OPEN
         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         â€¢ Test requests to Presence succeed
         â€¢ Gradual traffic ramp: 10% â†’ 25% â†’ 50% â†’ 100%
         â€¢ Push volume decreasing as presence works again
         
T+8:00   FULL RECOVERY
         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         â€¢ All systems nominal
         â€¢ Push backlog drained
         â€¢ Presence up-to-date
         
T+10:00  POST-INCIDENT
         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         â€¢ Postmortem scheduled
         â€¢ Action items: Memory alerting, Redis cluster sizing
         
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

BLAST RADIUS ANALYSIS:
                                                                             
Component              Impact Duration    User-Visible Effect               
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Presence Service       8 minutes          "Online" status stale             
Delivery Service       7 minutes          30-second message delay           
Push Gateway           6 minutes          Duplicate pushes to online users  
Message Storage        0 minutes          None (isolated)                   
WebSocket Gateway      0 minutes          None (isolated)                   
Message ordering       0 minutes          None (independent of presence)    
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

CONTAINMENT THAT WORKED:
1. Circuit breaker prevented Delivery Service from dying
2. Fail-fast allowed Delivery to process other work
3. Graceful degradation (presence-less delivery) maintained core function
4. Priority queuing in Push Gateway protected message notifications
5. Message storage isolation prevented data loss

CONTAINMENT THAT COULD IMPROVE:
1. Redis memory alerting should have fired earlier
2. Presence cache TTL could be longer (reduces write pressure)
3. Push gateway should have circuit breaker on third-party APIs
```

---

# Part 10: Performance Optimization & Hot Paths

## Critical Paths

```
PATH 1: SEND MESSAGE (Most critical)
User action â†’ ACK visible

Target: < 100ms P50, < 300ms P95

Steps:
1. TLS termination: 5ms
2. Auth validation: 5ms (cached token)
3. Rate limit check: 1ms (local)
4. Message validation: 1ms
5. Sequence assignment: 5ms (Redis)
6. Message write: 30ms (database)
7. Response: 5ms
Total: ~52ms typical

OPTIMIZATIONS APPLIED:
â€¢ Auth token cached in gateway
â€¢ Rate limit state in local memory
â€¢ Async delivery (not in critical path)
â€¢ Connection reuse (no handshake per message)

PATH 2: RECEIVE MESSAGE (Second most critical)
Message stored â†’ User sees it

Target: < 500ms P50

Steps:
1. Delivery service picks up: 10ms
2. Find user connections: 5ms
3. Route to WebSocket server: 10ms
4. Push to client: 20ms
5. Client renders: 50ms (client-side)
Total: ~95ms (excluding client)

PATH 3: LOAD CONVERSATION (User experience critical)
User opens chat â†’ Messages visible

Target: < 200ms P50

Steps:
1. Auth + conversation access check: 10ms
2. Query recent messages: 30ms (cached)
3. Fetch member info: 10ms (cached)
4. Response serialization: 5ms
5. Client render: 100ms (client-side)
Total: ~55ms server-side
```

## Caching Strategies

```
CACHE LAYER 1: Client-side
â€¢ Full message history for recent conversations
â€¢ User profiles for contacts
â€¢ Conversation list with snippets
â€¢ Media thumbnails

Benefits: 0ms latency for cached data
Invalidation: Server push or pull on open

CACHE LAYER 2: API Gateway (CDN edge)
â€¢ Static assets (avatars, stickers)
â€¢ Public media
â€¢ Not used for messages (personalized)

CACHE LAYER 3: Service Cache (Redis)
Cached:
â€¢ Conversation metadata: 60 second TTL
â€¢ User profiles: 300 second TTL
â€¢ Recent messages per conversation: 60 second TTL
â€¢ Group membership: 60 second TTL

Not cached:
â€¢ Unread counts (changes too frequently)
â€¢ Presence (separate system)
â€¢ Auth tokens (security concern)

CACHE LAYER 4: Database Query Cache
â€¢ Common query patterns cached at DB level
â€¢ Automatic invalidation on write
```

## Cache Invalidation

```
CONVERSATION CACHE INVALIDATION:

Trigger events:
â€¢ New message â†’ Invalidate "recent messages" cache
â€¢ Member change â†’ Invalidate "membership" cache
â€¢ Settings change â†’ Invalidate "metadata" cache

Strategy: Write-through with async invalidation
1. Write to database (synchronous)
2. Update cache (synchronous)
3. Broadcast invalidation to other cache nodes (async)

RACE CONDITION:
Read from stale cache while write in progress

Mitigation:
â€¢ Use cache-aside with short TTL
â€¢ Accept eventual consistency
â€¢ Critical reads bypass cache
```

## Precomputation vs Runtime Work

```
PRECOMPUTED:
â€¢ Unread count per conversation
  - Updated on every message write/read
  - Query: O(1) lookup
  - Without: O(messages) count at runtime

â€¢ Conversation list order (by last_message_time)
  - Updated on every message
  - Query: Sorted list, O(1)
  - Without: O(conversations) sort at runtime

â€¢ Group member list (denormalized)
  - Updated on membership change
  - Query: O(1) lookup
  - Without: O(members) join at runtime

COMPUTED AT RUNTIME:
â€¢ Search results (too variable to precompute)
â€¢ Media transcoding (done on upload, cached)
â€¢ Message formatting (done on client)

HYBRID:
â€¢ Read receipts aggregation
  - Individual receipts stored
  - Aggregated "3 people read" computed on query
  - Cached for 30 seconds
```

## Backpressure

```
WEBSOCKET GATEWAY BACKPRESSURE:

Monitor:
â€¢ Pending message queue per connection
â€¢ If queue > 1000 messages: Connection is slow

Response:
1. Stop queuing new messages for this connection
2. Drop cosmetic messages (typing, presence)
3. Keep message delivery in separate priority queue
4. If sustained: Close connection, force resync

MESSAGE STORE BACKPRESSURE:

Monitor:
â€¢ Write latency > threshold
â€¢ Queue depth growing

Response:
1. Start rejecting non-critical writes
2. Return "server busy" to clients (429)
3. Clients back off with exponential retry
4. Shed read traffic to replicas
```

## Load Shedding

```
PRIORITY LEVELS:
P0: Message send ACK (never shed)
P1: Message delivery (shed only in emergency)
P2: Message history reads (shed before delivery)
P3: Typing indicators (shed early)
P4: Analytics events (always shed first)

SHEDDING IMPLEMENTATION:

FUNCTION handle_request(request):
    priority = get_priority(request.type)
    load = get_current_load()
    
    IF load > CRITICAL_THRESHOLD:
        IF priority > P1:
            RETURN 503("Service Overloaded")
    
    IF load > HIGH_THRESHOLD:
        IF priority > P2:
            RETURN 503("Service Overloaded")
    
    // Process normally
    process(request)

HEADERS:
Retry-After: 5  // Tell client when to retry
X-Shed-Reason: load-shedding  // For debugging
```

## Optimizations NOT Done (and Why)

```
NOT DONE: Compress all messages
WHY: Most messages are tiny (< 100 bytes)
     Compression overhead > savings
     Only compress media

NOT DONE: Global deduplication of media
WHY: Privacy concerns (hash reveals same photo)
     Computation overhead
     Storage is cheap

NOT DONE: Predictive message prefetch
WHY: Hard to predict which conversation opens next
     Wastes bandwidth for wrong predictions
     Client cache sufficient

NOT DONE: Inline media in message payload
WHY: Delays message delivery for slow media
     Better UX: Show text immediately, load media async

NOT DONE: Real-time analytics
WHY: Hot path shouldn't feed analytics
     Sample or async pipeline instead
     Analytics can be delayed
```

---

# Part 11: Cost & Efficiency

## Major Cost Drivers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COST BREAKDOWN (at WhatsApp scale)                       â”‚
â”‚                                                                             â”‚
â”‚   1. MEDIA STORAGE (40% of cost)                                            â”‚
â”‚      â€¢ 30 PB/day new media                                                  â”‚
â”‚      â€¢ Hot storage: $0.023/GB/month                                         â”‚
â”‚      â€¢ 30 days hot: 900 PB Ã— $0.023 = $20M/month                            â”‚
â”‚                                                                             â”‚
â”‚   2. BANDWIDTH (25% of cost)                                                â”‚
â”‚      â€¢ Media download: 100 PB/day outbound                                  â”‚
â”‚      â€¢ At $0.05/GB: $5M/day = $150M/month                                   â”‚
â”‚      â€¢ With CDN: ~$50M/month                                                â”‚
â”‚                                                                             â”‚
â”‚   3. COMPUTE (20% of cost)                                                  â”‚
â”‚      â€¢ WebSocket servers: 10,000 servers                                    â”‚
â”‚      â€¢ Message services: 5,000 servers                                      â”‚
â”‚      â€¢ Supporting services: 5,000 servers                                   â”‚
â”‚      â€¢ Total: ~20,000 servers                                               â”‚
â”‚                                                                             â”‚
â”‚   4. DATABASE (10% of cost)                                                 â”‚
â”‚      â€¢ Message store: Petabytes SSD                                         â”‚
â”‚      â€¢ User/conversation metadata: Terabytes                                â”‚
â”‚      â€¢ Managed database services                                            â”‚
â”‚                                                                             â”‚
â”‚   5. THIRD-PARTY SERVICES (5% of cost)                                      â”‚
â”‚      â€¢ Push notification (APNs/FCM)                                         â”‚
â”‚      â€¢ CDN                                                                  â”‚
â”‚      â€¢ SMS verification                                                     â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## How Cost Scales with Traffic

```
LINEAR SCALING:
â€¢ Message storage: Each message costs storage
â€¢ Bandwidth: Each media download costs bandwidth
â€¢ Push notifications: Each notification has marginal cost

SUB-LINEAR SCALING:
â€¢ Fixed infrastructure: Doesn't scale with messages
â€¢ Caching: More traffic = better cache hit rate
â€¢ Batch operations: Larger batches = lower per-item cost

SUPER-LINEAR SCALING (Watch out!):
â€¢ Group messaging: 1000-member group = 1000x fanout
â€¢ Presence: N users Ã— M contacts = NÃ—M updates
â€¢ Search: Larger index = slower queries
```

### Cost Per Message Breakdown (Staff-Level Analysis)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           COST PER MESSAGE: DETAILED BREAKDOWN                              â”‚
â”‚                                                                             â”‚
â”‚   TEXT MESSAGE (100 bytes):                                                 â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚   Component                    Cost              Calculation                â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚   Storage (30 days hot)        $0.0000023        100B Ã— $0.023/GB/mo / 1B   â”‚
â”‚   Storage (1 year warm)        $0.0000007        100B Ã— $0.007/GB/mo / 1B   â”‚
â”‚   Compute (ingest)             $0.0000010        $0.10/M requests           â”‚
â”‚   Compute (delivery)           $0.0000010        $0.10/M requests           â”‚
â”‚   Push notification            $0.0000005        $0.05/100K notifications   â”‚
â”‚   Network (internal)           $0.0000002        100B Ã— $0.02/GB / 1B       â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚   TOTAL TEXT MESSAGE           $0.0000057        ~$5.70 per million         â”‚
â”‚                                                                             â”‚
â”‚   IMAGE MESSAGE (200KB):                                                    â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚   Storage (30 days)            $0.0046           200KB Ã— $0.023/GB/mo       â”‚
â”‚   CDN bandwidth                $0.0100           200KB Ã— $0.05/GB (egress)  â”‚
â”‚   Image processing             $0.0001           Thumbnail generation       â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚   TOTAL IMAGE MESSAGE          $0.0147           ~$14,700 per million       â”‚
â”‚                                                                             â”‚
â”‚   KEY INSIGHT:                                                              â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚   Image messages cost 2,500x more than text messages.                       â”‚
â”‚   Media is 30% of messages but 98% of infrastructure cost.                  â”‚
â”‚   Media optimization has highest ROI for cost reduction.                    â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

COST OPTIMIZATION PRIORITIES (ROI ordered):
1. Media compression and quality tiers: 50% reduction possible
2. Tiered storage for media: 30% reduction
3. CDN caching for popular media: 20% reduction
4. Message deduplication: Minimal (most messages unique)
5. Compute optimization: Diminishing returns below $1M/year

WHEN NOT TO OPTIMIZE:
â€¢ Don't optimize message storage for textâ€”it's <1% of cost
â€¢ Don't build custom compressionâ€”use off-the-shelf codecs
â€¢ Don't reduce replication below 3xâ€”durability is non-negotiable
â€¢ Don't cache messages in CDNâ€”personalized content, low hit rate
```

### Operational Cost as Design Constraint

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         TEAM OPERATIONAL BURDEN: THE HIDDEN COST                            â”‚
â”‚                                                                             â”‚
â”‚   INFRASTRUCTURE COST is visible in cloud bills.                            â”‚
â”‚   OPERATIONAL COST is invisible but often larger.                           â”‚
â”‚                                                                             â”‚
â”‚   ON-CALL BURDEN BY COMPONENT:                                              â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚   WebSocket Gateway:  High (connection storms, memory leaks)                â”‚
â”‚   Message Service:    Medium (straightforward but critical)                 â”‚
â”‚   Presence Service:   Low (eventual consistency hides issues)               â”‚
â”‚   Media Service:      High (transcoding failures, storage issues)           â”‚
â”‚   Push Gateway:       Medium (third-party dependencies)                     â”‚
â”‚                                                                             â”‚
â”‚   OPERATIONAL COST CALCULATION:                                             â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚   Example: Complex custom solution vs managed service                       â”‚
â”‚                                                                             â”‚
â”‚   Custom solution:                                                          â”‚
â”‚   â€¢ 5 engineers Ã— $300K/year = $1.5M/year                                   â”‚
â”‚   â€¢ On-call rotation (5 people, 50 weeks) = burnout                         â”‚
â”‚   â€¢ 2 AM pages: 20/month average                                            â”‚
â”‚                                                                             â”‚
â”‚   Managed service:                                                          â”‚
â”‚   â€¢ $500K/year service cost                                                 â”‚
â”‚   â€¢ 1 engineer for integration = $300K/year                                 â”‚
â”‚   â€¢ 2 AM pages: 2/month (their problem mostly)                              â”‚
â”‚                                                                             â”‚
â”‚   DECISION: Managed service saves $700K/year AND reduces burnout            â”‚
â”‚                                                                             â”‚
â”‚   DESIGN PRINCIPLES FOR LOW OPERATIONAL COST:                               â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚   1. Prefer stateless services (restart fixes most issues)                  â”‚
â”‚   2. Make state reconstructible (presence from connections)                 â”‚
â”‚   3. Use circuit breakers (failures don't page at 2 AM)                     â”‚
â”‚   4. Graceful degradation (typing indicators off â‰  outage)                  â”‚
â”‚   5. Clear ownership boundaries (one team per component)                    â”‚
â”‚                                                                             â”‚
â”‚   STAFF INSIGHT:                                                            â”‚
â”‚   A system that pages 5x/week is more expensive than one that               â”‚
â”‚   costs $1M more in infrastructure but pages 1x/month.                      â”‚
â”‚   Engineer time and morale are finite resources.                            â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Cost vs Reliability Trade-offs

```
TRADE-OFF 1: Replication Factor
â€¢ 3x replication: High durability, 3x storage cost
â€¢ 2x replication: Acceptable durability, 2x cost
â€¢ 1x replication: Risky, lowest cost
DECISION: 3x for messages (critical), 2x for media (can re-upload)

TRADE-OFF 2: Hot vs Cold Storage
â€¢ All hot: Fast access, expensive
â€¢ Tiered: Fast for recent, slow for old
â€¢ All cold: Cheap but slow
DECISION: Tier at 30 days (95% of reads are < 30 days)

TRADE-OFF 3: Push Notification Aggressiveness
â€¢ Push every message: Best UX, highest cost
â€¢ Batch pushes: Good UX, lower cost
â€¢ Push only for direct mentions: Okay UX, lowest cost
DECISION: Immediate for 1:1, batched for active groups

TRADE-OFF 4: Media Quality
â€¢ Original quality: Best UX, huge storage
â€¢ Aggressive compression: Lower quality, much smaller
â€¢ Resolution tiers: Balance
DECISION: Compress to "good enough" quality, offer original download
```

## What Over-Engineering Looks Like

```
OVER-ENGINEERING EXAMPLE 1:
"We need exactly-once delivery with global consensus"

Reality:
â€¢ At-least-once with client dedup is sufficient
â€¢ Global consensus adds 100ms+ latency
â€¢ Duplicates are rare and harmless
â€¢ Cost: 10x infrastructure for 0.01% edge case

OVER-ENGINEERING EXAMPLE 2:
"We need sub-10ms message delivery globally"

Reality:
â€¢ Physics limits: 100ms cross-continent
â€¢ Users don't perceive < 200ms differences
â€¢ Optimizing below 100ms requires edge compute
â€¢ Cost: 5x infrastructure for imperceptible improvement

OVER-ENGINEERING EXAMPLE 3:
"We need to store all messages forever in hot storage"

Reality:
â€¢ 99% of reads are last 7 days
â€¢ Users rarely access year-old messages
â€¢ Hot storage is 10x cost of cold
â€¢ Cost: 10x storage for 0.1% of reads
```

## Cost-Aware Redesign

```
REDESIGN: Media Storage Optimization

BEFORE:
â€¢ Store original + 3 thumbnail sizes
â€¢ All in hot storage
â€¢ 4x storage per image
â€¢ Cost: $80M/month

AFTER:
â€¢ Store original in hot for 7 days
â€¢ Generate thumbnails on-demand (cached)
â€¢ Move to cold after 7 days
â€¢ Regenerate thumbnails from cold if needed
â€¢ Cost: $30M/month

TRADE-OFF:
â€¢ Cold media access adds 500ms latency
â€¢ Users accept this for old photos
â€¢ Savings: $50M/month

REDESIGN: Presence Efficiency

BEFORE:
â€¢ Update presence every 10 seconds
â€¢ Broadcast to all contacts
â€¢ 100M users Ã— 100 contacts Ã— 6/min = 60B updates/min

AFTER:
â€¢ Update presence every 60 seconds
â€¢ Only broadcast to users with app open
â€¢ Lazy presence (query on conversation open)
â€¢ Updates: 500M/min (100x reduction)

TRADE-OFF:
â€¢ Presence up to 60 seconds stale
â€¢ Users don't notice
â€¢ Compute savings: 90%
```

---

# Part 12: Multi-Region & Global Considerations

## Data Locality

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA LOCALITY STRATEGY                                   â”‚
â”‚                                                                             â”‚
â”‚   PRINCIPLE: Keep conversation data close to participants                  â”‚
â”‚                                                                             â”‚
â”‚   USER DATA:                                                                â”‚
â”‚   â€¢ Stored in user's home region                                            â”‚
â”‚   â€¢ Determined by phone number prefix                                       â”‚
â”‚   â€¢ Can migrate if user moves (rare)                                        â”‚
â”‚                                                                             â”‚
â”‚   CONVERSATION DATA:                                                        â”‚
â”‚   â€¢ 1:1: Stored in region of user with most activity                       â”‚
â”‚   â€¢ Group: Stored in region with plurality of members                       â”‚
â”‚   â€¢ Cross-region conversation: Replicate to both regions                   â”‚
â”‚                                                                             â”‚
â”‚   EXAMPLE:                                                                  â”‚
â”‚   Alice (US) and Bob (US): Data in US                                       â”‚
â”‚   Alice (US) and Chen (Asia): Replicate US â†” Asia                          â”‚
â”‚   Group with 8 US, 2 EU: Data in US, async replicate to EU                 â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Replication Strategies

```
MESSAGE REPLICATION:

SYNCHRONOUS (within region):
â€¢ Primary write + 2 replicas in same region
â€¢ Ensures durability before ACK
â€¢ Latency: +5-10ms

ASYNCHRONOUS (cross-region):
â€¢ After ACK, replicate to other regions
â€¢ Disaster recovery (not active use)
â€¢ Lag: 100ms - 5 seconds typical

USER DATA REPLICATION:
â€¢ Master in home region
â€¢ Read replicas in other regions
â€¢ Writes always go to home region

CONVERSATION METADATA:
â€¢ Sync replicated within region (strong consistency)
â€¢ Async replicated cross-region (eventual consistency)
```

## Traffic Routing

```
ROUTING LOGIC:

USER CONNECTS:
1. DNS returns nearest edge location
2. Edge handles WebSocket connection
3. Requests routed to user's home region

MESSAGE SEND:
1. Accept at any region (where user is connected)
2. Forward to conversation's home region
3. Store and assign sequence there
4. Fan out delivery from there

CROSS-REGION MESSAGE:
Alice (US) sends to Bob (EU):
1. Alice's client â†’ US edge
2. US edge â†’ US message service (store)
3. US â†’ EU (async replication for Bob's home region)
4. Delivery service â†’ Bob via EU edge

OPTIMIZATION:
If Bob is online at EU edge:
â€¢ Push message directly via EU
â€¢ Don't wait for replication
â€¢ Mark as "delivered but may not be in EU replica yet"
```

## Failure Across Regions

```
SINGLE REGION FAILURE:

Scenario: US-West completely down

Impact:
â€¢ Users whose home is US-West: Degraded service
â€¢ Users in other regions: Minor impact (some contacts unavailable)

Response:
1. DNS removes US-West from rotation
2. US-West users routed to US-East (higher latency)
3. US-East becomes primary for US-West data
4. Catch up from replicas
5. When US-West recovers: Sync back

Message handling:
â€¢ Messages to US-West users queue in other regions
â€¢ Messages from US-West users sent via reroute
â€¢ Some latency, no message loss

NETWORK PARTITION:

Scenario: US-West and US-East cannot communicate

Impact:
â€¢ Split-brain potential
â€¢ Users in each region only see users in their region

Response:
1. Each region operates independently
2. Cross-region messages queue
3. When partition heals: Merge messages
4. Conflict resolution: Later timestamp wins

Conflict example:
â€¢ During partition, Alice (US-West) and Bob (US-East) both delete same group
â€¢ Partition heals
â€¢ Resolution: Keep one delete, apply to both
```

## When Multi-Region Is NOT Worth It

```
SKIP MULTI-REGION IF:
â€¢ All users in one geography
â€¢ < 1M users (cost doesn't justify)
â€¢ Can tolerate 30-minute failover RTO
â€¢ No regulatory data residency requirements

MULTI-REGION COSTS:
â€¢ 2-3x infrastructure (replicate everything)
â€¢ 2-3x operational complexity
â€¢ Cross-region bandwidth charges
â€¢ More complex debugging

SIMPLER ALTERNATIVE:
â€¢ Single region with good backups
â€¢ Automated restore to new region
â€¢ RTO: 30 minutes to 4 hours
â€¢ Cost: Much lower

WHEN IT IS WORTH IT:
â€¢ Global user base
â€¢ < 1 minute RTO requirement
â€¢ Data residency requirements (EU data in EU)
â€¢ > 10M users
```

---

# Part 13: Security & Abuse Considerations

## Abuse Vectors

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ABUSE VECTORS AND MITIGATIONS                            â”‚
â”‚                                                                             â”‚
â”‚   SPAM:                                                                     â”‚
â”‚   â€¢ Mass messaging to strangers                                             â”‚
â”‚   â€¢ Group join spam                                                         â”‚
â”‚   â€¢ Media/link spam                                                         â”‚
â”‚   Mitigation:                                                               â”‚
â”‚   â€¢ Rate limit messages to non-contacts                                     â”‚
â”‚   â€¢ Rate limit group joins                                                  â”‚
â”‚   â€¢ Link/media scanning                                                     â”‚
â”‚   â€¢ ML-based spam detection                                                 â”‚
â”‚                                                                             â”‚
â”‚   HARASSMENT:                                                               â”‚
â”‚   â€¢ Repeated unwanted messages                                              â”‚
â”‚   â€¢ Account creation to bypass blocks                                       â”‚
â”‚   â€¢ Group invite harassment                                                 â”‚
â”‚   Mitigation:                                                               â”‚
â”‚   â€¢ Easy blocking                                                           â”‚
â”‚   â€¢ Phone number ban (not just account)                                     â”‚
â”‚   â€¢ Require mutual contact for DMs                                          â”‚
â”‚                                                                             â”‚
â”‚   ILLEGAL CONTENT:                                                          â”‚
â”‚   â€¢ CSAM                                                                    â”‚
â”‚   â€¢ Terrorism-related content                                               â”‚
â”‚   â€¢ Copyright infringement                                                  â”‚
â”‚   Mitigation:                                                               â”‚
â”‚   â€¢ PhotoDNA/hashing for known illegal content                              â”‚
â”‚   â€¢ AI detection for new content                                            â”‚
â”‚   â€¢ Human review pipeline                                                   â”‚
â”‚   â€¢ Law enforcement cooperation                                             â”‚
â”‚                                                                             â”‚
â”‚   FRAUD:                                                                    â”‚
â”‚   â€¢ Impersonation                                                           â”‚
â”‚   â€¢ Phishing links                                                          â”‚
â”‚   â€¢ Scam messages                                                           â”‚
â”‚   Mitigation:                                                               â”‚
â”‚   â€¢ Verified badges for businesses                                          â”‚
â”‚   â€¢ Link safety warnings                                                    â”‚
â”‚   â€¢ Report and review pipeline                                              â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Rate Abuse

```
RATE LIMITS BY CATEGORY:

Messages to contacts: 100/minute
Messages to non-contacts: 10/minute
Group creations: 10/day
Group joins: 50/day
Media uploads: 50/day, 1GB/day
Profile changes: 10/day

ABUSE PATTERNS:

Pattern: Burst of messages to 1000 users
Detection: > 100 unique recipients in 1 hour
Response: Rate limit, flag for review

Pattern: Scripted message sending
Detection: Perfectly regular intervals
Response: CAPTCHA challenge, temporary block

Pattern: Bot creating groups
Detection: Groups with same pattern (name, avatar)
Response: Account suspension
```

## Data Exposure Risks

```
RISK 1: Message Leaked to Wrong Recipient
Cause: Bug in routing logic
Impact: Critical privacy violation
Mitigation:
â€¢ Access control check at multiple layers
â€¢ Conversation membership verified on every operation
â€¢ Audit logging of all message access
â€¢ Regular security testing

RISK 2: Metadata Exposure
Cause: Logs containing message content
Impact: Privacy violation, potential legal issues
Mitigation:
â€¢ Never log message content
â€¢ Minimal metadata in logs
â€¢ Log retention limits
â€¢ Access controls on logs

RISK 3: Insider Access
Cause: Employee accesses user messages
Impact: Trust violation, legal issues
Mitigation:
â€¢ End-to-end encryption (even staff can't read)
â€¢ Access audit logs
â€¢ Principle of least privilege
â€¢ Background checks

RISK 4: API Leaking Data
Cause: API returns more than needed
Impact: Data exposure
Mitigation:
â€¢ Minimal response payloads
â€¢ Authorization on every field
â€¢ Regular API audits
```

## Privilege Boundaries

```
ACCESS CONTROL MODEL:

USER PRIVILEGES:
â€¢ Read own messages
â€¢ Send to conversations they're member of
â€¢ Manage own profile
â€¢ Block other users

GROUP ADMIN PRIVILEGES:
â€¢ Add/remove members
â€¢ Change group settings
â€¢ Delete any message in group
â€¢ Promote other admins

SYSTEM PRIVILEGES:
â€¢ Never access message content
â€¢ Can access metadata for:
  - Abuse investigation (with approval)
  - Law enforcement (with legal process)
  - Technical debugging (anonymized)

PRINCIPLE OF LEAST PRIVILEGE:
â€¢ Engineers cannot access production user data
â€¢ Debugging uses synthetic data
â€¢ Access to user data requires approval chain
â€¢ All access logged and audited
```

## Why Perfect Security Is Impossible

```
FUNDAMENTAL TENSIONS:

1. Usability vs Security
   â€¢ Perfect security: 20-character password, 2FA every time
   â€¢ Users want: Quick access, "remember this device"
   â€¢ Compromise: Risk-based authentication

2. Privacy vs Abuse Prevention
   â€¢ Perfect privacy: E2EE, no server-side inspection
   â€¢ Abuse prevention: Need to scan for illegal content
   â€¢ Compromise: Client-side scanning, or accept some abuse

3. Availability vs Security
   â€¢ Perfect security: Offline verification of everything
   â€¢ Availability: Need to work when networks are spotty
   â€¢ Compromise: Cache auth tokens, accept staleness

4. Debugging vs Privacy
   â€¢ Perfect privacy: No logs, no tracing
   â€¢ Debugging: Need to understand failures
   â€¢ Compromise: Anonymized logs, minimal retention

STAFF INSIGHT:
Security is always a trade-off. The goal is not perfect
security but appropriate security for the threat model.
Messaging has high privacy expectationsâ€”lean toward
privacy when in doubt, accept some operational complexity.
```

---

# Part 14: Evolution Over Time

## V1: Naive Design

```
INITIAL DESIGN (Startup scale: 10K users)

Architecture:
â€¢ Single server
â€¢ MySQL database
â€¢ Long polling for real-time
â€¢ Local file storage for media

Data model:
â€¢ messages table (id, sender, recipient, content, timestamp)
â€¢ users table (id, name, phone)
â€¢ No conversations abstraction

Delivery:
â€¢ Poll database every 2 seconds for new messages
â€¢ Show immediately on poll
â€¢ No delivery receipts

What works at 10K users:
â€¢ Simple to understand and debug
â€¢ Single source of truth
â€¢ All features work

What's already straining:
â€¢ Long polling creates many connections
â€¢ Database scanned on every poll
â€¢ No offline message queue
```

## What Breaks First

```
AT 100K USERS:
â€¢ Long polling: Too many connections
â€¢ Database: Query per poll kills performance
â€¢ Media: Local disk running out

FIX:
â€¢ Migrate to WebSocket (reduce connections 10x)
â€¢ Add message index by recipient
â€¢ Move media to S3

AT 1M USERS:
â€¢ Single database: Write capacity maxed
â€¢ Single server: Cannot handle connections
â€¢ Message ordering: Race conditions appearing

FIX:
â€¢ Shard database by user_id
â€¢ Multiple WebSocket servers
â€¢ Add Redis for sequence numbers

AT 10M USERS:
â€¢ Cross-shard queries: Killing performance
â€¢ Group messaging: Fan-out overwhelming
â€¢ Presence: Every status change storms

FIX:
â€¢ Denormalize inbox (fan-out on write)
â€¢ Async group delivery with queues
â€¢ Presence becomes eventually consistent
```

## V2: Improved Design

```
ARCHITECTURE (Scale: 100M users)

Components:
â€¢ WebSocket gateway cluster (100 servers)
â€¢ Message service (50 servers)
â€¢ Delivery service (50 servers)
â€¢ Presence service (Redis cluster)
â€¢ Message store (Cassandra cluster)
â€¢ Media store (S3)
â€¢ Cache layer (Redis)

Improvements over V1:
â€¢ Proper WebSocket for real-time
â€¢ Async delivery pipeline
â€¢ Conversation abstraction
â€¢ Delivery receipts
â€¢ Presence (eventually consistent)
â€¢ Multi-region awareness

Still problematic:
â€¢ Large groups are slow
â€¢ Global ordering not guaranteed
â€¢ Sync for long-offline users is slow
â€¢ Hot conversations create hotspots
```

### Team Ownership Boundaries and Coordination

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           TEAM STRUCTURE FOR MESSAGING PLATFORM                             â”‚
â”‚                                                                             â”‚
â”‚   TEAM 1: Message Core (6-8 engineers)                                      â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚   Owns: Message Service, Message Store, Sync Service                        â”‚
â”‚   Responsibility: Message lifecycle from send to storage                    â”‚
â”‚   On-call: Message delivery failures, storage issues                        â”‚
â”‚   SLO: 99.99% message durability, <100ms write latency                      â”‚
â”‚                                                                             â”‚
â”‚   TEAM 2: Real-Time Delivery (5-6 engineers)                                â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚   Owns: WebSocket Gateway, Delivery Service, Push Gateway                   â”‚
â”‚   Responsibility: Getting messages to online/offline users                  â”‚
â”‚   On-call: Connection issues, push notification failures                    â”‚
â”‚   SLO: 99.9% delivery within 1 second for online users                      â”‚
â”‚                                                                             â”‚
â”‚   TEAM 3: Presence & Signals (3-4 engineers)                                â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚   Owns: Presence Service, Typing Indicators, Read Receipts                  â”‚
â”‚   Responsibility: Ephemeral real-time signals                               â”‚
â”‚   On-call: Low-priority (graceful degradation built in)                     â”‚
â”‚   SLO: Best-effort (no paging for presence outages)                         â”‚
â”‚                                                                             â”‚
â”‚   TEAM 4: Media & Storage (4-5 engineers)                                   â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚   Owns: Media Service, CDN Integration, Storage Tiering                     â”‚
â”‚   Responsibility: Media upload, processing, delivery, lifecycle             â”‚
â”‚   On-call: Media processing failures, storage capacity                      â”‚
â”‚   SLO: 99.9% media availability, <5 seconds upload latency                  â”‚
â”‚                                                                             â”‚
â”‚   TEAM 5: Groups & Social (4-5 engineers)                                   â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚   Owns: Group Service, User Service, Contacts, Blocking                     â”‚
â”‚   Responsibility: Social graph and group management                         â”‚
â”‚   On-call: Group permission issues, block enforcement failures              â”‚
â”‚   SLO: 99.99% for membership checks (security-critical)                     â”‚
â”‚                                                                             â”‚
â”‚   TEAM 6: Platform & Infrastructure (5-6 engineers)                         â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚   Owns: API Gateway, Rate Limiting, Auth, Monitoring, Deployment            â”‚
â”‚   Responsibility: Cross-cutting concerns, platform stability                â”‚
â”‚   On-call: Deployment issues, monitoring gaps, auth failures                â”‚
â”‚   SLO: Platform-wide availability SLO                                       â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

CROSS-TEAM COORDINATION POINTS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Feature               Teams Involved         Coordination Needed
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Message send flow     Core + Delivery        Interface contract, latency budget
Large group changes   Groups + Core          Fanout strategy changes
New message type      Core + Media + Clients Schema evolution, rollout
Push notification     Delivery + Platform    Token lifecycle, rate limits
E2E encryption        All teams              Key management, schema changes
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

INTERFACE CONTRACTS:
â€¢ Each team owns a service with defined API contract
â€¢ Breaking changes require cross-team review
â€¢ Deprecation: 6 weeks notice minimum
â€¢ Monitoring: Each team owns dashboards for their services

ESCALATION PATH:
Page â†’ Team on-call â†’ Team lead â†’ Messaging TL â†’ Product area VP
Postmortem required for any incident lasting > 30 minutes
```

### Zero-Downtime Migration: Fan-Out on Write to Fan-Out on Read

```
CONTEXT: Large groups (>1000 members) need to migrate from FOW to FOR
This is a live migration with 100B messages/day flowing through

PHASE 1: DUAL-WRITE (Week 1-2)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Configuration: fow_enabled=true, for_enabled=true for target groups

FUNCTION send_to_large_group(message, group):
    // Write message to group message table (FOR)
    store_group_message(message, group.id)
    
    // ALSO fan out to member inboxes (FOW) - for safety
    FOR member IN group.members:
        store_inbox_message(message, member.id)
    
    // Both paths populated, reads still use inbox (FOW)

Validation:
â€¢ Compare message counts: group_messages vs sum(inbox_messages)
â€¢ Latency comparison: FOR write vs FOW write
â€¢ No user-visible changes yet

PHASE 2: DUAL-READ (Week 3-4)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Configuration: read_from_for=shadow for target groups

FUNCTION get_group_messages(user, group, cursor):
    // Primary: Still read from inbox (FOW)
    inbox_messages = query_inbox(user.id, group.id, cursor)
    
    // Shadow: Also read from group table (FOR)
    group_messages = query_group_messages(group.id, cursor)
    
    // Log differences for analysis
    compare_and_log(inbox_messages, group_messages)
    
    // Return inbox (proven correct)
    RETURN inbox_messages

Validation:
â€¢ Shadow read latency must be faster
â€¢ Message content must match exactly
â€¢ Ordering must match

PHASE 3: READ SWITCH (Week 5-6)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Configuration: read_from_for=true, still dual-write

FUNCTION get_group_messages(user, group, cursor):
    // Now read from group table (FOR)
    group_messages = query_group_messages(group.id, cursor)
    
    // Shadow: Read from inbox to verify
    inbox_messages = query_inbox(user.id, group.id, cursor)
    compare_and_log(inbox_messages, group_messages)
    
    RETURN group_messages

Rollback: If errors > 0.1%, revert read_from_for=false instantly

PHASE 4: STOP FOW WRITES (Week 7-8)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Configuration: fow_enabled=false for target groups

FUNCTION send_to_large_group(message, group):
    // Only write to group message table (FOR)
    store_group_message(message, group.id)
    // No more inbox writes for large groups

PHASE 5: CLEANUP (Week 9+)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Stop shadow reads
â€¢ Background job to delete old inbox entries for migrated groups
â€¢ Reclaim storage (may be petabytes)

ROLLBACK PLAN AT EACH PHASE:
Phase 1: Turn off for_enabled â†’ immediate
Phase 2: Turn off shadow reads â†’ immediate  
Phase 3: Set read_from_for=false â†’ reads revert, still have data
Phase 4: Re-enable fow_enabled â†’ messages written to both
Phase 5: Cannot rollback (data deleted), but shouldn't need to

STAFF INSIGHT:
Migrations at scale are multi-week endeavors with rollback at every step.
The dual-write/dual-read pattern is the safest approach.
Never delete old data until new path is proven for weeks.
```

## Long-Term Stable Architecture

```
MATURE ARCHITECTURE (Scale: 1B+ users)

Core principles:
1. Separate hot and cold paths
2. Eventual consistency by default, strong where needed
3. Cell-based architecture for isolation
4. Graceful degradation baked in

Components evolved:
â€¢ Cells: Self-contained units (10M users each)
â€¢ Cross-cell routing for conversations spanning cells
â€¢ Tiered storage (hot/warm/cold)
â€¢ Sophisticated presence (subscription-based)
â€¢ E2E encryption (client-side)
â€¢ ML-based abuse detection

Operational maturity:
â€¢ Canary deployments
â€¢ Automatic rollback on error spike
â€¢ Chaos engineering
â€¢ Runbook automation
â€¢ 24/7 on-call with clear escalation

What's stable:
â€¢ Core message flow unchanged for years
â€¢ Data model rarely changes
â€¢ Most work is optimization, not redesign
```

## How Incidents Drive Redesign

```
INCIDENT 1: New Year's Eve Outage

What happened:
â€¢ 50x traffic spike at midnight (by timezone)
â€¢ Message queue backed up
â€¢ Users saw "sending..." for minutes
â€¢ Cascaded across regions

Root cause:
â€¢ Queue capacity sized for 10x, not 50x
â€¢ No backpressure to slow senders
â€¢ Retry storms made it worse

Redesign:
â€¢ Backpressure from queue to API
â€¢ Client-side queuing with local retry
â€¢ Queue capacity 100x average
â€¢ Graceful degradation (disable typing, etc.)

INCIDENT 2: Celebrity Account Storm

What happened:
â€¢ Celebrity posted controversy
â€¢ 10M users tried to message them
â€¢ All messages routed to one shard
â€¢ Shard overloaded, affected other users on shard

Root cause:
â€¢ Sharding by recipient concentrated hot users
â€¢ No rate limiting on inbound to public figures
â€¢ No isolation between accounts

Redesign:
â€¢ Special handling for high-volume recipients
â€¢ Inbound rate limits (queue excess)
â€¢ Better shard isolation
â€¢ "Fan-out on read" for celebrity inboxes
```

---

# Part 15: Alternatives & Explicit Rejections

## Alternative 1: Pure P2P (No Central Server)

```
APPROACH:
â€¢ Messages sent directly between devices
â€¢ No central server for routing
â€¢ Device-to-device encryption built-in

WHY IT SEEMS ATTRACTIVE:
â€¢ Perfect privacy (no server access)
â€¢ Lower infrastructure cost
â€¢ No central point of failure
â€¢ Simple mental model

WHY STAFF ENGINEERS REJECT IT:
â€¢ Offline delivery: How to reach offline users?
  - Need some always-on component
â€¢ NAT traversal: Devices behind firewalls
  - Need STUN/TURN servers anyway
â€¢ Multi-device sync: How to sync phone â†” laptop?
  - Need central sync service
â€¢ Group messaging: NÂ² connections don't scale
  - Need central relay
â€¢ Abuse prevention: Can't moderate P2P
  - Platform becomes spam haven

VERDICT: Pure P2P doesn't work for mass-market messaging.
         End up building central services anyway.
         Better to design for server-assisted from start.
```

## Alternative 2: Blockchain-Based Messaging

```
APPROACH:
â€¢ Messages stored on blockchain
â€¢ Decentralized, censorship-resistant
â€¢ Cryptocurrency for spam prevention

WHY IT SEEMS ATTRACTIVE:
â€¢ "Web3" buzzword appeal
â€¢ Censorship resistance
â€¢ User ownership of data
â€¢ Built-in payment rails

WHY STAFF ENGINEERS REJECT IT:
â€¢ Latency: Block time = message delivery time
  - Even fast chains: 1-15 seconds per block
  - Unacceptable for real-time messaging
â€¢ Cost: Pay per message (gas fees)
  - $0.01/message Ã— 100B messages/day = $1B/day
â€¢ Privacy: Blockchain is public
  - Everyone can see message metadata
â€¢ Scale: Blockchains don't scale
  - Even optimistic: 10K TPS vs our 10M TPS
â€¢ Complexity: Wallets, keys, etc.
  - Mass market won't manage private keys

VERDICT: Blockchain adds cost, latency, and complexity
         with no real benefit for messaging use case.
         Censorship resistance not needed by 99% of users.
```

## Alternative 3: Actor Model (Erlang/Elixir Style)

```
APPROACH:
â€¢ Each user is an actor process
â€¢ Messages passed between actors
â€¢ Platform built on Erlang/BEAM VM

WHY IT SEEMS ATTRACTIVE:
â€¢ Natural fit for messaging domain
â€¢ Built-in fault tolerance
â€¢ Lightweight processes (millions per server)
â€¢ WhatsApp famously uses Erlang

WHY STAFF ENGINEERS ARE CAUTIOUS:
â€¢ Hiring: Erlang engineers are rare
  - Harder to scale engineering team
â€¢ Debugging: Actor systems are hard to trace
  - Distributed state is opaque
â€¢ Persistence: Still need external database
  - Actors are in-memory only
â€¢ Libraries: Ecosystem smaller than JVM/Go
  - May need to build more from scratch
â€¢ Migration: Hard to move from existing systems
  - Big-bang rewrite needed

NUANCED VERDICT:
â€¢ For greenfield: Actor model can work well
â€¢ For existing system: Migration cost too high
â€¢ Staff choice: Use actor model concepts, implement in Go/Java
  - Get benefits without platform commitment
```

---

# Part 16: Interview Calibration

## How Interviewers Probe This System

```
OPENER: "Design a messaging platform like WhatsApp"

EXPECTED FIRST QUESTIONS (L6 candidates ask these):
â€¢ "What's the expected scale? Users, messages per day?"
â€¢ "1:1 messaging, group messaging, or both?"
â€¢ "What are the latency expectations?"
â€¢ "Any regulatory/compliance requirements?"
â€¢ "Is E2E encryption in scope?"

RED FLAG FIRST QUESTIONS (L5 candidates ask these):
â€¢ "Should I use Kafka?" (jumping to implementation)
â€¢ "What database should I use?" (jumping to details)
â€¢ Starting to draw without any clarification

COMMON PROBES:
â€¢ "How do you ensure message ordering?"
â€¢ "What happens when a user is offline?"
â€¢ "How do you handle a 1000-person group?"
â€¢ "What if the database goes down?"
â€¢ "How do you prevent spam?"
â€¢ "Walk me through the delivery of a single message"
```

## Common L5 Mistakes

```
MISTAKE 1: Designing for Precision Instead of Scale
L5: "We use 2PC to ensure exactly-once delivery"
Problem: 2PC adds latency and complexity
Staff: "At-least-once with idempotent receivers is sufficient"

MISTAKE 2: Ignoring Offline Users
L5: Shows only WebSocket path
Problem: 40% of users are offline at any time
Staff: "Here's the offline queue and sync mechanism"

MISTAKE 3: Total Ordering
L5: "Single queue for all messages ensures ordering"
Problem: Single queue doesn't scale
Staff: "Per-conversation ordering with logical clocks"

MISTAKE 4: Underestimating Group Complexity
L5: "Send to each member like 1:1"
Problem: 1000-member group = 1000x fanout
Staff: "Different strategy based on group size"

MISTAKE 5: Treating All Messages Equal
L5: "All messages go through same path"
Problem: Media is 1000x larger than text
Staff: "Separate paths for text, media, signals"
```

## Staff-Level Answers

```
ON ORDERING:
"Message ordering is complex. Within a conversation, we need
per-sender ordering as a minimumâ€”messages from Alice to Bob
should appear in the order Alice sent them. For cross-sender
ordering, we use Lamport timestamps to establish causality.
The key insight is that users only care about ordering within
a single conversation viewâ€”we don't need global ordering."

ON OFFLINE HANDLING:
"Offline users are actually the common case, not the exception.
When a message arrives for an offline user, we store it in their
inbox and send a push notification. On reconnect, the client
provides its last sync cursor, and we return all messages since
then. The tricky part is handling long-offline usersâ€”we paginate
and prioritize recent conversations."

ON LARGE GROUPS:
"Large groups require different fanout strategies. For a 10-person
group, fan-out on write works fineâ€”store one message, deliver to
10 people. For a 10,000-person group, that's 10,000 deliveries
per message. Instead, we use fan-out on read: store the message
once, and each member fetches it when they open the conversation.
The threshold is around 50-100 members."

ON FAILURE:
"The non-negotiable is: never lose a message after ACK. Everything
else can degrade. During a partial outage, we'll continue to accept
and store messagesâ€”delivery might be delayed. We prioritize message
storage over delivery, and delivery over cosmetic features like
typing indicators. Users can handle a few seconds of delay; they
can't handle lost messages."
```

## Example Phrases Staff Engineers Use

```
SCOPE DEFINITION:
â€¢ "Let's define what 'delivered' means in this context..."
â€¢ "I'm going to explicitly exclude E2E encryption for now..."
â€¢ "The interesting trade-off here is between X and Y..."

TRADE-OFF ARTICULATION:
â€¢ "We could do strong consistency here, but the latency cost is..."
â€¢ "I'm choosing eventual consistency because..."
â€¢ "The risk of this approach is X, which we mitigate by Y..."

SHOWING DEPTH:
â€¢ "In my experience, what actually breaks first is..."
â€¢ "The non-obvious problem with that approach is..."
â€¢ "At first glance you might do X, but actually Y is better because..."

HANDLING UNCERTAINTY:
â€¢ "I'm not sure about the exact numbers, but order of magnitude..."
â€¢ "This would need benchmarking, but my intuition says..."
â€¢ "Let me reason through thisâ€”if we assume X, then..."

DRIVING DISCUSSION:
â€¢ "Let me walk you through the happy path first, then failures..."
â€¢ "I'll sketch the architecture, then deep-dive on [component]..."
â€¢ "Before I go further, should I elaborate on X or move to Y?"
```

### Google L6 Interview Calibration: Deep Signals

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         WHAT THE INTERVIEWER IS LOOKING FOR                                 â”‚
â”‚                                                                             â”‚
â”‚   SIGNAL 1: Problem Decomposition (First 5 minutes)                         â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚   L5: Jumps to database choice or message queue                             â”‚
â”‚   L6: Asks clarifying questions, identifies the REAL hard problems          â”‚
â”‚       "The core challenge isn't sending messagesâ€”it's ordering guarantees   â”‚
â”‚        at scale and handling the offline-to-online transition."             â”‚
â”‚                                                                             â”‚
â”‚   L6 SIGNAL PHRASES:                                                        â”‚
â”‚   â€¢ "Before I design, let me understand the consistency requirements..."    â”‚
â”‚   â€¢ "The offline case is actually more interesting than the online case..." â”‚
â”‚   â€¢ "Group messaging fundamentally changes the fanout calculus..."          â”‚
â”‚                                                                             â”‚
â”‚   SIGNAL 2: Failure-First Thinking (Throughout)                             â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚   L5: Designs happy path, adds failure handling when asked                  â”‚
â”‚   L6: Incorporates failure modes into initial design                        â”‚
â”‚       "I'm using at-least-once with client dedup because exactly-once       â”‚
â”‚        requires 2PC which adds unacceptable latency for messaging."         â”‚
â”‚                                                                             â”‚
â”‚   L6 SIGNAL PHRASES:                                                        â”‚
â”‚   â€¢ "When this failsâ€”and it willâ€”here's what happens..."                    â”‚
â”‚   â€¢ "I'm designing the degraded mode first, then optimizing the happy path" â”‚
â”‚   â€¢ "The blast radius of this failure is contained to..."                   â”‚
â”‚                                                                             â”‚
â”‚   SIGNAL 3: Scale Intuition (Unprompted)                                    â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚   L5: Uses "at scale" vaguely, doesn't quantify                             â”‚
â”‚   L6: Provides order-of-magnitude estimates, identifies bottlenecks         â”‚
â”‚       "At 100M concurrent connections, the WebSocket gateway needs          â”‚
â”‚        roughly 1000 servers assuming 100K connections per server.           â”‚
â”‚        Connection lifecycle, not message volume, is the bottleneck."        â”‚
â”‚                                                                             â”‚
â”‚   L6 SIGNAL PHRASES:                                                        â”‚
â”‚   â€¢ "Let me do a quick back-of-envelope calculation..."                     â”‚
â”‚   â€¢ "This scales linearly until X, then we need a different approach..."    â”‚
â”‚   â€¢ "The dangerous assumption here is that [specific assumption]..."        â”‚
â”‚                                                                             â”‚
â”‚   SIGNAL 4: Cost Awareness (Often missed)                                   â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚   L5: Ignores cost, designs for correctness only                            â”‚
â”‚   L6: Treats cost as a design constraint, identifies waste                  â”‚
â”‚       "Media is 98% of our infrastructure cost despite being 30% of         â”‚
â”‚        messages. That's where I'd focus optimization effort."               â”‚
â”‚                                                                             â”‚
â”‚   L6 SIGNAL PHRASES:                                                        â”‚
â”‚   â€¢ "This is over-engineering for our current scale..."                     â”‚
â”‚   â€¢ "The cost scales as O(nÂ²) which becomes prohibitive at..."              â”‚
â”‚   â€¢ "I'd explicitly NOT build X because the value doesn't justify..."       â”‚
â”‚                                                                             â”‚
â”‚   SIGNAL 5: Organizational Awareness                                        â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚   L5: Designs as if one team builds everything                              â”‚
â”‚   L6: Considers team boundaries, on-call burden, migration paths            â”‚
â”‚       "I'd split this into three services with clear ownership: Message     â”‚
â”‚        Core owns durability, Delivery owns real-time, Presence is           â”‚
â”‚        separate because it can degrade without affecting core function."    â”‚
â”‚                                                                             â”‚
â”‚   L6 SIGNAL PHRASES:                                                        â”‚
â”‚   â€¢ "This is complex enough that I'd want two teams owning it..."           â”‚
â”‚   â€¢ "The on-call burden of this design is manageable because..."            â”‚
â”‚   â€¢ "To migrate to this, we'd need a dual-write period of..."               â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

COMMON L5 MISTAKES IN MESSAGING SYSTEM DESIGN:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

MISTAKE 1: "Use Kafka for messages"
WHY IT'S WRONG: Kafka is for event streaming, not user messaging.
               Different retention, ordering, and access patterns.
L6 RESPONSE:  "Kafka could work for internal delivery queues, but user
              messages need a different storage modelâ€”per-conversation
              ordering with efficient random access."

MISTAKE 2: "Just use Redis for everything"
WHY IT'S WRONG: Redis is in-memory. Message data is durable.
               100B messages Ã— 500 bytes = 50TB. Doesn't fit.
L6 RESPONSE:  "Redis is perfect for ephemeral data like presence and
              caching. Messages go to a write-optimized store like
              Cassandra with appropriate replication."

MISTAKE 3: "Exactly-once delivery with transactions"
WHY IT'S WRONG: Exactly-once across distributed systems is expensive.
               Messaging tolerates duplicates better than latency.
L6 RESPONSE:  "At-least-once with idempotent receivers is sufficient.
              Client-side deduplication is cheap. The 0.01% duplicate
              rate is acceptable, the 100ms latency from 2PC is not."

MISTAKE 4: "Poll for new messages"
WHY IT'S WRONG: Polling at scale is N users Ã— 1 poll/sec = N req/sec
               for mostly empty responses. Wasteful.
L6 RESPONSE:  "WebSocket connections with server push. Yes, it's stateful,
              but the alternative is 10x the server load. The statefulness
              is manageable because connection state is recoverable."

MISTAKE 5: "Single sequence number for all messages"
WHY IT'S WRONG: Single sequence = single point of contention.
               Also, cross-conversation ordering is meaningless.
L6 RESPONSE:  "Per-conversation sequence with Lamport timestamps for
              causality. Users only see one conversation at a time,
              so cross-conversation ordering is irrelevant."
```

---

# Part 17: Diagrams

## Diagram 1: Core Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MESSAGING PLATFORM ARCHITECTURE                          â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚                          CLIENTS                                    â”‚    â”‚
â”‚   â”‚          iOS    Android    Web    Desktop    Tablet                â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â”‚                                              â”‚
â”‚                              â–¼                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚                       EDGE LAYER                                    â”‚    â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚    â”‚
â”‚   â”‚   â”‚   API    â”‚    â”‚WebSocket â”‚    â”‚   Push   â”‚                     â”‚    â”‚
â”‚   â”‚   â”‚ Gateway  â”‚    â”‚ Gateway  â”‚    â”‚ Gateway  â”‚                     â”‚    â”‚
â”‚   â”‚   â”‚  (HTTP)  â”‚    â”‚  (WS)    â”‚    â”‚(APNs/FCM)â”‚                     â”‚    â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â”‚                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚                      SERVICE LAYER                                  â”‚    â”‚
â”‚   â”‚                                                                     â”‚    â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚    â”‚
â”‚   â”‚  â”‚ Message  â”‚  â”‚ Delivery â”‚  â”‚ Presence â”‚  â”‚  Sync    â”‚            â”‚    â”‚
â”‚   â”‚  â”‚ Service  â”‚  â”‚ Service  â”‚  â”‚ Service  â”‚  â”‚ Service  â”‚            â”‚    â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚    â”‚
â”‚   â”‚                                                                     â”‚    â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚    â”‚
â”‚   â”‚  â”‚  Group   â”‚  â”‚   User   â”‚  â”‚  Media   â”‚                          â”‚    â”‚
â”‚   â”‚  â”‚ Service  â”‚  â”‚ Service  â”‚  â”‚ Service  â”‚                          â”‚    â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â”‚                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚                       DATA LAYER                                    â”‚    â”‚
â”‚   â”‚                                                                     â”‚    â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚    â”‚
â”‚   â”‚  â”‚ Message  â”‚  â”‚   User   â”‚  â”‚  Cache   â”‚  â”‚  Media   â”‚            â”‚    â”‚
â”‚   â”‚  â”‚  Store   â”‚  â”‚  Store   â”‚  â”‚ (Redis)  â”‚  â”‚  (S3)    â”‚            â”‚    â”‚
â”‚   â”‚  â”‚(Cassandraâ”‚  â”‚ (MySQL)  â”‚  â”‚          â”‚  â”‚          â”‚            â”‚    â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚    â”‚
â”‚   â”‚                                                                     â”‚    â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚    â”‚
â”‚   â”‚  â”‚           Message Queue (Kafka)           â”‚                      â”‚    â”‚
â”‚   â”‚  â”‚     delivery-tasks | notifications        â”‚                      â”‚    â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Diagram 2: Message Send Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MESSAGE SEND FLOW                                        â”‚
â”‚                                                                             â”‚
â”‚   ALICE (Sender)                                                            â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                             â”‚
â”‚        â”‚                                                                    â”‚
â”‚        â”‚ 1. POST /messages                                                  â”‚
â”‚        â”‚    {conversation_id, content}                                      â”‚
â”‚        â–¼                                                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                            â”‚
â”‚   â”‚    API     â”‚  2. Auth check                                             â”‚
â”‚   â”‚  Gateway   â”‚  3. Rate limit                                             â”‚
â”‚   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                                            â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â”‚ 4. Send to Message Service                                        â”‚
â”‚         â–¼                                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                            â”‚
â”‚   â”‚  Message   â”‚  5. Validate message                                       â”‚
â”‚   â”‚  Service   â”‚  6. Get sequence number (Redis INCR)                       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  7. Store message (Cassandra)                              â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â”‚ 8. Return message_id to Alice                                     â”‚
â”‚         â”‚    (Alice sees âœ“ sent)                                            â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â”‚ 9. Enqueue delivery task                                          â”‚
â”‚         â–¼                                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                            â”‚
â”‚   â”‚  Delivery  â”‚  10. Get conversation members                              â”‚
â”‚   â”‚  Service   â”‚  11. For each recipient:                                   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                                            â”‚
â”‚         â”‚                                                                   â”‚
â”‚    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚    â”‚                                     â”‚                                  â”‚
â”‚    â–¼                                     â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚  Presence  â”‚ 12. Is Bob online? â”‚    Push    â”‚                           â”‚
â”‚  â”‚  Service   â”‚                    â”‚  Gateway   â”‚                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚    YES â”‚                                 â”‚ NO                               â”‚
â”‚        â–¼                                 â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚ WebSocket  â”‚ 13. Push via WS    â”‚   APNs/    â”‚ 14. Push notification     â”‚
â”‚  â”‚  Gateway   â”‚                    â”‚    FCM     â”‚                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚        â”‚                                                                    â”‚
â”‚        â”‚ 15. Bob receives message                                           â”‚
â”‚        â–¼                                                                    â”‚
â”‚   BOB (Recipient)                                                           â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                           â”‚
â”‚        â”‚                                                                    â”‚
â”‚        â”‚ 16. Send delivery ACK                                              â”‚
â”‚        â–¼                                                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                            â”‚
â”‚   â”‚  Delivery  â”‚ 17. Update status â†’ DELIVERED                              â”‚
â”‚   â”‚  Service   â”‚ 18. Notify Alice (Alice sees âœ“âœ“)                           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                            â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Diagram 3: Failure Propagation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FAILURE PROPAGATION                                      â”‚
â”‚                                                                             â”‚
â”‚   SCENARIO: Message Store Becomes Slow (P99 > 2 seconds)                    â”‚
â”‚                                                                             â”‚
â”‚   T+0: Message Store latency increases                                      â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â–¼                                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  MESSAGE SERVICE                                                    â”‚   â”‚
â”‚   â”‚  â€¢ Write latency: 30ms â†’ 2000ms                                     â”‚   â”‚
â”‚   â”‚  â€¢ Thread pool saturates                                            â”‚   â”‚
â”‚   â”‚  â€¢ Requests queue up                                                â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                  â”‚                                          â”‚
â”‚   T+30s: API Gateway affected    â”‚                                          â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â–¼                                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  API GATEWAY                                                        â”‚   â”‚
â”‚   â”‚  â€¢ Request timeout rate: 0.1% â†’ 15%                                 â”‚   â”‚
â”‚   â”‚  â€¢ Client retries increase load                                     â”‚   â”‚
â”‚   â”‚  â€¢ Connection pool to Message Service exhausted                     â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                  â”‚                                          â”‚
â”‚   T+60s: User impact visible     â”‚                                          â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â–¼                                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  CLIENTS                                                            â”‚   â”‚
â”‚   â”‚  â€¢ Messages stuck at "sending..."                                   â”‚   â”‚
â”‚   â”‚  â€¢ Users retry manually (more load)                                 â”‚   â”‚
â”‚   â”‚  â€¢ Angry social media posts                                         â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚   MITIGATION ACTIVATED                                                      â”‚
â”‚   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚                                                                             â”‚
â”‚   T+2m: Circuit breaker triggers                                            â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â–¼                                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  MESSAGE SERVICE                                                    â”‚   â”‚
â”‚   â”‚  â€¢ Circuit breaker OPEN                                             â”‚   â”‚
â”‚   â”‚  â€¢ Fast-fail new requests (no timeout wait)                         â”‚   â”‚
â”‚   â”‚  â€¢ Return 503 with Retry-After header                               â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                  â”‚                                          â”‚
â”‚   T+3m: Backpressure to clients  â”‚                                          â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â–¼                                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  CLIENTS                                                            â”‚   â”‚
â”‚   â”‚  â€¢ Queue messages locally                                           â”‚   â”‚
â”‚   â”‚  â€¢ Show "connection issues" banner                                  â”‚   â”‚
â”‚   â”‚  â€¢ Retry with exponential backoff                                   â”‚   â”‚
â”‚   â”‚  â€¢ Messages saved, will send when healthy                           â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚   T+10m: Message Store recovers                                             â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â–¼                                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  GRADUAL RECOVERY                                                   â”‚   â”‚
â”‚   â”‚  â€¢ Circuit breaker: Half-open (test requests)                       â”‚   â”‚
â”‚   â”‚  â€¢ Test requests succeed â†’ Circuit CLOSED                           â”‚   â”‚
â”‚   â”‚  â€¢ Client queues drain gradually                                    â”‚   â”‚
â”‚   â”‚  â€¢ Full recovery in ~5 minutes                                      â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚   OUTCOME:                                                                  â”‚
â”‚   â€¢ 10-minute degradation, not outage                                       â”‚
â”‚   â€¢ Zero messages lost                                                      â”‚   
â”‚   â€¢ Messages delayed 5-15 minutes                                           â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Diagram 4: System Evolution

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SYSTEM EVOLUTION                                         â”‚
â”‚                                                                             â”‚
â”‚   V1: MONOLITH (10K users)                                                  â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚   â”‚  Single Server                                   â”‚                      â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚                      â”‚
â”‚   â”‚  â”‚   App    â”‚  â”‚  MySQL   â”‚  â”‚  Files   â”‚        â”‚                      â”‚
â”‚   â”‚  â”‚  (PHP)   â”‚  â”‚  (1 DB)  â”‚  â”‚  (local) â”‚        â”‚                      â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚                      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                                                             â”‚
â”‚   â–¼ Scaling pain: DB bottleneck, file storage full                          â”‚
â”‚                                                                             â”‚
â”‚   V2: SEPARATED TIERS (1M users)                                            â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚   â”‚  API Servers â”‚  â”‚   Database   â”‚  â”‚    Media     â”‚                      â”‚
â”‚   â”‚    (x10)     â”‚  â”‚  (Primary +  â”‚  â”‚    (S3)      â”‚                      â”‚
â”‚   â”‚              â”‚  â”‚   Replica)   â”‚  â”‚              â”‚                      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                                                             â”‚
â”‚   â–¼ Scaling pain: Long polling, real-time delays                            â”‚
â”‚                                                                             â”‚
â”‚   V3: REAL-TIME ADDED (10M users)                                           â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚   â”‚   API    â”‚  â”‚WebSocket â”‚  â”‚ Database â”‚  â”‚ Redis â”‚                       â”‚
â”‚   â”‚ Servers  â”‚  â”‚ Servers  â”‚  â”‚ (Sharded)â”‚  â”‚ Cache â”‚                       â”‚
â”‚   â”‚  (x50)   â”‚  â”‚  (x20)   â”‚  â”‚          â”‚  â”‚       â”‚                       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                                             â”‚
â”‚   â–¼ Scaling pain: Group fanout, presence storms                             â”‚
â”‚                                                                             â”‚
â”‚   V4: MICROSERVICES (100M users)                                            â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚                        Services                               â”‚          â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”‚          â”‚
â”‚   â”‚  â”‚Messageâ”‚ â”‚Deliveryâ”‚ â”‚Presenceâ”‚ â”‚ Sync â”‚ â”‚ Group â”‚ â”‚ User â”‚ â”‚          â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚                     Data Stores                               â”‚          â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚          â”‚
â”‚   â”‚  â”‚Cassandraâ”‚  â”‚ MySQL  â”‚  â”‚ Redis â”‚  â”‚  S3   â”‚  â”‚   Kafka   â”‚â”‚          â”‚
â”‚   â”‚  â”‚(messagesâ”‚  â”‚(users) â”‚  â”‚(cache)â”‚  â”‚(media)â”‚  â”‚  (queue)  â”‚â”‚          â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                             â”‚
â”‚   â–¼ Scaling pain: Regional latency, compliance requirements                 â”‚
â”‚                                                                             â”‚
â”‚   V5: MULTI-REGION CELLS (1B+ users)                                        â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚      US Cell       â”‚  â”‚      EU Cell       â”‚  â”‚    APAC Cell     â”‚      â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚      â”‚
â”‚   â”‚  â”‚Full service  â”‚  â”‚  â”‚  â”‚Full service  â”‚  â”‚  â”‚ â”‚Full service  â”‚ â”‚      â”‚
â”‚   â”‚  â”‚   stack      â”‚  â”‚  â”‚  â”‚   stack      â”‚  â”‚  â”‚ â”‚   stack      â”‚ â”‚      â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚      â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚      â”‚
â”‚   â”‚  â”‚ Full data    â”‚  â”‚  â”‚  â”‚ Full data    â”‚  â”‚  â”‚ â”‚ Full data    â”‚ â”‚      â”‚
â”‚   â”‚  â”‚   replicas   â”‚â—„â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â–º replicas  â”‚â—„â”€â”¼â”€â”€â”¼â”€â”¼â”€â”€â–ºreplicas   â”‚ â”‚      â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚              â–²                      â–²                      â–²                â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Cross-region replication â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Part 18: Brainstorming, Exercises & Redesigns

## "What If X Changes?" Questions

```
WHAT IF: Message volume 10x overnight (viral event)

Immediate impact:
â€¢ Message queues back up
â€¢ Database write capacity exceeded
â€¢ Push notification system overwhelmed

Short-term response:
â€¢ Shed non-critical features (typing, presence)
â€¢ Rate limit less-critical paths
â€¢ Scale horizontally (pre-provisioned capacity)

Long-term change:
â€¢ More aggressive auto-scaling
â€¢ Higher base capacity
â€¢ Better traffic prediction

WHAT IF: Average message size 100x (voice messages become primary)

Impact:
â€¢ Storage costs explode
â€¢ Bandwidth costs explode
â€¢ Upload/download times increase

Response:
â€¢ Aggressive compression (opus codec)
â€¢ Streaming upload/download
â€¢ Shorter retention for voice
â€¢ Separate voice message tier

WHAT IF: Regulatory requirement for message retention

Impact:
â€¢ Can't delete user messages on request
â€¢ Need audit trail for access
â€¢ Legal hold functionality needed

Response:
â€¢ Parallel compliance data store
â€¢ Access logging and approval workflow
â€¢ Separate retained vs user-visible state
â€¢ Legal review before deletion

WHAT IF: E2E encryption becomes mandatory

Impact:
â€¢ Cannot scan for abuse on server
â€¢ Cannot recover messages if user loses key
â€¢ More complex key management

Response:
â€¢ Client-side content scanning (controversial)
â€¢ Key escrow options (with user consent)
â€¢ Robust key backup mechanisms
â€¢ Accept reduced abuse detection capability
```

## Redesign Under New Constraints

```
CONSTRAINT: Zero trust network (all internal traffic encrypted)

Original: Services communicate over private network
New: Every service-to-service call authenticated + encrypted

Changes needed:
â€¢ mTLS between all services
â€¢ Service identity certificates
â€¢ Certificate rotation automation
â€¢ ~5ms latency overhead per hop

CONSTRAINT: Data must stay in user's home country

Original: Global replication, route anywhere
New: Messages stored only in user's country

Changes needed:
â€¢ Per-country data stores
â€¢ Routing based on sender country
â€¢ Cross-country conversations: ???
  - Option A: Store in sender's country (recipient must query cross-border)
  - Option B: Store copy in both countries (compliance risk?)
  - Option C: Store in neutral zone (latency, complexity)
â€¢ Significantly more operational complexity

CONSTRAINT: Maximum 10ms P99 message send latency

Original: 100ms P50, 300ms P95
New: 10ms P99

Changes needed:
â€¢ No synchronous database write (use async + local ACK)
â€¢ Edge message acceptance
â€¢ Eventual consistency everywhere
â€¢ Risk: Can lose messages if edge fails before sync

Trade-off analysis:
â€¢ 10ms P99 is unrealistic for durable messaging
â€¢ Would need to sacrifice durability guarantee
â€¢ Better: Accept 50ms P99, which is achievable with durability
```

## Failure Injection Exercises

```
EXERCISE 1: Chaos Monkey for WebSocket Gateways

Inject: Kill random WebSocket gateway every hour

Expected behavior:
â€¢ Affected users disconnect (10 seconds no messages)
â€¢ Clients auto-reconnect to different gateway
â€¢ Sync service provides missed messages
â€¢ No user-visible message loss

What to monitor:
â€¢ Reconnection success rate
â€¢ Time to reconnect
â€¢ Message loss/duplication
â€¢ User complaints

EXERCISE 2: Slow Message Store

Inject: Add 500ms latency to 10% of message store writes

Expected behavior:
â€¢ P50 latency mostly unaffected
â€¢ P90 increases noticeably
â€¢ Circuit breaker should not trigger (only 10%)
â€¢ No message loss

What to monitor:
â€¢ Latency percentiles
â€¢ Timeout rates
â€¢ User retry behavior
â€¢ Queue depths

EXERCISE 3: Push Notification Failure

Inject: Block all push notifications for 30 minutes

Expected behavior:
â€¢ Offline users don't get notified
â€¢ Online users (WebSocket) unaffected
â€¢ When push recovers, backlog clears
â€¢ Users opening app get all messages (sync)

What to monitor:
â€¢ Push delivery rate
â€¢ App open rate (users check manually?)
â€¢ Message delivery lag
â€¢ User complaints
```

## Trade-off Debates

```
DEBATE 1: Fanout on Write vs Fanout on Read

For Fanout on Write:
â€¢ Fast reads (message already in recipient's inbox)
â€¢ Simple read path
â€¢ Good for mostly 1:1 messaging

For Fanout on Read:
â€¢ Efficient writes (store once)
â€¢ Better for large groups
â€¢ Easier to handle membership changes

Staff position:
â€¢ Hybrid: FOW for small groups (<50), FOR for large groups
â€¢ 1:1 is essentially FOW (store in both inboxes)
â€¢ Threshold tuned based on actual usage patterns

DEBATE 2: Strong vs Eventual Consistency for Membership

For Strong:
â€¢ Immediate effect of blocks/bans
â€¢ No leaked messages to removed members
â€¢ Simpler mental model

For Eventual:
â€¢ Better availability
â€¢ Lower latency
â€¢ Partition tolerance

Staff position:
â€¢ Strong consistency for membership within region
â€¢ Accept eventual consistency cross-region
â€¢ Err on side of blocking (if unsure if blocked, assume blocked)

DEBATE 3: Client-Generated vs Server-Generated Message IDs

For Client-Generated:
â€¢ Idempotency built in (retry with same ID)
â€¢ Works offline (generate ID without server)
â€¢ Matches client's mental model

For Server-Generated:
â€¢ Guaranteed unique (no collision risk)
â€¢ Server controls ordering
â€¢ Simpler client

Staff position:
â€¢ Client generates ID (UUID or similar)
â€¢ Server validates uniqueness
â€¢ Benefits of idempotency outweigh collision risk
â€¢ Collision probability with UUIDv4: negligible
```

## Additional Brainstorming Questions (Staff-Level Depth)

```
FAILURE & RESILIENCE SCENARIOS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Q1: What happens if the sequence number service (Redis) fails during 
    a high-traffic period? Design the fallback.
    
    Explore: Local sequence with reconciliation, optimistic assignment,
    accepting temporary disorder

Q2: A bug causes 1% of messages to be stored with incorrect 
    conversation_id. How do you detect, contain, and remediate?
    
    Explore: Anomaly detection, message integrity checks, rollback scope

Q3: During a network partition, both regions accept messages for the 
    same conversation. How do you merge when partition heals?
    
    Explore: CRDT-style merge, last-write-wins, user-visible conflicts

Q4: Push notification provider (APNs) is returning 50% failures. 
    How do you degrade and what do you surface to users?
    
    Explore: Retry strategy, user notification, fallback channels

SCALE STRESS TESTS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Q5: A single group has 100K members and 10 messages/second. 
    That's 1M delivery tasks/second for one group. Design the solution.
    
    Explore: Lazy delivery, read-on-open, notification batching

Q6: A celebrity with 10M followers joins. Each follower tries to 
    message them. Design inbound rate limiting without blocking legitimate 
    contacts.
    
    Explore: Tiered inboxes, contact vs non-contact routing, queueing

Q7: New Year's Eve: 100x traffic for 1 hour, timezone by timezone. 
    How do you prepare, and what can you shed during peak?
    
    Explore: Pre-scaling, degradation playbook, geographic load prediction

Q8: A user has 50,000 unread messages after a 6-month absence. 
    How do you handle their sync request?
    
    Explore: Pagination strategy, summary generation, selective sync

COST OPTIMIZATION EXERCISES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Q9: Media storage costs $50M/year. Reduce by 50% without 
    significant user impact. What do you cut?
    
    Explore: Compression, tiering, expiry, on-demand regeneration

Q10: Push notifications cost $10M/year. The finance team asks for 30% 
     reduction. What's safe to cut?
     
     Explore: Batching, priority tiers, muted group handling

Q11: WebSocket servers are 40% of compute cost. Can you reduce 
     connection count without affecting user experience?
     
     Explore: Intelligent reconnect, client-side backoff, shared connections

ORGANIZATIONAL & EVOLUTION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Q12: You need to add end-to-end encryption. What's the multi-year 
     migration plan?
     
     Explore: Hybrid encryption period, key management, abuse detection impact

Q13: Regulatory requirement: Messages must be deletable within 24 hours 
     of user request, including all replicas and backups. Design the system.
     
     Explore: Deletion propagation, backup handling, audit trail

Q14: Two teams disagree: Message Core wants 3-day queue retention for 
     reliability; Delivery wants 1-hour for cost. How do you resolve?
     
     Explore: Data-driven analysis, tiered retention, SLO alignment

Q15: The system was designed for 1:1 messaging. Product now wants 
     1-to-many broadcast channels (like Telegram). What breaks?
     
     Explore: Fanout explosion, read path changes, separate data model
```

## Deep Exercises: Redesign Under Constraints

```
EXERCISE 1: ZERO-INFRASTRUCTURE MESSAGING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Constraint: No server infrastructure. Purely peer-to-peer.
Question: What's possible and what's impossible?

Analysis points:
â€¢ Offline delivery: Impossible without always-on peer
â€¢ NAT traversal: Requires STUN/TURN (so not truly zero-infra)
â€¢ Group messaging: Mesh networking at NÂ² scale
â€¢ Abuse prevention: No central authority
Conclusion: Pure P2P fails for consumer messaging. Understand why.

EXERCISE 2: 10MS GLOBAL MESSAGE DELIVERY
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Constraint: P99 message delivery < 10ms, globally.
Question: What would you sacrifice?

Analysis points:
â€¢ Physics: 100ms cross-continent, can't beat light
â€¢ Solution: Edge delivery with async durability
â€¢ Trade-off: Message might be delivered before durably stored
â€¢ Risk: Message loss if edge fails
Conclusion: Impossible without sacrificing durability. Explain the physics.

EXERCISE 3: SERVERLESS MESSAGING ARCHITECTURE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Constraint: All compute on AWS Lambda (or equivalent). No persistent servers.
Question: What problems arise?

Analysis points:
â€¢ WebSocket: Lambda doesn't support long-lived connections
â€¢ Alternative: API Gateway WebSocket (managed, but limited)
â€¢ Cold start: 100-500ms added latency on first message
â€¢ Cost: Pay-per-invocation might be cheaper... or 10x more expensive
Conclusion: Possible with caveats. Calculate cost crossover point.

EXERCISE 4: PRIVACY-PRESERVING ABUSE DETECTION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Constraint: Full E2E encryption. Still detect CSAM and spam.
Question: What's the design space?

Analysis points:
â€¢ Client-side scanning: Controversial (Apple abandoned)
â€¢ Metadata-only detection: Limited (can detect patterns, not content)
â€¢ User reporting: Works but delayed
â€¢ Perceptual hashing: Requires decrypted access somewhere
Conclusion: No good solution. Understand the fundamental tension.

EXERCISE 5: MESSAGING WITHOUT MESSAGE STORAGE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Constraint: Server stores no message content (extreme privacy).
Question: How do you provide history and sync?

Analysis points:
â€¢ Client-side storage becomes source of truth
â€¢ New device: Must get history from existing device
â€¢ Lost device: History gone forever
â€¢ Backup: Encrypted backup to cloud, user holds key
Conclusion: Possible but significant UX tradeoffs. Evaluate.

EXERCISE 6: MULTI-DEVICE SYNC WITHOUT CONFLICTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Constraint: User has 4 devices. All must show identical message state.
Question: What's the sync protocol?

Analysis points:
â€¢ Read on one device â†’ mark read on all (propagation delay?)
â€¢ Type on phone, switch to laptop â†’ continue typing indicator?
â€¢ Delete on tablet â†’ propagate to phone immediately?
â€¢ Offline edits on two devices â†’ which wins?
Conclusion: Vector clocks or CRDTs for conflict-free sync.

EXERCISE 7: SEARCH AT MESSAGING SCALE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Constraint: 100B messages. User searches "dinner plans with Bob last month."
Question: How do you make this fast?

Analysis points:
â€¢ Full-text index on 100B messages = massive storage
â€¢ Per-user index: N users Ã— avg messages = still huge
â€¢ Tiered search: Recent in fast index, old messages = slow query
â€¢ Privacy: Search index must be access-controlled
Conclusion: Trade-off between search speed and index cost.

EXERCISE 8: GRACEFUL CLIENT BEHAVIOR DURING OUTAGE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Constraint: Server is down for 10 minutes. What should client do?
Question: Design the client-side resilience strategy.

Analysis points:
â€¢ Queue messages locally with retry
â€¢ Show clear status ("connecting..." vs "sending...")
â€¢ Allow reading cached messages
â€¢ Don't lose user's typed message
â€¢ Reconnect with backoff (don't storm)
Conclusion: Client is a full participant in reliability.
```

## Topic-Specific Practice Questions

```
ORDERING & CONSISTENCY:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Q16: Alice sends "A" then "B". Due to network issues, "B" arrives at server 
     first. How does your system ensure Alice's messages display in order?
     
     Explore: Client-side sequence, server reordering, gap detection

Q17: In a group, Alice and Bob send messages at the exact same millisecond. 
     How do you decide ordering? Is it consistent across all members?
     
     Explore: Tiebreaker rules, deterministic ordering, Lamport timestamps

Q18: Message edits and deletes arrive out of order. How do you handle 
     "delete message 5" arriving before "message 5" itself?
     
     Explore: Tombstones, operation log, version vectors

PRESENCE & REAL-TIME SIGNALS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Q19: User has 500 contacts. How do you efficiently show which are online 
     without creating NÂ² traffic?
     
     Explore: Subscription limits, lazy loading, bloom filters for presence

Q20: Typing indicator shows for 5 seconds, but user stops typing after 2. 
     How do you handle "stop typing" signal loss?
     
     Explore: Auto-expire, explicit stop, heartbeat-based

Q21: Presence shows "online" but user closed app 2 minutes ago. Why does 
     this happen and is it acceptable?
     
     Explore: Heartbeat intervals, TTL trade-offs, "last seen" as fallback

CONNECTION & DELIVERY:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Q22: User is on flaky mobile network (connects/disconnects every 30 seconds). 
     How do you avoid constant sync overhead?
     
     Explore: Connection coalescing, sync checkpoints, resume tokens

Q23: Push notification says "New message from Alice" but when user opens app, 
     no message appears. What went wrong?
     
     Explore: Race conditions, sync timing, push as trigger not content

Q24: User has app open but phone went to sleep. Is WebSocket still alive? 
     How do you handle mobile app backgrounding?
     
     Explore: Keep-alive, OS constraints, push fallback for backgrounded apps

MEDIA & ATTACHMENTS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Q25: User sends 50MB video. How do you handle upload? What if it fails at 80%?
     
     Explore: Resumable upload, chunking, client-side compression

Q26: Media expires after 30 days but user still sees the message. What do they 
     see where the image was?
     
     Explore: Placeholder, "media expired", re-request option

Q27: Same image sent to 100 different conversations. Do you store it 100 times?
     
     Explore: Content-addressable storage, privacy implications, dedup cost

GROUPS & MEMBERSHIP:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Q28: Admin removes a user from group. User was offline. When they come online, 
     do they see messages sent while they were still a member?
     
     Explore: Membership snapshot at send time, retroactive visibility

Q29: Group has 10,000 members. Member list request takes 5 seconds. How do 
     you optimize?
     
     Explore: Pagination, lazy loading, "X and 9,997 others"

Q30: Two admins simultaneously remove each other from the group. Who wins?
     
     Explore: Last-write-wins, optimistic locking, admin hierarchy
```

## Comprehensive Design Exercises

```
FULL DESIGN EXERCISE 1: DISAPPEARING MESSAGES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Requirement: Messages auto-delete after 24 hours from being READ (not sent).

Design considerations:
â€¢ Per-recipient read time tracking
â€¢ Group messages: Delete when ALL have read? Or per-recipient?
â€¢ What if recipient never reads? Keep forever?
â€¢ Storage implication: Can't use TTL on write
â€¢ Sync implication: Deleted message must be removed from all devices

Pseudo-code for expiry check:
FUNCTION check_message_expiry(message, recipient):
    read_time = get_read_time(message.id, recipient.id)
    
    IF read_time IS NULL:
        RETURN NOT_EXPIRED  // Never read, don't expire
    
    IF now() - read_time > 24_HOURS:
        RETURN EXPIRED
    
    RETURN NOT_EXPIRED

FULL DESIGN EXERCISE 2: MESSAGE REACTIONS AT SCALE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Requirement: Users can react to messages with emoji. Show reaction counts.

Design considerations:
â€¢ Data model: Per-message reaction list vs reaction counts
â€¢ Popular message: 1M reactions on viral message in large group
â€¢ Real-time updates: Push reaction changes to all viewers?
â€¢ Undo reaction: Track individual reactions or just counts?
â€¢ Privacy: Who can see who reacted?

Storage trade-off:
Option A: Store every (message_id, user_id, emoji) - accurate but huge
Option B: Store (message_id, emoji, count) - compact but can't undo
Option C: Hybrid - detailed for first 1000, counts after

FULL DESIGN EXERCISE 3: SCHEDULED MESSAGES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Requirement: User can schedule message to send at future time.

Design considerations:
â€¢ Storage: Where do scheduled messages live before send time?
â€¢ Reliability: What if scheduler service is down at send time?
â€¢ Edit/cancel: User wants to change scheduled message
â€¢ Timezone: User in LA schedules for "9 AM" - whose 9 AM?
â€¢ Ordering: Scheduled message vs real-time messages interleave?

Scheduler reliability:
FUNCTION schedule_message(message, send_at):
    // Store in scheduled_messages table
    store_scheduled(message, send_at)
    
    // Also enqueue in distributed scheduler
    scheduler.enqueue(
        task_id = message.id,
        execute_at = send_at,
        action = "send_message",
        payload = message
    )
    
    // Belt and suspenders: Sweeper job checks for missed
    // scheduled messages every minute

FULL DESIGN EXERCISE 4: MESSAGE FORWARDING CHAIN
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Requirement: Track how many times a message was forwarded (like WhatsApp).

Design considerations:
â€¢ Forward count: Linear chain or tree?
â€¢ Privacy: Original sender visible to forward recipients?
â€¢ Virality detection: Alert if message forwarded 1000+ times
â€¢ Storage: Copy message or reference original?
â€¢ Editing: If original is edited, do forwards update?

Chain tracking:
Message:
    id: string
    original_message_id: string (null if original)
    forward_count: int (count of direct forwards from THIS message)
    chain_depth: int (how many hops from original)

FULL DESIGN EXERCISE 5: CROSS-PLATFORM MESSAGE SYNC
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Requirement: Sync messages between WhatsApp, Telegram, and your platform.

Design considerations:
â€¢ API differences: Each platform has different capabilities
â€¢ Identity mapping: Same user on different platforms
â€¢ Feature mismatch: Telegram has channels, WhatsApp doesn't
â€¢ Delivery receipt: How to track across platforms?
â€¢ Failure: One platform is down, others continue

This is a HARD problem that demonstrates L6 systems thinking:
â€¢ Abstraction layer for platform-agnostic message model
â€¢ Adapter pattern for each platform's API
â€¢ Event sourcing for reliable cross-platform sync
â€¢ Graceful degradation when platforms are unavailable
```

---

# Summary

Messaging platforms are deceptively complex systems. The core challenge isn't sending bytes between devicesâ€”it's maintaining ordering guarantees, handling offline users gracefully, and scaling fan-out without melting infrastructure.

**Key Staff-Level Insights:**

1. **Message delivery is sacred.** Everything elseâ€”typing indicators, presence, read receiptsâ€”can degrade. Messages must never be lost after ACK.

2. **Ordering is conversation-local.** Don't try to achieve global orderingâ€”it doesn't scale. Per-sender ordering with Lamport timestamps is sufficient.

3. **Offline is the common case.** Design the offline path first, then optimize the online path. Sync must work for users offline for days.

4. **Large groups are different.** Fan-out strategies must change based on group size. What works for 10 members fails at 10,000.

5. **Presence is eventually consistent.** Don't make presence a scaling bottleneck. Users tolerate 30-60 seconds of staleness.

6. **The client is smart.** Client-side caching, local storage, and queuing reduce server load and improve UX during failures.

**The Staff Engineer Difference:**

An L5 might design a working messaging system. An L6 designs a messaging system that gracefully degrades during failures, scales sub-linearly with traffic, handles the edge cases that break naive implementations, and evolves without requiring rewrites. The difference is in the depth of understandingâ€”not just how messages flow, but what happens when they don't.

---

# L6 Review: Final Verification

## This section now meets Google Staff Engineer (L6) expectations.

### Staff-Level Signals Covered:

| Category | Coverage | Key Additions |
|----------|----------|---------------|
| **Judgment & Decision-Making** | âœ… Complete | Explicit WHY for all major decisions; alternatives considered and rejected with reasoning |
| **Failure & Degradation Thinking** | âœ… Complete | Cascading failure timeline; blast radius analysis; partial failure behavior; retry storm quantification |
| **Scale & Evolution** | âœ… Complete | Order-of-magnitude estimates; dangerous assumptions identified; V1â†’V2â†’V3 evolution with incident-driven redesign |
| **Cost & Sustainability** | âœ… Complete | Cost per message breakdown; operational burden as constraint; explicit over-engineering avoidance |
| **Organizational Reality** | âœ… Complete | Team ownership boundaries; on-call burden; cross-team coordination; migration strategies |

### Checklist of L6 Signals:

- [x] Trade-offs explicitly stated with context
- [x] Partial failure scenarios (not just total outage)
- [x] Blast radius containment for each failure mode
- [x] Cascading failure timeline with trigger â†’ propagation â†’ containment
- [x] Quantified capacity assumptions and what breaks first
- [x] Cost treated as first-class design constraint
- [x] Operational burden considered in architecture
- [x] Team boundaries and ownership clarity
- [x] Migration path for major changes (FOWâ†’FOR example)
- [x] Rolling deployment strategy for stateful services
- [x] Thundering herd mitigation with quantified analysis
- [x] Interview calibration with L5 vs L6 distinction
- [x] Deep exercises requiring constraint-based redesign

### Topic Coverage in Exercises:

| Topic | Questions/Exercises |
|-------|---------------------|
| **Ordering & Consistency** | Q16, Q17, Q18 |
| **Presence & Real-Time** | Q19, Q20, Q21 |
| **Connection & Delivery** | Q22, Q23, Q24 |
| **Media & Attachments** | Q25, Q26, Q27 |
| **Groups & Membership** | Q28, Q29, Q30 |
| **Failure & Resilience** | Q1-Q4 |
| **Scale Stress** | Q5-Q8 |
| **Cost Optimization** | Q9-Q11 |
| **Organizational** | Q12-Q15 |
| **Full Design Exercises** | Disappearing messages, Reactions, Scheduling, Forwarding, Cross-platform |
| **Constraint Redesigns** | P2P, 10ms latency, Serverless, Privacy-preserving abuse, No storage |

### Remaining Considerations (Not Gaps):

1. **End-to-end encryption** is explicitly out of scope (acknowledged and explained)
2. **Voice/Video calling** is out of scope (different infrastructure)
3. **Content moderation ML** is referenced but not detailed (separate system)

These are intentional scope boundaries, not gaps. Each could be a separate chapter.

### Pseudo-Code Convention:

All code examples in this chapter use language-agnostic pseudo-code:
- `FUNCTION` keyword for function definitions
- `IF/ELSE/FOR/WHILE` for control flow
- Descriptive variable names in snake_case
- No language-specific syntax (no `def`, `public void`, `func`, etc.)
- Comments explain intent, not syntax

### How to Use This Chapter in Interview:

1. Start with the "L5 vs L6 Messaging Platform Decisions" table to frame your approach
2. Use the scale estimates to demonstrate quantitative thinking
3. Reference the failure timeline when asked "what if X fails"
4. Apply the trade-off debates when pushed on design choices
5. Use the brainstorming questions to anticipate follow-ups
6. Practice the full design exercises to build end-to-end fluency
7. Study the common L5 mistakes section to avoid interview pitfalls