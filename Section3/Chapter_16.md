# Chapter 14: Leader Election, Coordination, and Distributed Locks

## When Your System Needs a Bossâ€”And When It Doesn't

---

# Quick Visual: The Coordination Landscape

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COORDINATION: WHEN DO YOU NEED IT?                       â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  PREFER NO COORDINATION (Best)                                       â”‚  â”‚
â”‚   â”‚  â€¢ Idempotent operations                                             â”‚  â”‚
â”‚   â”‚  â€¢ CRDTs (conflict-free replicated data types)                       â”‚  â”‚
â”‚   â”‚  â€¢ Partition data so each node owns its subset                       â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â†“                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  IF YOU MUST: LEADER ELECTION                                        â”‚  â”‚
â”‚   â”‚  â€¢ Single coordinator for consistency                                â”‚  â”‚
â”‚   â”‚  â€¢ Database primary, job scheduler, metadata service                 â”‚  â”‚
â”‚   â”‚  â€¢ Use: Raft, ZooKeeper, etcd                                        â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â†“                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  IF YOU MUST: DISTRIBUTED LOCKS                                      â”‚  â”‚
â”‚   â”‚  â€¢ Short-term mutual exclusion                                       â”‚  â”‚
â”‚   â”‚  â€¢ Protect critical sections                                         â”‚  â”‚
â”‚   â”‚  â€¢ ALWAYS use fencing tokens!                                        â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚   RULE: Coordination is expensive. Minimize it. Plan for its failure.       â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Simple Example: L5 vs L6 Coordination Thinking

| Scenario | L5 Approach | L6 Approach |
|----------|------------|-------------|
| **Job scheduler** | "Use a distributed lock" | "First, can we partition jobs by ID? If not, use leader election with degraded mode" |
| **Rate limiter** | "Use Redis for global counters" | "Per-node limits + async sync. Accept approximate. Plan for Redis failure." |
| **Config updates** | "Lock the config during update" | "Versioned configs. Readers use stale with TTL. No lock needed." |
| **Lock failure** | "Retry until success" | "Timeout + graceful degradation. What if lock service is down?" |
| **Duplicate prevention** | "Distributed lock per request" | "Idempotency keys. No coordination needed." |

---

# Key Numbers to Remember

| Metric | Typical Value | Why It Matters |
|--------|---------------|----------------|
| **NTP clock skew** | 10-100ms | Can't use timestamps for coordination |
| **TrueTime uncertainty** | ~7ms | Spanner waits this long for external consistency |
| **Leader election time** | 10-30 seconds | This is your failover window |
| **Lock TTL** | 10-30 seconds | Balance: too short = constant renewal, too long = slow recovery |
| **Raft heartbeat** | 50-150ms | Lower = faster detection, higher = less network overhead |
| **Raft election timeout** | 150-300ms | Must be > heartbeat interval |

---

## Table of Contents

1. [Introduction: The Coordination Tax](#introduction)
2. [Why Coordination Is Hard](#why-coordination-is-hard)
3. [Leader Election: Crowning a King in a Democracy](#leader-election)
4. [Distributed Locks: The Double-Edged Sword](#distributed-locks)
5. [Consensus: The Foundation (High-Level)](#consensus)
6. [Failure Scenarios That Will Ruin Your Week](#failure-scenarios)
7. [Case Study: Job Scheduler](#case-study-job-scheduler)
8. [Case Study: Rate Limiter Coordination](#case-study-rate-limiter)
9. [Case Study: Metadata Service](#case-study-metadata-service)
10. [Anti-Patterns: How Good Intentions Go Wrong](#anti-patterns)
11. [When NOT to Use Locks](#when-not-to-use-locks)
12. [Graceful Degradation: What Happens When Coordination Fails](#graceful-degradation)
13. [Interview Explanations](#interview-explanations)
14. [Brainstorming Questions](#brainstorming-questions)
15. [Homework: Remove Coordination and Re-Architect](#homework)

---

<a name="introduction"></a>
## 1. Introduction: The Coordination Tax

There's a moment in every distributed systems engineer's career when they realize a terrifying truth: **the hardest problems aren't about moving dataâ€”they're about getting machines to agree on anything.**

You want five servers to agree on who's the leader? Prepare for edge cases that will haunt your dreams.

You want a distributed lock so only one worker processes a job? Get ready for the lock to become a bottleneck, a single point of failure, orâ€”worst of allâ€”something that *looks* like it's working but isn't.

**Coordination is the dark matter of distributed systems.** It's invisible when it works and catastrophic when it fails. This section is about understanding when you need it, when you don't, and how to survive when it breaks.

### The Fundamental Problem

In a distributed system, there is no global clock, no shared memory, and no guaranteed message delivery. Yet we often need exactly these things:

| What We Want | Why It's Hard |
|--------------|---------------|
| One leader at a time | Network partitions can create two "leaders" |
| Exactly-once processing | Failures can cause zero or duplicate processing |
| Mutual exclusion | Locks can be held by dead processes |
| Consistent ordering | Different nodes see events in different orders |
| Atomic operations | Partial failures leave inconsistent state |

Every coordination mechanism is a bet against the universeâ€”a bet that the network will behave, that clocks won't drift too far, that processes will fail cleanly. Sometimes you win. Sometimes at 3 AM, you don't.

---

<a name="why-coordination-is-hard"></a>
## 2. Why Coordination Is Hard

### 2.1 The Two Generals Problem

Imagine two generals on opposite sides of a valley. They need to attack simultaneously or the attack fails. They can only communicate by messenger, but messengers might be captured.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        THE TWO GENERALS PROBLEM                         â”‚
â”‚                                                                         â”‚
â”‚     General A                                              General B    â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚     â”‚       â”‚         "Attack at dawn!"                    â”‚       â”‚    â”‚
â”‚     â”‚   A   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   B   â”‚    â”‚
â”‚     â”‚       â”‚                                              â”‚       â”‚    â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”˜                                              â””â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                         â”‚
â”‚     A doesn't know if B received the message.                           â”‚
â”‚                                                                         â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚     â”‚       â”‚            "Got it, I'll attack!"            â”‚       â”‚    â”‚
â”‚     â”‚   A   â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   B   â”‚    â”‚
â”‚     â”‚       â”‚                                              â”‚       â”‚    â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”˜                                              â””â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                         â”‚
â”‚     Now B doesn't know if A knows that B will attack.                   â”‚
â”‚     This loops infinitely. Neither can ever be certain.                 â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Lesson:** In an unreliable network, **you cannot achieve guaranteed agreement with just message passing.** This isn't a limitation of your codeâ€”it's mathematically proven.

### 2.2 The FLP Impossibility Result

In 1985, Fischer, Lynch, and Paterson proved that **no deterministic consensus protocol can guarantee progress in an asynchronous system if even one process can fail.**

Translation for engineers:
- You cannot build a perfect consensus system
- Any system you build will either sacrifice availability (block forever) or consistency (give wrong answer)
- This is fundamental, not a bug in your implementation

**What this means practically:** All real coordination systems make trade-offs. They use timeouts, probabilistic guarantees, or stronger assumptions (like partially synchronous networks).

### 2.3 The Three Impossibilities You'll Fight

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”€â”
â”‚                     THE COORDINATION TRILEMMA                            â”‚
â”‚                                                                          â”‚
â”‚                           CORRECTNESS                                    â”‚
â”‚                          (Agreement)                                     â”‚
â”‚                               â–²                                          â”‚
â”‚                              â•± â•²                                         â”‚
â”‚                             â•±   â•²                                        â”‚
â”‚                            â•±     â•²                                       â”‚
â”‚                           â•±  ???  â•²                                      â”‚
â”‚                          â•±         â•²                                     â”‚
â”‚                         â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼                                    â”‚
â”‚                   LIVENESS        FAULT                                  â”‚
â”‚                  (Progress)     TOLERANCE                                â”‚
â”‚                                                                          â”‚
â”‚     You can have two strongly, the third weakly.                         â”‚
â”‚                                                                          â”‚
â”‚     Correctness + Liveness = Works until any failure (fragile)           â”‚
â”‚     Correctness + Fault Tolerance = May block forever (unavailable)      â”‚
â”‚     Liveness + Fault Tolerance = May give wrong answer (inconsistent)    â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”€â”€â”˜
```

### 2.4 Why Clocks Don't Help

"Just use timestamps to decide who wins!" â€” Famous last words

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          CLOCK SKEW DISASTER                            â”‚
â”‚                                                                         â”‚
â”‚   Server A clock: 10:00:00.000                                          â”‚
â”‚   Server B clock: 10:00:00.150 (150ms ahead)                            â”‚
â”‚                                                                         â”‚
â”‚   Timeline (real time):                                                 â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶        â”‚
â”‚                                                                         â”‚
â”‚   T=0ms:     A writes X=1 (timestamp 10:00:00.000)                      â”‚
â”‚   T=50ms:    B writes X=2 (timestamp 10:00:00.200) â† APPEARS LATER!     â”‚
â”‚                                                                         â”‚
â”‚   If using last-write-wins by timestamp:                                â”‚
â”‚   B's write wins, even though A wrote later in real time.               â”‚
â”‚                                                                         â”‚
â”‚   Result: Your "consistent" system just lost data.                      â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Clock skew in practice:**
- NTP can drift 10-100ms between syncs
- Cloud VMs can have sudden clock jumps (especially on VM migration)
- Leap seconds cause chaos
- Even Google's TrueTime has 7ms uncertainty bounds

**Lesson:** Clocks are useful for *ordering* but not for *coordination*. Never use timestamps as the sole arbitration mechanism for critical decisions.

### 2.5 Logical Clocks and Time in Distributed Systems

Since physical clocks can't be trusted, distributed systems use **logical clocks** to establish ordering.

#### 2.5.1 Lamport Clocks

```
LAMPORT CLOCK (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Rules:
  1. Before each event: clock++
  2. When sending: clock++, attach clock to message
  3. When receiving: clock = max(local, received) + 1

Property: If A happened-before B, then L(A) < L(B)
Warning:  L(A) < L(B) does NOT imply A happened-before B

Example:
  Node A: tick() â†’ clock=1, send() â†’ clock=2 (message carries 2)
  Node B: receive(2) â†’ clock = max(0, 2) + 1 = 3
  
  Conclusion: A's event (2) happened-before B's event (3)
```

#### 2.5.2 Vector Clocks

```
VECTOR CLOCK (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Unlike Lamport clocks, vector clocks detect concurrent events.
Each node maintains: VC[i] = "events I know about from node i"

Operations:
  tick():     VC[self]++
  send():     VC[self]++, attach VC to message
  receive(R): for each i: VC[i] = max(VC[i], R[i]), then VC[self]++

Compare(VC1, VC2):
  - If all VC1[i] â‰¤ VC2[i] and at least one <  â†’ VC1 happened-before VC2
  - If all VC1[i] â‰¥ VC2[i] and at least one >  â†’ VC2 happened-before VC1
  - If some VC1[i] < VC2[i] AND some VC1[j] > VC2[j] â†’ CONCURRENT!

Example:
  A.tick() â†’ {A:1, B:0, C:0}
  B.tick() â†’ {A:0, B:1, C:0}   (no communication)
  
  Compare: A has A>0 but B<1, B has B>0 but A<1 â†’ CONCURRENT
  Result: Need conflict resolution (LWW, merge, etc.)
```

#### 2.5.3 Hybrid Logical Clocks (HLC)

```
HYBRID LOGICAL CLOCK (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Format: (physical_time, logical_counter)
Used by: CockroachDB, MongoDB, TiDB

now():
  wall = get_wall_time()
  if wall > physical:
    physical = wall, logical = 0
  else:
    logical++
  return (physical, logical)

receive(recv_physical, recv_logical):
  wall = get_wall_time()
  if wall > max(physical, recv_physical):
    physical = wall, logical = 0
  elif physical > recv_physical:
    logical++
  elif recv_physical > physical:
    physical = recv_physical, logical = recv_logical + 1
  else:
    logical = max(logical, recv_logical) + 1

Example:
  ts1 = now() â†’ (1642000000000, 0)
  ts2 = now() â†’ (1642000000000, 1)  â† same ms, logical incremented
  [wait 1ms]
  ts3 = now() â†’ (1642000000001, 0)  â† new ms, logical reset

Benefit: Orderable timestamps that track real time
```

#### 2.5.4 Google TrueTime

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          GOOGLE TRUETIME                                â”‚
â”‚                                                                         â”‚
â”‚   TrueTime doesn't give you a single timestamp.                         â”‚
â”‚   It gives you an INTERVAL: [earliest, latest]                          â”‚
â”‚                                                                         â”‚
â”‚   API:                                                                  â”‚
â”‚   - TT.now() â†’ returns TTinterval [earliest, latest]                    â”‚
â”‚   - TT.after(t) â†’ true if t is definitely in the past                   â”‚
â”‚   - TT.before(t) â†’ true if t is definitely in the future                â”‚
â”‚                                                                         â”‚
â”‚   Implementation:                                                       â”‚
â”‚   - GPS receivers in every datacenter                                   â”‚
â”‚   - Atomic clocks as backup                                             â”‚
â”‚   - Uncertainty bound: typically 1-7ms                                  â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                                                                 â”‚   â”‚
â”‚   â”‚   Real time:      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶        â”‚   â”‚
â”‚   â”‚                             â”‚                                   â”‚   â”‚
â”‚   â”‚   TT.now():        [earliestâ”‚â”€â”€â”€â”€â”€â”€â”€â”‚latest]                    â”‚   â”‚
â”‚   â”‚                             â”‚       â”‚                           â”‚   â”‚
â”‚   â”‚                             â”‚â—€â”€â”€â”€â”€â”€â–¶â”‚                           â”‚   â”‚
â”‚   â”‚                             uncertainty                         â”‚   â”‚
â”‚   â”‚                             (Îµ â‰ˆ 7ms)                           â”‚   â”‚
â”‚   â”‚                                                                 â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚   SPANNER'S COMMIT WAIT:                                                â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚
â”‚   After committing, Spanner waits until TT.after(commit_time) is true.  â”‚
â”‚   This guarantees the commit timestamp is definitely in the past.       â”‚
â”‚   Cost: ~7ms added to every write.                                      â”‚
â”‚   Benefit: External consistency (linearizability) without locks!        â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```
TRUETIME API (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TT.now()      â†’ returns interval [earliest, latest]
TT.after(t)   â†’ true if t is DEFINITELY in the past
TT.before(t)  â†’ true if t is DEFINITELY in the future

SPANNER COMMIT WAIT:
  1. commit_ts = TT.now().latest
  2. apply_writes(commit_ts)
  3. wait until TT.after(commit_ts)  // ~7ms wait
  4. return commit_ts

Guarantee: Any future transaction sees our writes,
           any past transaction does not.
           = External consistency (linearizability) without locks!
```

### 2.6 Clock Synchronization Protocols

| Protocol | Accuracy | Use Case |
|----------|----------|----------|
| **NTP** | 10-100ms | General purpose |
| **PTP (IEEE 1588)** | 1Î¼s-1ms | Financial, telecom |
| **GPS** | ~10ns | Spanner, TrueTime |
| **Atomic Clocks** | ~1ns | Backup for GPS |

**Staff-Level Insight:** The choice of clock synchronization affects your entire system design:
- **NTP-only:** Use logical clocks, assume 100ms+ skew
- **PTP:** Can use physical timestamps with ~1ms uncertainty
- **TrueTime:** Can achieve external consistency with commit-wait

---

<a name="leader-election"></a>
## 3. Leader Election: Crowning a King in a Democracy

### 3.1 What Leader Election Solves

Many distributed systems need a single authoritative node:

| Use Case | Why One Leader? |
|----------|-----------------|
| Database primary | Single source of truth for writes |
| Job scheduler | Avoid duplicate job execution |
| Metadata service | Consistent view of cluster state |
| Distributed lock service | Coordinate lock ownership |
| Message queue coordinator | Assign partitions to consumers |

Without leader election, you have two unpalatable options:
1. **No coordination:** Everyone acts independently (chaos, duplication, conflicts)
2. **Static configuration:** Hardcode the leader (single point of failure, manual intervention)

Leader election gives you **dynamic, automatic failover** with **exactly one leader** at any time.

### 3.2 How Leader Election Works (Conceptually)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      LEADER ELECTION LIFECYCLE                          â”‚
â”‚                                                                         â”‚
â”‚   PHASE 1: ELECTION                                                     â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                     â”‚
â”‚                                                                         â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”                                       â”‚
â”‚     â”‚  A  â”‚    â”‚  B  â”‚    â”‚  C  â”‚    All nodes: "I want to be leader!"  â”‚
â”‚     â”‚ ğŸ—³ï¸  |     â”‚ ğŸ—³ï¸  â”‚    â”‚ ğŸ—³ï¸  â”‚                                       â”‚
â”‚     â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚                                                                         â”‚
â”‚   PHASE 2: VOTING / CONSENSUS                                           â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                            â”‚
â”‚                                                                         â”‚
â”‚     Nodes exchange votes based on:                                      â”‚
â”‚     - Who has the most up-to-date data?                                 â”‚
â”‚     - Who has the highest ID? (tie-breaker)                             â”‚
â”‚     - Who can reach majority of nodes?                                  â”‚
â”‚                                                                         â”‚
â”‚   PHASE 3: LEADERSHIP                                                   â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                   â”‚
â”‚                                                                         â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”                                       â”‚
â”‚     â”‚  A  â”‚    â”‚  B  â”‚    â”‚  C  â”‚                                       â”‚
â”‚     â”‚ ğŸ‘‘  â”‚â”€â”€â”€â–¶â”‚     â”‚â”€â”€â”€â–¶â”‚     â”‚    A is leader, B and C are followers â”‚
â”‚     â”‚LEAD â”‚    â”‚FOLL â”‚    â”‚FOLL â”‚                                       â”‚
â”‚     â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚                                                                         â”‚
â”‚   PHASE 4: HEARTBEAT                                                    â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                    â”‚
â”‚                                                                         â”‚
â”‚     Leader sends periodic heartbeats                                    â”‚
â”‚     Followers reset election timer on each heartbeat                    â”‚
â”‚     If no heartbeat for timeout period â†’ back to Phase 1                â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.3 Leader Election Mechanisms

#### Mechanism 1: Lease-Based Leadership

```
LEASE-BASED LEADERSHIP (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Constants: LEASE_TTL = 10s, RENEWAL_INTERVAL = 3s

try_become_leader():
  acquired = store.SET_IF_NOT_EXISTS("leader", node_id, TTL=10s)
        if acquired:
    is_leader = true
    start renewal_loop

renewal_loop:
  while is_leader:
    sleep(RENEWAL_INTERVAL)
    renewed = store.SET_IF_EQUALS("leader", node_id, TTL=10s)
            if not renewed:
      is_leader = false
      on_leadership_lost()  // STOP ALL LEADER ACTIVITIES

on_leadership_lost():
  // CRITICAL: Immediately stop processing, close connections
  // Do NOT assume you're still the leader
```

**Key Properties:**
- Leader must actively renew lease
- If network partitions leader from lease store, leadership is lost automatically
- No split-brain: old leader's lease expires before new leader can acquire

#### Mechanism 2: Quorum-Based Election

```
QUORUM-BASED ELECTION (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

quorum_size = (num_peers / 2) + 1

start_election():
  term++
  votes = 1  // vote for self
  
  for each peer:
    response = peer.request_vote(candidate=self, term)
    if response.granted: votes++
  
  if votes >= quorum_size:
    become_leader()
        else:
    wait_random_timeout()
    maybe retry

heartbeat_loop():
  while i_am_leader:
            acks = 0
    for each peer:
      if peer.heartbeat(leader=self, term): acks++
    
    if acks < quorum_size - 1:
      step_down()  // lost quorum
    
    sleep(HEARTBEAT_INTERVAL)
```

### 3.4 What Leader Election Introduces (The Costs)

Leader election solves problems but creates new ones:

| Cost | Description |
|------|-------------|
| **Unavailability during election** | No leader = no progress for leader-dependent operations |
| **Election storms** | Under network instability, repeated elections waste resources |
| **Split-brain risk** | Improper implementation can lead to two leaders |
| **Leader bottleneck** | All coordination through one node limits throughput |
| **Failover latency** | Time between leader death and new leader = downtime |

**Real Example: ZooKeeper Election Storm**

```
Timeline of an actual incident:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
00:00 - Leader dies (hardware failure)
00:01 - Followers detect missing heartbeat
00:02 - Election starts, Node B wins
00:03 - Node B dies (same hardware issue, shared rack)
00:04 - Another election, Node C wins
00:05 - Network glitch, C appears dead
00:06 - Election again, Node A (recovered) wins
00:07 - Cluster finally stable

7 minutes of instability, 3 elections, 0 progress on actual work.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

**Lesson:** Leader election is not free. Design for fast elections, but also design your system to tolerate brief periods without a leader.

---

<a name="distributed-locks"></a>
## 4. Distributed Locks: The Double-Edged Sword

### 4.1 What Distributed Locks Promise

A distributed lock is supposed to provide **mutual exclusion** across multiple machines:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DISTRIBUTED LOCK CONCEPT                           â”‚
â”‚                                                                         â”‚
â”‚   Goal: Only ONE process executes the critical section at a time        â”‚
â”‚                                                                         â”‚
â”‚   Process A        Lock Service        Process B                        â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€                        â”‚
â”‚       â”‚                 â”‚                  â”‚                            â”‚
â”‚       â”‚â”€â”€acquire()â”€â”€â”€â”€â”€â–¶â”‚                  â”‚                            â”‚
â”‚       â”‚â—€â”€â”€â”€â”€grantedâ”€â”€â”€â”€â”€â”‚                  â”‚                            â”‚
â”‚       â”‚                 â”‚â—€â”€â”€acquire()â”€â”€â”€â”€â”€â”€â”‚                            â”‚
â”‚       â”‚    [critical    â”‚â”€â”€â”€â”€blockedâ”€â”€â”€â”€â”€â”€â–¶â”‚                            â”‚
â”‚       â”‚     section]    â”‚                  â”‚                            â”‚
â”‚       â”‚â”€â”€release()â”€â”€â”€â”€â”€â–¶â”‚                  â”‚                            â”‚
â”‚       â”‚                 â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”€â”€â”‚                            â”‚
â”‚       â”‚                 â”‚â”€â”€â”€â”€grantedâ”€â”€â”€â”€â”€â”€â–¶â”‚                            â”‚
â”‚       â”‚                 â”‚                  â”‚   [critical section]       â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 The Redlock Controversy: Why This Is Harder Than It Looks

Redis's Redlock algorithm was proposed as a distributed lock. Martin Kleppmann (author of "Designing Data-Intensive Applications") famously critiqued it. The debate reveals fundamental issues:

**The Problem:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE DISTRIBUTED LOCK RACE CONDITION                  â”‚
â”‚                                                                         â”‚
â”‚   Timeline:                                                             â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶        â”‚
â”‚                                                                         â”‚
â”‚   T1: Client A acquires lock (TTL = 10 seconds)                         â”‚
â”‚   T2: Client A starts long GC pause (or network issue)                  â”‚
â”‚   T3: Lock expires (A doesn't know, still paused)                       â”‚
â”‚   T4: Client B acquires lock (valid!)                                   â”‚
â”‚   T5: Client A wakes up, thinks it still has lock                       â”‚
â”‚   T6: Both A and B execute critical section simultaneously!             â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚   â”‚ Client A â”‚ â†â”€â”€ Thinks it has lock â”€â”€â†’   â”‚ Client B â”‚                â”‚
â”‚   â”‚          â”‚                              â”‚          â”‚                â”‚
â”‚   â”‚ [writes] â”‚                              â”‚ [writes] â”‚                â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                         âš ï¸ DATA CORRUPTION âš ï¸                            â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**This happens because:**
1. Clocks can drift (TTL expires sooner/later than expected)
2. GC pauses can freeze a process for seconds
3. Network delays can make a process appear dead when it's not
4. The lock holder has no way to know the lock has expired

### 4.3 Fencing Tokens: The Solution

```
FENCING TOKENS (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

LOCK SERVICE:
  acquire(client_id, ttl):
    if lock_free or lock_expired:
      token++
      holder = client_id
      expiry = now + ttl
      return {acquired: true, fencing_token: token}
    return {acquired: false}

PROTECTED RESOURCE:
  write(data, fencing_token):
    if fencing_token < highest_token_seen:
      REJECT("stale token")
    highest_token_seen = fencing_token
    do_write(data)
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FENCING TOKENS IN ACTION                             â”‚
â”‚                                                                         â”‚
â”‚   T1: Client A acquires lock, gets token=33                             â”‚
â”‚   T2: Client A pauses (GC)                                              â”‚
â”‚   T3: Lock expires                                                      â”‚
â”‚   T4: Client B acquires lock, gets token=34                             â”‚
â”‚   T5: Client B writes to storage with token=34                          â”‚
â”‚   T6: Storage records highest_token = 34                                â”‚
â”‚   T7: Client A wakes up, tries to write with token=33                   â”‚
â”‚   T8: Storage REJECTS write: 33 < 34                                    â”‚
â”‚                                                                         â”‚
â”‚   Result: Data integrity preserved!                                     â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Critical Insight:** Fencing tokens only work if the downstream resource checks them. If you're writing to a legacy database that doesn't understand fencing tokens, you're not protected.

### 4.4 Implementing Distributed Locks Correctly

```
REDIS DISTRIBUTED LOCK (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

acquire(timeout):
  lock_id = uuid()
  deadline = now + timeout
  
  while now < deadline:
    // Atomic: SET if not exists + increment fencing token
    result = redis.EVAL("""
      if not EXISTS(lock_key) then
        token = INCR(token_key)
        SET(lock_key, lock_id, EX=ttl)
                    return token
                return nil
    """)
    
    if result != nil:
      return result  // fencing token
    
    sleep(100ms)
  
  raise Timeout

release():
  // Atomic: DELETE only if we still own it
  redis.EVAL("""
    if GET(lock_key) == lock_id then
      DEL(lock_key)
  """)

Usage:
  token = lock.acquire()
  process_job(token)  // pass fencing token to storage
    lock.release()
```

### 4.5 The Hidden Costs of Distributed Locks

| Cost | Impact |
|------|--------|
| **Lock service is SPOF** | If lock service is down, all locked operations fail |
| **Latency** | Every lock operation adds network round-trip |
| **Deadlocks** | Complex lock hierarchies can deadlock |
| **Starvation** | Busy locks may starve some clients |
| **Reduced throughput** | Serialization limits parallelism |
| **Debugging difficulty** | "Who holds the lock?" is hard to answer |

### 4.6 Advanced Locking Patterns

#### 4.6.1 Read-Write Locks

```
READ-WRITE LOCK (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Rules:
  â€¢ Multiple readers can hold lock simultaneously
  â€¢ Writer needs exclusive access
  â€¢ Writer waits for all readers to finish

acquire_read():
  while timeout not expired:
    if no writer waiting:
      add self to readers
      reader_count++
      if still no writer: return success
      else: release and retry
  raise Timeout

acquire_write():
  if SET write_lock (NX): // claim writer slot
    while timeout not expired:
      if reader_count == 0:
        return success
    release and raise Timeout
  else:
    raise Contention

release_read(): reader_count--
release_write(): DEL write_lock (only if we own it)
```

#### 4.6.2 Hierarchical Locks (Lock Ordering)

```
HIERARCHICAL LOCKS (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Hierarchy: database (0) â†’ table (1) â†’ row (2)
Rule: Acquire parent before child. Release child before parent.

acquire(resource_type, resource_id):
  my_level = HIERARCHY[resource_type]
  
  for each held_lock:
    if held_lock.level > my_level:
      raise HierarchyViolation  // can't acquire parent while holding child
  
  acquire_actual_lock(resource_type, resource_id)

release(resource_type, resource_id):
  my_level = HIERARCHY[resource_type]
  
  for each held_lock:
    if held_lock.level > my_level:
      raise HierarchyViolation  // can't release parent while holding child
  
  release_actual_lock(resource_type, resource_id)

Example:
  âœ“ acquire(database) â†’ acquire(table) â†’ acquire(row)
  âœ“ release(row) â†’ release(table) â†’ release(database)
  âœ— acquire(row) â†’ acquire(table)  // VIOLATION!
```

#### 4.6.3 Try-Lock with Deadlock Detection

```
DEADLOCK DETECTION (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Uses wait-for graph: Process A waits for Process B â†’ edge Aâ†’B

register_wait(waiter, holder):
  add_edge(waiter â†’ holder)
  if has_cycle(waiter):
    remove_edge(waiter)
    return DeadlockDetected
  return null

has_cycle(start):
  visited = {}
  current = start
  while current != null:
    if current in visited: return true  // CYCLE!
    visited.add(current)
    current = graph.get_next(current)
  return false

DEADLOCK-AWARE LOCK:
  acquire(resource):
    while timeout not expired:
      if try_acquire(): return success
      
      holder = get_current_holder()
      if register_wait(self, holder) == Deadlock:
        raise DeadlockAbort  // victim chosen
      
      sleep(10ms)
    raise Timeout
```

#### 4.6.4 Intention Locks (Multi-Granularity Locking)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INTENTION LOCK HIERARCHY                             â”‚
â”‚                                                                         â”‚
â”‚   Lock Modes:                                                           â”‚
â”‚   - IS (Intention Shared): Intend to acquire S locks on descendants     â”‚
â”‚   - IX (Intention Exclusive): Intend to acquire X locks on descendants  â”‚
â”‚   - S (Shared): Read lock                                               â”‚
â”‚   - X (Exclusive): Write lock                                           â”‚
â”‚   - SIX (S + IX): Read this, intend to write descendants                â”‚
â”‚                                                                         â”‚
â”‚   Compatibility Matrix:                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”                                  â”‚
â”‚   â”‚        â”‚ IS â”‚ IX â”‚  S â”‚  X â”‚ SIX â”‚                                  â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤                                  â”‚
â”‚   â”‚   IS   â”‚ âœ“  â”‚ âœ“  â”‚ âœ“  â”‚ âœ—  â”‚  âœ“  â”‚                                  â”‚
â”‚   â”‚   IX   â”‚ âœ“  â”‚ âœ“  â”‚ âœ—  â”‚ âœ—  â”‚  âœ—  â”‚                                  â”‚
â”‚   â”‚    S   â”‚ âœ“  â”‚ âœ—  â”‚ âœ“  â”‚ âœ—  â”‚  âœ—  â”‚                                  â”‚
â”‚   â”‚    X   â”‚ âœ—  â”‚ âœ—  â”‚ âœ—  â”‚ âœ—  â”‚  âœ—  â”‚                                  â”‚
â”‚   â”‚  SIX   â”‚ âœ“  â”‚ âœ—  â”‚ âœ—  â”‚ âœ—  â”‚  âœ—  â”‚                                  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜                                  â”‚
â”‚                                                                         â”‚
â”‚   Example: Read table T1, write row R1                                  â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                 â”‚
â”‚   1. Acquire IS on Database                                             â”‚
â”‚   2. Acquire IX on Table T1                                             â”‚
â”‚   3. Acquire S on Table T1 (read whole table)                           â”‚
â”‚   4. Acquire X on Row R1 (write specific row)                           â”‚
â”‚                                                                         â”‚
â”‚   This allows other transactions to:                                    â”‚
â”‚   - Read other tables (compatible with IS on Database)                  â”‚
â”‚   - Write other rows in T1 (compatible with IX on Table)                â”‚
â”‚                                                                         â”‚
â”‚   But blocks:                                                           â”‚
â”‚   - Exclusive lock on T1 (our S lock blocks it)                         â”‚
â”‚   - Any lock on R1 (our X lock blocks it)                               â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```
INTENTION LOCKS (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Lock modes: IS (intention shared), IX (intention exclusive),
            S (shared), X (exclusive), SIX (S + IX)

acquire(txn, resource_path, mode):
  // resource_path = ["database", "table:users", "row:123"]
  
  intention = IS if mode == S else IX
  
  // Acquire intention locks on ancestors
  for ancestor in resource_path[:-1]:
    acquire_lock(txn, ancestor, intention)
  
  // Acquire actual lock on target
  acquire_lock(txn, resource_path[-1], mode)

acquire_lock(txn, resource, mode):
  for each (holder, holder_mode) on resource:
    if not COMPATIBLE[mode, holder_mode]:
      raise Conflict
  grant_lock(txn, resource, mode)
```

---

<a name="consensus"></a>
## 5. Consensus: The Foundation (High-Level)

### 5.1 What Consensus Actually Means

Consensus is getting a group of nodes to **agree on a single value**, even when:
- Some nodes may fail
- Messages may be lost or delayed
- There is no global clock

**The Consensus Guarantees:**

| Property | Meaning |
|----------|---------|
| **Agreement** | All non-faulty nodes decide on the same value |
| **Validity** | The decided value was proposed by some node |
| **Termination** | All non-faulty nodes eventually decide |

### 5.2 Why You Need Consensus (Without Knowing It)

Every time you use these, you're using consensus under the hood:

- **etcd, ZooKeeper, Consul:** Configuration stores using Raft/Paxos
- **Kafka:** Leader election for partition leadership
- **CockroachDB, TiDB:** Distributed transactions
- **Kubernetes:** etcd-backed cluster state

### 5.3 Consensus Trade-offs (No Algorithms, Just Intuition)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONSENSUS COST MODEL                                 â”‚
â”‚                                                                         â”‚
â”‚   For a write to be committed, it must be replicated to a QUORUM        â”‚
â”‚   (majority) of nodes.                                                  â”‚
â”‚                                                                         â”‚
â”‚   3-node cluster: quorum = 2 (survives 1 failure)                       â”‚
â”‚   5-node cluster: quorum = 3 (survives 2 failures)                      â”‚
â”‚   7-node cluster: quorum = 4 (survives 3 failures)                      â”‚
â”‚                                                                         â”‚
â”‚   Write latency = time to reach quorum (slowest of the fast majority)   â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”                   â”‚
â”‚   â”‚ 5ms â”‚    â”‚ 8ms â”‚    â”‚12ms â”‚    â”‚45ms â”‚    â”‚200msâ”‚                   â”‚
â”‚   â”‚Node1â”‚    â”‚Node2â”‚    â”‚Node3â”‚    â”‚Node4â”‚    â”‚Node5â”‚                   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²                                               â”‚
â”‚                         â”‚                                               â”‚
â”‚              Write commits after Node3 acks (12ms)                      â”‚
â”‚              (quorum of 3 reached)                                      â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Intuitions:**

1. **Odd numbers are better:** 3 nodes and 4 nodes both survive 1 failure, but 4 nodes need more communication.

2. **Quorum overlap guarantees consistency:** Any two quorums share at least one node, so no two conflicting decisions can both succeed.

3. **Leader bottleneck:** Most consensus protocols route all writes through a leader.

4. **Read optimization:** Reads can go to any node (with some consistency trade-offs) or only to leader (for strongest consistency).

### 5.4 When You Need Consensus

| Situation | Need Consensus? | Why |
|-----------|-----------------|-----|
| Picking a leader | âœ… Yes | Must agree on exactly one |
| Committing a transaction | âœ… Yes | All or nothing across nodes |
| Updating cluster configuration | âœ… Yes | All nodes must see same config |
| Incrementing a counter | âš ï¸ Maybe | Depends on accuracy requirements |
| Logging events | âŒ Usually no | Ordering often not critical |
| Caching | âŒ No | Eventual consistency is fine |

### 5.5 Raft Consensus Deep Dive

Understanding Raft is essential for Staff-level engineers. It's the consensus algorithm behind etcd, Consul, CockroachDB, and TiDB.

#### 5.5.1 Raft Core Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           RAFT ARCHITECTURE                             â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚                         REPLICATED LOG                           â”‚  â”‚
â”‚   â”‚                                                                  â”‚  â”‚
â”‚   â”‚   Index:  1       2       3       4       5       6              â”‚  â”‚
â”‚   â”‚         â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”            â”‚  â”‚
â”‚   â”‚  Term:  â”‚ 1 â”‚   â”‚ 1 â”‚   â”‚ 2 â”‚   â”‚ 2 â”‚   â”‚ 2 â”‚   â”‚ 3 â”‚            â”‚  â”‚
â”‚   â”‚         â”œâ”€â”€â”€â”¤   â”œâ”€â”€â”€â”¤   â”œâ”€â”€â”€â”¤   â”œâ”€â”€â”€â”¤   â”œâ”€â”€â”€â”¤   â”œâ”€â”€â”€â”¤            â”‚  â”‚
â”‚   â”‚  Cmd:   â”‚x=1â”‚   â”‚y=2â”‚   â”‚x=3â”‚   â”‚z=4â”‚   â”‚y=5â”‚   â”‚x=6â”‚            â”‚  â”‚
â”‚   â”‚         â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜            â”‚  â”‚
â”‚   â”‚                                   â–²                              â”‚  â”‚
â”‚   â”‚                             commitIndex                          â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                         â”‚
â”‚   Leader replicates log entries to followers.                           â”‚
â”‚   Entry is committed when replicated to majority.                       â”‚
â”‚   Committed entries are applied to state machine.                       â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 5.5.2 Raft State Machine

```
RAFT CORE (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

States: FOLLOWER, CANDIDATE, LEADER

Persistent state (survives restart):
  current_term, voted_for, log[]

Volatile state:
  commit_index, last_applied, state

Leader-only:
  next_index[peer], match_index[peer]

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ELECTION (on timeout):
  state = CANDIDATE
  term++
  voted_for = self
  votes = 1
  
  for each peer:
    if peer.request_vote(term, my_log_info).granted:
      votes++
  
  if votes > majority: become_leader()
  else: state = FOLLOWER

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

VOTE REQUEST HANDLER:
  if request.term > my_term:
    my_term = request.term
    state = FOLLOWER
  
  grant = (haven't voted OR voted for this candidate)
          AND candidate_log >= my_log
  
  if grant: voted_for = candidate
  return grant

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

LOG COMPARISON (who's more up-to-date):
  compare last_term first, then last_index
  higher term wins; if equal, longer log wins

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

CLIENT REQUEST (leader only):
  append entry to local log
  replicate to followers (AppendEntries RPC)
  wait until majority acks â†’ committed
  apply to state machine

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

APPEND_ENTRIES (heartbeat + log replication):
  send: term, prev_log_index, prev_log_term, entries, commit_index
  follower: reject if log doesn't match, accept and append if it does
  leader: on reject, decrement next_index and retry

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

COMMIT:
  entry committed when replicated to majority
  only commit entries from current term
```

#### 5.5.3 Raft Safety Properties

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        RAFT SAFETY GUARANTEES                           â”‚
â”‚                                                                         â”‚
â”‚   PROPERTY 1: Election Safety                                           â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                         â”‚
â”‚   At most one leader can be elected in a given term.                    â”‚
â”‚                                                                         â”‚
â”‚   Why: Each node votes once per term. Leader needs majority.            â”‚
â”‚        Two majorities always overlap, so only one can get majority.     â”‚
â”‚                                                                         â”‚
â”‚   PROPERTY 2: Leader Append-Only                                        â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                        â”‚
â”‚   A leader never overwrites or deletes entries in its log.              â”‚
â”‚   It only appends new entries.                                          â”‚
â”‚                                                                         â”‚
â”‚   PROPERTY 3: Log Matching                                              â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                          â”‚
â”‚   If two logs contain an entry with the same index and term,            â”‚
â”‚   then the logs are identical in all entries up to that index.          â”‚
â”‚                                                                         â”‚
â”‚   Why: AppendEntries includes prev_log_index and prev_log_term.         â”‚
â”‚        Follower rejects if they don't match, forcing backtrack.         â”‚
â”‚                                                                         â”‚
â”‚   PROPERTY 4: Leader Completeness                                       â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚
â”‚   If a log entry is committed in a given term, that entry will be       â”‚
â”‚   present in the logs of all leaders for higher terms.                  â”‚
â”‚                                                                         â”‚
â”‚   Why: Leader election requires up-to-date log. Committed entries       â”‚
â”‚        are on majority. New leader must have received votes from        â”‚
â”‚        at least one node with the committed entry.                      â”‚
â”‚                                                                         â”‚
â”‚   PROPERTY 5: State Machine Safety                                      â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                     â”‚
â”‚   If a server has applied a log entry at a given index,                 â”‚
â”‚   no other server will ever apply a different entry for that index.     â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 5.5.4 Raft Optimizations for Production

```
RAFT PRODUCTION OPTIMIZATIONS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. PRE-VOTE
   Before starting election, ask "would you vote for me?"
   If can't win, stay follower. Prevents term inflation from partitioned nodes.

2. PIPELINING
   Send multiple AppendEntries batches without waiting for each ack.
   Dramatically improves throughput.

3. LEARNER NODES
   Add new node as non-voting learner first.
   Replicate log to catch up. Then promote to voter.
   Prevents cluster disruption during scaling.

4. BATCHING
   Collect multiple client requests (e.g., 100 or wait 1ms).
   Single consensus round for the batch.
   Amortizes consensus cost.

5. READ LEASES
   Leader maintains lease (refreshed by heartbeats).
   If lease valid: serve read locally (no consensus).
   If expired: confirm leadership with quorum first.
```

#### 5.5.5 Raft vs Paxos Comparison

| Aspect | Raft | Multi-Paxos |
|--------|------|-------------|
| **Understandability** | Designed for clarity | Notoriously complex |
| **Leader** | Always required | Can be leaderless (basic Paxos) |
| **Log ordering** | Strictly ordered | Gaps allowed, fill later |
| **Membership change** | Joint consensus | Separate Paxos instance |
| **Performance** | 2 RTTs for writes | 2 RTTs (with stable leader) |
| **Implementations** | etcd, Consul, TiKV | Chubby (internal), Spanner |

### 5.6 Advanced Consensus Variants

#### 5.6.1 EPaxos (Egalitarian Paxos)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           EPAXOS OVERVIEW                               â”‚
â”‚                                                                         â”‚
â”‚   Unlike Raft/Multi-Paxos, EPaxos is LEADERLESS.                        â”‚
â”‚   Any node can propose commands directly.                               â”‚
â”‚                                                                         â”‚
â”‚   FAST PATH (no conflicts):                                             â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                             â”‚
â”‚   Proposer â”€â”€proposeâ”€â”€â–¶ Fast Quorum (F+1 nodes)                         â”‚
â”‚                              â”‚                                          â”‚
â”‚                              â–¼                                          â”‚
â”‚                         COMMITTED in 1 RTT!                             â”‚
â”‚                                                                         â”‚
â”‚   SLOW PATH (conflicts detected):                                       â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                      â”‚
â”‚   Proposer â”€â”€proposeâ”€â”€â–¶ Fast Quorum                                     â”‚
â”‚                         â”‚                                               â”‚
â”‚                    conflicts!                                           â”‚
â”‚                         â”‚                                               â”‚
â”‚            â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                               â”‚
â”‚   Proposer â”€â”€acceptâ”€â”€â”€â–¶ Classic Quorum (majority)                       â”‚
â”‚                              â”‚                                          â”‚
â”‚                              â–¼                                          â”‚
â”‚                         COMMITTED in 2 RTTs                             â”‚
â”‚                                                                         â”‚
â”‚   BENEFITS:                                                             â”‚
â”‚   - Lower latency for non-conflicting commands                          â”‚
â”‚   - No leader bottleneck                                                â”‚
â”‚   - Better geo-distribution (closest replica handles request)           â”‚
â”‚                                                                         â”‚
â”‚   DRAWBACKS:                                                            â”‚
â”‚   - Complex implementation                                              â”‚
â”‚   - Command interference detection overhead                             â”‚
â”‚   - Execution order requires dependency tracking                        â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 5.6.2 Flexible Paxos

```
FLEXIBLE PAXOS (Concept)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Key insight: Only quorum INTERSECTION matters, not quorum SIZE.

Traditional (5 nodes): Write=3, Read=3 (must overlap)
Flexible (5 nodes):    Write=4, Read=2 (still overlap! 4+2 > 5)

Use cases:
  â€¢ Read-heavy: smaller read quorum, larger write quorum
  â€¢ Write-heavy: smaller write quorum, larger read quorum

Invariant: write_quorum + read_quorum > num_nodes
```

### 5.7 Consistency Models Deep Dive

Understanding consistency models is essential for staff-level engineers. The choice of consistency model affects correctness, performance, and user experience.

#### 5.7.1 The Consistency Spectrum

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONSISTENCY MODEL SPECTRUM                           â”‚
â”‚                                                                         â”‚
â”‚   Strongest                                              Weakest        â”‚
â”‚   â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶      â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€-â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€--â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚Lineariza-  â”‚ â”‚Sequential â”‚ â”‚ Causal    â”‚ â”‚Read-yourâ”‚ â”‚Eventual   â”‚  â”‚
â”‚   â”‚bility      â”‚ â”‚Consistencyâ”‚ â”‚Consistencyâ”‚ â”‚-writes  â”‚ â”‚Consistencyâ”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€--â”€â”˜  â”‚
â”‚                                                                         â”‚
â”‚   "Real-time     "All see      "Causally    "See own   "Eventually      â”‚
â”‚    ordering"      same order"   related      writes"    converge"       â”‚
â”‚                                 ordered"                                â”‚
â”‚                                                                         â”‚
â”‚   PERFORMANCE COST:                                                     â”‚
â”‚   High â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Low  â”‚
â”‚                                                                         â”‚
â”‚   AVAILABILITY:                                                         â”‚
â”‚   Low â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ High â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 5.7.2 Linearizability (Strong Consistency)

```
LINEARIZABILITY EXPLAINED
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Definition: Every operation takes effect instantaneously at some point
            between invocation and response.

Key properties:
  1. Real-time ordering: If A completes before B starts â†’ A before B
  2. Single-copy semantics: Behaves as if there's one copy

Example (IS linearizable):
  Client A:  |--write(x=1)--|
  Client B:             |--read()â†’0--|    (write hasn't "taken effect" yet)
  Client C:                    |--read()â†’1--|

Example (NOT linearizable):
  Client A:  |--write(x=1)--|
  Client B:                      |--read()â†’1--|
  Client C:                               |--read()â†’0--|  â† VIOLATION!
  
  Once B sees 1, C cannot see 0. Values can't "un-happen".

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

IMPLEMENTING LINEARIZABLE READS:

Option 1 - ReadIndex (leader-based):
  1. Confirm still leader (heartbeat quorum)
  2. Wait for commit index to advance
  3. Read from state machine

Option 2 - Quorum read:
  1. Read from majority of nodes
  2. Return value with highest log index
```

#### 5.7.3 Sequential Consistency

```
SEQUENTIAL CONSISTENCY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Definition: All processes see operations in the SAME order,
            and each process's ops appear in program order.

Difference from linearizability: No real-time ordering required.

Example (sequentially consistent, NOT linearizable):

  Real time:
  Process 1:  write(x=1) ..................  read(y)â†’0
  Process 2:  .........  write(y=1) .......  read(x)â†’0

  Both read 0! Both writes completed before reads (in real time).
  NOT linearizable.

  BUT sequentially consistent with order:
    read(x)â†’0, read(y)â†’0, write(x=1), write(y=1)

  Both see same order, each process's ops in program order. âœ“
```

#### 5.7.4 Causal Consistency

```
CAUSAL CONSISTENCY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Definition: Only causally-related operations must be ordered.
  â€¢ If A depends on B â†’ all processes see B before A
  â€¢ Concurrent ops â†’ can appear in any order

Used by: MongoDB (default), Cassandra, DynamoDB

Implementation: Track dependencies with vector clocks

write(key, value):
  vector_clock.tick()
  store value with current vector_clock and dependencies
  
receive_write(write):
  if all dependencies satisfied:
    apply_write()
  else:
    buffer until dependencies arrive

Example:
  User A posts: "I'm getting married!"
  User B likes the post
  User C comments: "Congratulations!"

  Causal order: Post â†’ Like, Post â†’ Comment
  
  All replicas must show:
    â€¢ Like after Post âœ“
    â€¢ Comment after Post âœ“
    â€¢ Like vs Comment? Any order OK (concurrent)
```

#### 5.7.5 Consistency Model Comparison

| Model | Real-time Order | Total Order | Causal Order | Use Case |
|-------|-----------------|-------------|--------------|----------|
| **Linearizability** | âœ… Yes | âœ… Yes | âœ… Yes | Locks, counters, leader election |
| **Sequential** | âŒ No | âœ… Yes | âœ… Yes | Shared memory, caches |
| **Causal** | âŒ No | âŒ No | âœ… Yes | Social feeds, collaborative editing |
| **Eventual** | âŒ No | âŒ No | âŒ No | DNS, session stores |

#### 5.7.6 CAP Theorem and Consistency

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CAP THEOREM PRACTICAL GUIDE                          â”‚
â”‚                                                                         â”‚
â”‚   During a network partition, you must choose:                          â”‚
â”‚                                                                         â”‚
â”‚   CP (Consistency + Partition tolerance):                               â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                             â”‚
â”‚   - Sacrifice availability                                              â”‚
â”‚   - Minority partition cannot serve requests                            â”‚
â”‚   - Examples: etcd, ZooKeeper, Spanner                                  â”‚
â”‚                                                                         â”‚
â”‚   AP (Availability + Partition tolerance):                              â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                             â”‚
â”‚   - Sacrifice consistency                                               â”‚
â”‚   - All partitions can serve requests (may diverge)                     â”‚
â”‚   - Examples: Cassandra, DynamoDB, Riak                                 â”‚
â”‚                                                                         â”‚
â”‚   MODERN UNDERSTANDING:                                                 â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                â”‚
â”‚   - CAP is about the partition state, not normal operation              â”‚
â”‚   - During normal operation, you can have both C and A                  â”‚
â”‚   - The real question: "What happens during partition?"                 â”‚
â”‚                                                                         â”‚
â”‚   PACELC (more nuanced):                                                â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                               â”‚
â”‚   If Partition: choose Availability or Consistency                      â”‚
â”‚   Else (normal): choose Latency or Consistency                          â”‚
â”‚                                                                         â”‚
â”‚   Examples:                                                             â”‚
â”‚   - Spanner: PC/EC (Consistent always, sacrifice latency)               â”‚
â”‚   - Cassandra: PA/EL (Available in partition, low latency normally)     â”‚
â”‚   - MongoDB: PA/EC (Available in partition, consistent normally)        â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

<a name="failure-scenarios"></a>
## 6. Failure Scenarios That Will Ruin Your Week

### 6.1 Split Brain

The most dangerous failure mode in distributed coordination.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        SPLIT BRAIN SCENARIO                             â”‚
â”‚                                                                         â”‚
â”‚   Normal Operation:                                                     â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                      â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                    CLUSTER (5 nodes)                            â”‚   â”‚
â”‚   â”‚                                                                 â”‚   â”‚
â”‚   â”‚      â”Œâ”€â”€â”€â”    â”Œâ”€â”€â”€â”    â”Œâ”€â”€â”€â”    â”Œâ”€â”€â”€â”    â”Œâ”€â”€â”€â”                  â”‚   â”‚
â”‚   â”‚      â”‚ A â”‚â”€â”€â”€â”€â”‚ B â”‚â”€â”€â”€â”€â”‚ C â”‚â”€â”€â”€â”€â”‚ D â”‚â”€â”€â”€â”€â”‚ E â”‚                  â”‚   â”‚
â”‚   â”‚      â”‚(L)â”‚    â”‚   â”‚    â”‚   â”‚    â”‚   â”‚    â”‚   â”‚                  â”‚   â”‚
â”‚   â”‚      â””â”€â”€â”€â”˜    â””â”€â”€â”€â”˜    â””â”€â”€â”€â”˜    â””â”€â”€â”€â”˜    â””â”€â”€â”€â”˜                  â”‚   â”‚
â”‚   â”‚                                                                 â”‚   â”‚
â”‚   â”‚      A is the leader. All is well.                              â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚   Network Partition:                                                    â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                    â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚    PARTITION 1      â”‚  â•‘  â”‚         PARTITION 2                 â”‚   â”‚
â”‚   â”‚                     â”‚  â•‘  â”‚                                     â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”    â”Œâ”€â”€â”€â”    â”‚  â•‘  â”‚   â”Œâ”€â”€â”€â”    â”Œâ”€â”€â”€â”    â”Œâ”€â”€â”€â”           â”‚   â”‚
â”‚   â”‚   â”‚ A â”‚â”€â”€â”€â”€â”‚ B â”‚    â”‚  â•‘  â”‚   â”‚ C â”‚â”€â”€â”€â”€â”‚ D â”‚â”€â”€â”€â”€â”‚ E â”‚           â”‚   â”‚
â”‚   â”‚   â”‚(L)â”‚    â”‚   â”‚    â”‚  â•‘  â”‚   â”‚(L)â”‚    â”‚   â”‚    â”‚   â”‚           â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€â”˜    â””â”€â”€â”€â”˜    â”‚  â•‘  â”‚   â””â”€â”€â”€â”˜    â””â”€â”€â”€â”˜    â””â”€â”€â”€â”˜           â”‚   â”‚
â”‚   â”‚                     â”‚  â•‘  â”‚                                     â”‚   â”‚
â”‚   â”‚   A thinks it's     â”‚  â•‘  â”‚   C,D,E elect C as new leader       â”‚   â”‚
â”‚   â”‚   still leader      â”‚  â•‘  â”‚   (they have quorum!)               â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â•‘                                            â”‚
â”‚                      NETWORK PARTITION                                  â”‚
â”‚                                                                         â”‚
â”‚   TWO LEADERS! Both accepting writes! DATA DIVERGENCE!                  â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why This Happens:**
1. Network partition isolates minority (A, B) from majority (C, D, E)
2. A doesn't know it's partitionedâ€”from its view, C, D, E just stopped responding
3. C, D, E have quorum (3/5) and elect new leader
4. A continues accepting writes (unless it checks for quorum)

**Prevention:**

```python
class SplitBrainSafeLeader:
    """Leader that steps down if it loses quorum."""
    
    def heartbeat_loop(self):
        while self.is_leader:
            reachable = 0
            for peer in self.peers:
                try:
                    peer.heartbeat()
                    reachable += 1
                except Unreachable:
                    pass
            
            # Include self in count
            if (reachable + 1) < self.quorum_size:
                logging.critical(
                    "Lost quorum! Stepping down to prevent split-brain"
                )
                self.is_leader = False
                self.stop_accepting_writes()
            
            time.sleep(self.heartbeat_interval)
```

### 6.2 Partial Failure

In distributed systems, operations can half-succeedâ€”the worst possible outcome.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        PARTIAL FAILURE SCENARIO                         â”‚
â”‚                                                                         â”‚
â”‚   Operation: Transfer $100 from Account A to Account B                  â”‚
â”‚   Step 1: Deduct from A  â†’  SUCCESS                                     â”‚
â”‚   Step 2: Add to B       â†’  NETWORK TIMEOUT (??)                        â”‚
â”‚                                                                         â”‚
â”‚   What actually happened?                                               â”‚
â”‚                                                                         â”‚
â”‚   Option 1: The add failed (B has no money, A has less)                 â”‚
â”‚   Option 2: The add succeeded but ack was lost (both correct)           â”‚
â”‚   Option 3: The add is still in flight (will succeed later)             â”‚
â”‚                                                                         â”‚
â”‚   You cannot tell which happened!                                       â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Solutions:**

1. **Idempotency Keys:** Make operations safe to retry

```python
def transfer(from_account, to_account, amount, idempotency_key):
    # Check if already processed
    if db.exists(f"transfer:{idempotency_key}"):
        return db.get(f"transfer:{idempotency_key}")  # Return cached result
    
    # Process transfer
    result = do_transfer(from_account, to_account, amount)
    
    # Record result
    db.set(f"transfer:{idempotency_key}", result, ttl=86400)
    
    return result
```

2. **Saga Pattern:** Compensating transactions (covered in Part 2)

3. **Two-Phase Commit:** Prepare all parties before committing

### 6.2B Cascading Failure Timeline: When Coordination Collapses

Coordination failures cascade through dependent services in predictable patterns. Understanding this timeline helps Staff engineers design containment strategies and set realistic SLAs.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CASCADING FAILURE TIMELINE: ZOOKEEPER LEADER ELECTION           â”‚
â”‚                                                                         â”‚
â”‚   T+0:    ZooKeeper leader node experiences high GC pause (2 seconds)  â”‚
â”‚   T+2s:   Followers detect missed heartbeat, initiate leader election   â”‚
â”‚   T+3s:   All sessions with old leader enter "connection loss" state    â”‚
â”‚   T+5s:   Leader election completes, new leader elected                 â”‚
â”‚   T+5-15s: Session reconnection storm â€” all clients reconnect           â”‚
â”‚            simultaneously                                               â”‚
â”‚   T+8s:   New leader overwhelmed by session re-establishment            â”‚
â”‚   T+10s:  Lock holders uncertain â€” fencing tokens may be invalid        â”‚
â”‚   T+12s:  Applications with locks begin "safety timeout" â€” pause        â”‚
â”‚            operations                                                    â”‚
â”‚   T+15s:  Thundering herd: all paused operations retry simultaneously   â”‚
â”‚   T+20s:  New leader CPU at 100% from reconnection + lock reacquisition â”‚
â”‚   T+30s:  Some clients timeout and escalate to "coordination            â”‚
â”‚            unavailable" mode                                             â”‚
â”‚   T+60s:  Stability returns as reconnection storm subsides              â”‚
â”‚                                                                         â”‚
â”‚   User-Visible Impact:                                                  â”‚
â”‚   - 30-60 seconds of degraded service for all lock-dependent features   â”‚
â”‚   - Partial availability: some operations succeed, others timeout       â”‚
â”‚   - Error rates spike to 20-40% during recovery window                  â”‚
â”‚                                                                         â”‚
â”‚   Blast Radius:                                                         â”‚
â”‚   Every service using this ZK cluster is affected simultaneously.      â”‚
â”‚   If 50 services depend on the cluster, all 50 experience degradation.  â”‚
â”‚                                                                         â”‚
â”‚   Containment Strategies:                                               â”‚
â”‚   1. Separate ZK clusters per criticality tier (critical vs. non-       â”‚
â”‚      critical workloads)                                                â”‚
â”‚   2. Circuit breakers on ZK clients (fail fast when ZK is unhealthy)   â”‚
â”‚   3. Cached last-known-good state (allow degraded mode during outages) â”‚
â”‚                                                                         â”‚
â”‚   Prevention:                                                           â”‚
â”‚   - GC tuning (reduce pause times to < 100ms)                           â”‚
â”‚   - Dedicated coordination nodes (no co-located workloads)              â”‚
â”‚   - Connection pooling to limit reconnection storms                     â”‚
â”‚   - Exponential backoff on reconnection attempts                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Timeline Breakdown:**

- **T+0**: ZooKeeper leader node experiences high GC pause (2 seconds)
- **T+2s**: Followers detect missed heartbeat, initiate leader election
- **T+3s**: All sessions with old leader enter "connection loss" state
- **T+5s**: Leader election completes, new leader elected
- **T+5-15s**: Session reconnection stormâ€”all clients reconnect simultaneously
- **T+8s**: New leader overwhelmed by session re-establishment
- **T+10s**: Lock holders uncertainâ€”fencing tokens may be invalid
- **T+12s**: Applications with locks begin "safety timeout"â€”pause operations
- **T+15s**: Thundering herd: all paused operations retry simultaneously
- **T+20s**: New leader CPU at 100% from reconnection + lock reacquisition
- **T+30s**: Some clients timeout and escalate to "coordination unavailable" mode
- **T+60s**: Stability returns as reconnection storm subsides

**User-Visible Impact:**

30-60 seconds of degraded service for all lock-dependent features. Partial availability: some operations succeed, others timeout. Error rates spike to 20-40% during recovery window.

**Blast Radius:**

Every service using this ZK cluster is affected simultaneously. If 50 services depend on the cluster, all 50 experience degradation.

**Containment:**

- Separate ZK clusters per criticality tier (critical vs. non-critical workloads)
- Circuit breakers on ZK clients (fail fast when ZK is unhealthy)
- Cached last-known-good state (allow degraded mode during outages)

**Prevention:**

- GC tuning (reduce pause times to < 100ms)
- Dedicated coordination nodes (no co-located workloads)
- Connection pooling to limit reconnection storms
- Exponential backoff on reconnection attempts

### 6.3 Clock Skew

Clocks lie. Plan accordingly.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CLOCK SKEW DISASTER SCENARIOS                       â”‚
â”‚                                                                         â”‚
â”‚   SCENARIO 1: Lease Expires Early                                       â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                         â”‚
â”‚                                                                         â”‚
â”‚   Real time:    |-------- 10 seconds --------|                          â”‚
â”‚   Node A clock: |---- 8 seconds ----|                                   â”‚
â”‚                                     â†‘                                   â”‚
â”‚                         Node A thinks lease expired!                    â”‚
â”‚                         Stops working, but lease is actually valid.     â”‚
â”‚                                                                         â”‚
â”‚   SCENARIO 2: Lock Appears Available When It's Not                      â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                     â”‚
â”‚                                                                         â”‚
â”‚   Node A acquires lock at T=0 with TTL=10s                              â”‚
â”‚   Node B's clock is 15 seconds ahead                                    â”‚
â”‚   Node B thinks lock expired at T=0 (B's time shows T=15)               â”‚
â”‚   Node B takes lock while A still holds it!                             â”‚
â”‚                                                                         â”‚
â”‚   SCENARIO 3: Out-of-Order Events                                       â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                          â”‚
â”‚                                                                         â”‚
â”‚   Node A: Event at timestamp 100                                        â”‚
â”‚   Node B: Event at timestamp 95 (clock was behind)                      â”‚
â”‚   Log shows B happened before A, but A actually happened first!         â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Mitigations:**

| Approach | Description | Trade-off |
|----------|-------------|-----------|
| **Logical clocks** | Vector clocks, Lamport timestamps | No wall-clock time, complex |
| **Hybrid clocks** | Physical + logical (HLC) | Still needs bounded skew |
| **TrueTime (Google)** | GPS + atomic clocks, bounded uncertainty | Expensive hardware |
| **Conservative TTLs** | Account for worst-case skew | Longer lock durations |

```
CLOCK-SKEW-SAFE LOCK (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MAX_CLOCK_SKEW = 5 seconds (conservative)

acquire(ttl=30):
  // Use longer TTL to handle skew
  effective_ttl = ttl + MAX_CLOCK_SKEW
  lock_service.acquire(effective_ttl)
  
  // But locally, assume shorter validity (pessimistic)
  local_expiry = now + ttl - MAX_CLOCK_SKEW

is_still_valid():
  return now < local_expiry

Key: Be generous to others, conservative for yourself.
```

### 6.4 Failure Detection: How Do You Know a Node Is Dead?

In distributed systems, you can't distinguish between a dead node and a slow/partitioned one. Failure detection is about **probabilistic suspicion**, not certainty.

#### 6.4.1 The Phi Accrual Failure Detector

```
PHI ACCRUAL FAILURE DETECTOR
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Used by: Akka, Cassandra

Key idea: Instead of binary alive/dead, output a "suspicion level" (phi).
          Higher phi = more likely dead.

phi = -log10(P(heartbeat would arrive by now))

Interpretation:
  phi = 1  â†’  10% chance alive
  phi = 2  â†’   1% chance alive
  phi = 3  â†’  0.1% chance alive
  phi = 8  â†’  Threshold for "dead" (configurable)

Benefits:
  â€¢ Adapts to network conditions automatically
  â€¢ Uses historical heartbeat distribution
  â€¢ Configurable threshold per use case

Algorithm:
  1. Track heartbeat intervals in sliding window
  2. Calculate mean and std_dev of intervals
  3. When checking: how likely is current gap given history?
  4. If phi > threshold â†’ consider node dead
```

#### 6.4.2 SWIM Protocol (Scalable Weakly-consistent Infection-style Membership)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          SWIM PROTOCOL                                  â”‚
â”‚                                                                         â”‚
â”‚   Used by: Consul, Serf, HashiCorp Memberlist                           â”‚
â”‚                                                                         â”‚
â”‚   PROBLEM: Traditional heartbeat to all nodes = O(nÂ²) messages          â”‚
â”‚   SWIM: Achieves O(n) message complexity                                â”‚
â”‚                                                                         â”‚
â”‚   MECHANISM:                                                            â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                            â”‚
â”‚                                                                         â”‚
â”‚   1. DIRECT PROBE: Each period, pick random member, send ping           â”‚
â”‚                                                                         â”‚
â”‚      â”Œâ”€â”€â”€â”          ping           â”Œâ”€â”€â”€â”                                â”‚
â”‚      â”‚ A â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚ B â”‚                                â”‚
â”‚      â”‚   â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚   â”‚                                â”‚
â”‚      â””â”€â”€â”€â”˜          ack            â””â”€â”€â”€â”˜                                â”‚
â”‚                                                                         â”‚
â”‚   2. INDIRECT PROBE: If no ack, ask K random members to probe           â”‚
â”‚                                                                         â”‚
â”‚      â”Œâ”€â”€â”€â”   ping-req    â”Œâ”€â”€â”€â”    ping     â”Œâ”€â”€â”€â”                        â”‚
â”‚      â”‚ A â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚ C â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚ B â”‚                        â”‚
â”‚      â”‚   â”‚               â”‚   â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚   â”‚                        â”‚
â”‚      â”‚   â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚   â”‚    ack      â”‚   â”‚                        â”‚
â”‚      â””â”€â”€â”€â”˜     ack       â””â”€â”€â”€â”˜             â””â”€â”€â”€â”˜                        â”‚
â”‚                                                                         â”‚
â”‚   3. SUSPECT: If still no response, mark as SUSPECT (not dead yet)      â”‚
â”‚                                                                         â”‚
â”‚   4. CONFIRM DEAD: After timeout, mark as DEAD and disseminate          â”‚
â”‚                                                                         â”‚
â”‚   DISSEMINATION (Infection-style):                                      â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                     â”‚
â”‚   Membership updates piggybacked on protocol messages                   â”‚
â”‚   Spreads like gossip: log(n) rounds to reach all members               â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```
SWIM PROTOCOL (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

State: alive{}, suspected{}, dead{}

protocol_round():
  target = pick_random_member()
  
  // Step 1: Direct probe
  if ping(target).success:
    mark_alive(target)
    return
  
  // Step 2: Indirect probe (ask K others to ping target)
  for prober in random_k_members(3):
    if prober.ping(target).success:
      mark_alive(target)
      return
  
  // Step 3: Mark as suspect (not dead yet!)
  mark_suspect(target)

check_suspect_timeout():
  for each suspected member:
    if now - suspect_time > TIMEOUT:
      mark_dead(member)

refute_suspicion():
  // If I'm suspected, increment my incarnation to prove I'm alive
  incarnation++
  broadcast("ALIVE", self, incarnation)

Key insight: Incarnation numbers let you refute suspicion.
             Higher incarnation = more recent state.
```

#### 6.4.3 Failure Detection Trade-offs

| Detector Type | Detection Time | False Positive Rate | Network Cost |
|---------------|----------------|--------------------| -------------|
| **Fixed timeout** | Fast | High when network varies | Low |
| **Phi Accrual** | Adaptive | Low (self-tuning) | Low |
| **SWIM** | Medium | Low | O(n) total |
| **All-to-all heartbeat** | Fast | Low | O(nÂ²) |

**Staff-Level Insight:** The choice of failure detector affects your SLOs:
- **Fast detection (1s):** More false positives, more failovers, more disruption
- **Slow detection (30s):** Fewer false positives, but longer outages
- **Phi Accrual:** Self-tuning, but complex to operate
- **SWIM:** Scales well, but membership changes are eventually consistent

---

<a name="case-study-job-scheduler"></a>
## 7. Case Study: Job Scheduler

### 7.1 The Problem

Design a distributed job scheduler that:
- Runs jobs at scheduled times
- Ensures each job runs **exactly once**
- Handles worker failures
- Scales horizontally

### 7.2 Naive Approach (And Why It Fails)

```
BROKEN JOB SCHEDULER
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

poll_and_run():
  while true:
    job = db.query("SELECT ... WHERE status='pending' LIMIT 1")
            if job:
      db.execute("UPDATE ... SET status='running' WHERE id=?", job.id)
      run_job(job)
      db.execute("UPDATE ... SET status='completed' WHERE id=?", job.id)
    sleep(1)

PROBLEMS:
  1. Two workers see same job â†’ duplicate execution!
  2. Worker crashes after claiming â†’ job stuck in 'running'
  3. Race between SELECT and UPDATE â†’ lost updates
```

**Failure Modes:**
1. Two workers grab the same job (duplicate execution)
2. Worker crashes after claiming job (job stuck in 'running')
3. No coordination on which jobs to prioritize

### 7.3 Correct Approach: Leader-Based Scheduler

```
COORDINATED JOB SCHEDULER (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

LEADER (assigns jobs):
  jobs = get_pending_jobs()
  workers = get_active_workers()
  
  for job in jobs:
    assign to next worker (round-robin)
    push job_id to worker's queue

WORKER (executes jobs):
  register_as_active()
  job_id = pop_from_my_queue()
        
        if job_id:
    lock = acquire_lock(job_id)
    try:
      // Double-check still assigned to me
      if job.worker_id != me: return
      
      set_status('running')
      result = run_job(job)
      set_status('completed', fencing_token=lock.token)
    except:
      set_status('failed', error)
        finally:
      release_lock()
```

### 7.4 What Happens When Coordination Fails

| Failure | Impact | Mitigation |
|---------|--------|------------|
| Leader dies | New jobs not assigned until new leader | Fast election (< 10s), job queue buffers |
| Redis down | No locks, no queues | Graceful degradation, local queue fallback |
| Worker dies mid-job | Job stuck in 'running' | Timeout-based job reclamation |
| Network partition | Workers can't reach leader | Local job buffering, eventual sync |

```
JOB RECLAIMER (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

run():
  every 60 seconds:
    stuck = find_jobs(status='running', updated_at < 5_min_ago)
    
    for job in stuck:
      if not is_worker_alive(job.worker_id):
        reset_job(status='pending', worker=NULL, attempts++)
        log("Reclaimed stuck job from dead worker")
```

---

<a name="case-study-rate-limiter"></a>
## 8. Case Study: Rate Limiter Coordination

### 8.1 The Problem

Implement a rate limiter that:
- Limits requests per user per time window
- Works across multiple servers
- Has sub-millisecond latency
- Is reasonably accurate (not perfect)

### 8.2 The Coordination Spectrum

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RATE LIMITER COORDINATION SPECTRUM                   â”‚
â”‚                                                                         â”‚
â”‚   â† Less Coordination                    More Coordination â†’            â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚   Local      â”‚  â”‚   Periodic   â”‚  â”‚   Real-time  â”‚  â”‚  Central  â”‚   â”‚
â”‚   â”‚   Counters   â”‚  â”‚   Sync       â”‚  â”‚   Sync       â”‚  â”‚  Counter  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚   Accuracy:  â­          â­â­â­          â­â­â­â­â­       â­â­â­â­â­         â”‚
â”‚   Latency:   â­â­â­â­â­     â­â­â­â­         â­â­â­          â­â­            â”‚
â”‚   Complexity: â­          â­â­           â­â­â­â­        â­â­â­             â”‚
â”‚   Fault Tol: â­â­â­â­â­     â­â­â­â­         â­â­           â­              â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 8.3 Approach 1: Local Counters (No Coordination)

```
LOCAL RATE LIMITER (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

is_allowed(user_id):
  if counters[user_id] >= limit:
    return false
  counters[user_id]++
  return true

PROBLEMS:
  â€¢ Same server â†’ get 1/N of limit
  â€¢ Spread across servers â†’ get NÃ— limit
  â€¢ Load balancer changes â†’ unpredictable limits
```

**Problems:**
- If user hits same server, they get 1/N of actual limit
- If user spreads across servers, they get NÃ— actual limit
- Load balancer changes can drastically change effective limit

### 8.4 Approach 2: Periodic Sync (Light Coordination)

```
PERIODIC SYNC RATE LIMITER (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

FAST PATH (every request):
  is_allowed(user_id):
    if local_counts[user_id] >= local_limits[user_id]:
      return false
    local_counts[user_id]++
    return true

BACKGROUND SYNC (every 1 second):
  sync():
    // Push local counts to Redis
    for user_id, count in local_counts:
      redis.INCRBY(f"rate:{user_id}:{window}", count)
    local_counts.clear()
    
    // Get updated global counts, recalculate local limits
    for user_id:
      global_count = redis.GET(...)
      remaining = global_limit - global_count
      local_limits[user_id] = remaining / num_servers
```

### 8.5 What Happens When Coordination Fails

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  RATE LIMITER DEGRADATION MODES                         â”‚
â”‚                                                                         â”‚
â”‚   REDIS DOWN:                                                           â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                          â”‚
â”‚                                                                         â”‚
â”‚   Option A: Fail Open (allow all requests)                              â”‚
â”‚     - Risk: No rate limiting, system may be overwhelmed                 â”‚
â”‚     - Use when: Rate limiting is best-effort                            â”‚
â”‚                                                                         â”‚
â”‚   Option B: Fail Closed (deny all requests)                             â”‚
â”‚     - Risk: Legitimate traffic blocked                                  â”‚
â”‚     - Use when: Protecting critical resources                           â”‚
â”‚                                                                         â”‚
â”‚   Option C: Local-only mode (fall back to local counters)               â”‚
â”‚     - Risk: Inaccurate limits, but still some protection                â”‚
â”‚     - Use when: "Best effort is good enough"                            â”‚
â”‚                                                                         â”‚
â”‚   NETWORK PARTITION:                                                    â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                    â”‚
â”‚                                                                         â”‚
â”‚   Servers can't sync with each other.                                   â”‚
â”‚   Each server uses own local view.                                      â”‚
â”‚   Effective limit = local_limit Ã— num_partitioned_servers               â”‚
â”‚                                                                         â”‚
â”‚   Example: 3 servers, limit 100/min, partition isolates 2               â”‚
â”‚   Group 1 (1 server): allows 33/min                                     â”‚
â”‚   Group 2 (2 servers): allows 66/min                                    â”‚
â”‚   Total possible: 99/min (close enough!)                                â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```
RESILIENT RATE LIMITER (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

is_allowed(user_id):
  try:
    return check_distributed(user_id)
  catch RedisError:
    log("Redis down, using local mode")
    return check_local(user_id)

check_local(user_id):
  local_limit = global_limit / expected_server_count
  return local_counter.check(user_id, local_limit)
```

---

<a name="case-study-metadata-service"></a>
## 9. Case Study: Metadata Service

### 9.1 The Problem

Design a metadata service that:
- Stores cluster configuration (shard mappings, feature flags, etc.)
- Must be strongly consistent (all nodes see same config)
- Must be highly available
- Used by hundreds of services for every request

This is essentially what etcd, ZooKeeper, and Consul do.

### 9.2 The Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    METADATA SERVICE ARCHITECTURE                        â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                     METADATA CLUSTER (3-5 nodes)                â”‚   â”‚
â”‚   â”‚                                                                 â”‚   â”‚
â”‚   â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚   â”‚
â”‚   â”‚    â”‚  Node 1 â”‚â—€â”€â”€â–¶â”‚  Node 2 â”‚â—€â”€â”€â–¶â”‚  Node 3 â”‚                    â”‚   â”‚
â”‚   â”‚    â”‚         â”‚    â”‚         â”‚    â”‚         â”‚                    â”‚   â”‚
â”‚   â”‚    â”‚ LEADER  â”‚    â”‚FOLLOWER â”‚    â”‚FOLLOWER â”‚                    â”‚   â”‚
â”‚   â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                    â”‚   â”‚
â”‚   â”‚         â”‚              â”‚              â”‚                         â”‚   â”‚
â”‚   â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚   â”‚
â”‚   â”‚                        â”‚                                        â”‚   â”‚
â”‚   â”‚                   RAFT CONSENSUS                                â”‚   â”‚
â”‚   â”‚              (Writes go through leader,                         â”‚   â”‚
â”‚   â”‚               replicated to quorum)                             â”‚   â”‚
â”‚   â”‚                                                                 â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â”‚                                          â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚              â–¼               â–¼               â–¼                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚   â”‚   Service A  â”‚  â”‚   Service B  â”‚  â”‚   Service C  â”‚                  â”‚
â”‚   â”‚              â”‚  â”‚              â”‚  â”‚              â”‚                  â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                  â”‚
â”‚   â”‚  â”‚ LOCAL  â”‚  â”‚  â”‚  â”‚ LOCAL  â”‚  â”‚  â”‚  â”‚ LOCAL  â”‚  â”‚                  â”‚
â”‚   â”‚  â”‚ CACHE  â”‚  â”‚  â”‚  â”‚ CACHE  â”‚  â”‚  â”‚  â”‚ CACHE  â”‚  â”‚                  â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                                         â”‚
â”‚   Services cache metadata locally, subscribe to updates via WATCH       â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 9.3 Coordination Patterns in Metadata Service

#### Pattern 1: Leader Election via Metadata Service

```
LEADER ELECTION VIA ETCD (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

campaign():
  lease = etcd.create_lease(TTL=10s)
  
  success = etcd.transaction(
    compare: key doesn't exist (version == 0)
    success: PUT key=node_id with lease
    failure: []
        )
        
        if success:
    start_keep_alive_loop()
    return true
  return false

keep_alive():
  while is_leader:
    lease.refresh()
    sleep(3s)  // well before 10s TTL
```

#### Pattern 2: Distributed Lock via Metadata Service

```
DISTRIBUTED LOCK VIA ETCD (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

acquire(timeout):
  while not timed_out:
    lease = etcd.create_lease(10s)
    
    success = etcd.transaction(
      compare: key doesn't exist
      success: PUT key="locked" with lease
      failure: GET key (see who has it)
    )
    
    if success: return true
    
    // Wait for lock release
    etcd.watch(key).wait_for_delete()
  
  raise Timeout

release():
  lease.revoke()  // automatically deletes key
```

### 9.4 What Happens When Metadata Service Fails

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                METADATA SERVICE FAILURE SCENARIOS                       â”‚
â”‚                                                                         â”‚
â”‚   SCENARIO 1: Leader Failure                                            â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                         â”‚
â”‚                                                                         â”‚
â”‚   Impact:                                                               â”‚
â”‚   - Writes blocked for 1-10 seconds (election time)                     â”‚
â”‚   - Reads can continue from followers                                   â”‚
â”‚   - Watches may miss events during transition                           â”‚
â”‚                                                                         â”‚
â”‚   Mitigation:                                                           â”‚
â”‚   - Clients retry with backoff                                          â”‚
â”‚   - Clients cache last known good config                                â”‚
â”‚   - Fast election (sub-second with good config)                         â”‚
â”‚                                                                         â”‚
â”‚   SCENARIO 2: Loss of Quorum                                            â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                            â”‚
â”‚                                                                         â”‚
â”‚   Impact:                                                               â”‚
â”‚   - ALL operations blocked (reads and writes)                           â”‚
â”‚   - Service completely unavailable                                      â”‚
â”‚   - All dependent services affected                                     â”‚
â”‚                                                                         â”‚
â”‚   Mitigation:                                                           â”‚
â”‚   - 5-node cluster (survives 2 failures) instead of 3                   â”‚
â”‚   - Cross-AZ deployment                                                 â”‚
â”‚   - Clients use cached config with degraded mode                        â”‚
â”‚                                                                         â”‚
â”‚   SCENARIO 3: Network Partition                                         â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                         â”‚
â”‚                                                                         â”‚
â”‚   Impact:                                                               â”‚
â”‚   - Minority partition: can't read or write                             â”‚
â”‚   - Majority partition: continues operating                             â”‚
â”‚   - Clients in minority partition lose access                           â”‚
â”‚                                                                         â”‚
â”‚   Mitigation:                                                           â”‚
â”‚   - Clients should cache and operate in degraded mode                   â”‚
â”‚   - Alert on partition immediately                                      â”‚
â”‚   - Have runbook for manual partition resolution                        â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```
RESILIENT METADATA CLIENT (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

get(key):
  try:
    value = etcd.get(key)
    cache[key] = value  // update cache on success
            return value
  catch EtcdException:
    if key in cache:
      log("Using cached value")
      return cache[key]
    raise Unavailable

watch(key, callback):
  while true:
    try:
      for event in etcd.watch(key):
        callback(event)
    catch EtcdException:
      log("Watch disconnected, reconnecting...")
      sleep(1s)
```

---

## 9.5 Coordination Services Deep Dive

Understanding the internals of coordination services helps you choose the right one and operate it effectively.

### 9.5.1 Service Comparison

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COORDINATION SERVICE COMPARISON                             â”‚
â”‚                                                                                â”‚
â”‚   Feature           â”‚ ZooKeeper    â”‚ etcd         â”‚ Consul       â”‚ Chubby      â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚   Consensus         â”‚ ZAB          â”‚ Raft         â”‚ Raft         â”‚ Paxos       â”‚
â”‚   Data Model        â”‚ Hierarchical â”‚ Flat KV      â”‚ Flat KV      â”‚ Hierarchicalâ”‚
â”‚   Language          â”‚ Java         â”‚ Go           â”‚ Go           â”‚ C++         â”‚
â”‚   Watch Model       â”‚ One-shot     â”‚ Streaming    â”‚ Blocking     â”‚ Callback    â”‚
â”‚   Transactions      â”‚ Multi-op     â”‚ Mini-txn     â”‚ Check-set    â”‚ Sequences   â”‚
â”‚   Max Data Size     â”‚ 1MB/znode    â”‚ 1.5MB/key    â”‚ 512KB/key    â”‚ 256KB/file  â”‚
â”‚   Session/Lease     â”‚ Session      â”‚ Lease        â”‚ Session      â”‚ Lock delay  â”‚
â”‚   Typical Use       â”‚ Hadoop, Kafkaâ”‚ Kubernetes   â”‚ Service mesh â”‚ Google only â”‚
â”‚                                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 9.5.2 ZooKeeper Internals

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ZOOKEEPER ARCHITECTURE                           â”‚
â”‚                                                                         â”‚
â”‚   ZAB (ZooKeeper Atomic Broadcast) Protocol:                            â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                            â”‚
â”‚                                                                         â”‚
â”‚   PHASE 1: LEADER ELECTION                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚  Nodes exchange votes: (proposed_leader, zxid, epoch)          â”‚    â”‚
â”‚   â”‚  Winner: highest epoch, then highest zxid, then highest id     â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                         â”‚
â”‚   PHASE 2: DISCOVERY                                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚  Leader collects last zxid from each follower                  â”‚    â”‚
â”‚   â”‚  Establishes new epoch (higher than any seen)                  â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                         â”‚
â”‚   PHASE 3: SYNCHRONIZATION                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚  Leader syncs followers to same state                          â”‚    â”‚
â”‚   â”‚  Methods: DIFF, TRUNC, SNAP depending on lag                   â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                         â”‚
â”‚   PHASE 4: BROADCAST                                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚  Normal operation: 2-phase commit for writes                   â”‚    â”‚
â”‚   â”‚  Leader: PROPOSE â†’ followers ACK â†’ Leader: COMMIT              â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                         â”‚
â”‚   DATA MODEL:                                                           â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                          â”‚
â”‚   /                                                                     â”‚
â”‚   â”œâ”€â”€ /app                                                              â”‚
â”‚   â”‚   â”œâ”€â”€ /app/leader              (ephemeral)                          â”‚
â”‚   â”‚   â”œâ”€â”€ /app/config                                                   â”‚
â”‚   â”‚   â””â”€â”€ /app/workers                                                  â”‚
â”‚   â”‚       â”œâ”€â”€ /app/workers/worker-001  (ephemeral, sequential)          â”‚
â”‚   â”‚       â””â”€â”€ /app/workers/worker-002  (ephemeral, sequential)          â”‚
â”‚   â””â”€â”€ /locks                                                            â”‚
â”‚       â””â”€â”€ /locks/resource-x        (ephemeral)                          â”‚
â”‚                                                                         â”‚
â”‚   Node Types:                                                           â”‚
â”‚   - Persistent: survives client disconnect                              â”‚
â”‚   - Ephemeral: deleted when session ends                                â”‚
â”‚   - Sequential: ZK appends monotonic counter to name                    â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```
ZOOKEEPER PATTERNS (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PATTERN 1: Leader Election (Sequential Nodes)
  1. Create sequential ephemeral node: /election/candidate-000001
  2. Get all children, sort by sequence
  3. If I'm lowest â†’ I'm leader
  4. Else watch node just before me (efficient: only 1 watch)
  5. When predecessor deleted â†’ check again

PATTERN 2: Distributed Lock
  1. Create sequential ephemeral node: /locks/lock-000001
  2. If I'm lowest â†’ I hold lock
  3. Else watch predecessor
  4. On timeout: delete my node, raise error

PATTERN 3: Group Membership
  join: create ephemeral node /groups/mygroup/member-id
  leave: node auto-deleted when session ends
  get_members: list children of /groups/mygroup
  watch: ChildrenWatch for membership changes
```

### 9.5.3 etcd Internals

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ETCD ARCHITECTURE                               â”‚
â”‚                                                                         â”‚
â”‚   Built on Raft consensus with MVCC storage.                            â”‚
â”‚                                                                         â”‚
â”‚   KEY FEATURES:                                                         â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                        â”‚
â”‚                                                                         â”‚
â”‚   1. MVCC (Multi-Version Concurrency Control)                           â”‚
â”‚      - Every key has revision history                                   â”‚
â”‚      - Enables watch from any revision                                  â”‚
â”‚      - Compaction removes old revisions                                 â”‚
â”‚                                                                         â”‚
â”‚   2. LEASE SYSTEM                                                       â”‚
â”‚      - TTL-based key expiration                                         â”‚
â”‚      - Multiple keys can attach to one lease                            â”‚
â”‚      - Efficient for ephemeral data                                     â”‚
â”‚                                                                         â”‚
â”‚   3. WATCH                                                              â”‚
â”‚      - Streaming watches (not one-shot)                                 â”‚
â”‚      - Watch from specific revision                                     â”‚
â”‚      - Prefix watches                                                   â”‚
â”‚                                                                         â”‚
â”‚   4. TRANSACTIONS                                                       â”‚
â”‚      - Compare-and-swap style                                           â”‚
â”‚      - If (conditions) Then (ops) Else (ops)                            â”‚
â”‚      - Atomic across multiple keys                                      â”‚
â”‚                                                                         â”‚
â”‚   STORAGE LAYOUT:                                                       â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                      â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Raft Log (append-only)                                         â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”                             â”‚   â”‚
â”‚   â”‚  â”‚ 1  â”‚ â”‚ 2  â”‚ â”‚ 3  â”‚ â”‚ 4  â”‚ â”‚ 5  â”‚ ...                         â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜                             â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â”‚                                             â”‚
â”‚                           â–¼                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  BoltDB (MVCC storage)                                          â”‚   â”‚
â”‚   â”‚                                                                 â”‚   â”‚
â”‚   â”‚  Key Index:     key â†’ [(rev1, val1), (rev2, val2), ...]         â”‚   â”‚
â”‚   â”‚  Revision Map:  rev â†’ (key, value, create_rev, mod_rev)         â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```
ETCD PATTERNS (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PATTERN 1: Leader Election
  election = etcd.election(name)
  lease = etcd.lease(15s)
  election.campaign(node_id, lease)  // blocks until leader
  // Lease auto-renewed

PATTERN 2: Lock with Fencing Token
  lock = etcd.lock(name, ttl=30s)
  lock.acquire()
  fencing_token = lock.revision  // monotonically increasing!

PATTERN 3: Reliable Watch
  while true:
    try:
      for event in etcd.watch(prefix, start_revision):
                    callback(event)
        revision = event.mod_revision + 1  // resume point
    catch Disconnect:
      sleep(1s), reconnect

PATTERN 4: Compare-and-Swap
  etcd.transaction(
    compare: value(key) == expected
    success: put(key, new_value)
    failure: get(key)  // return current value
  )

PATTERN 5: Atomic Batch
  etcd.transaction(
    compare: []  // no preconditions
    success: [put(k1,v1), put(k2,v2), ...]
  )
  // All updates get same revision
```

### 9.5.4 Google Chubby (For Reference)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GOOGLE CHUBBY OVERVIEW                               â”‚
â”‚                                                                         â”‚
â”‚   Chubby is Google's distributed lock service. Not open source,         â”‚
â”‚   but its design influenced ZooKeeper and others.                       â”‚
â”‚                                                                         â”‚
â”‚   KEY INNOVATIONS:                                                      â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                     â”‚
â”‚                                                                         â”‚
â”‚   1. COARSE-GRAINED LOCKS                                               â”‚
â”‚      - Designed for locks held for hours/days, not milliseconds         â”‚
â”‚      - Small number of clients per lock (< 100s)                        â”‚
â”‚      - Advisory locks (clients must cooperate)                          â”‚
â”‚                                                                         â”‚
â”‚   2. LOCK DELAY                                                         â”‚
â”‚      - When lock holder dies, lock is not immediately available         â”‚
â”‚      - Delay (e.g., 60 seconds) prevents rapid lock churn               â”‚
â”‚      - Allows old lock holder to complete in-flight work                â”‚
â”‚                                                                         â”‚
â”‚   3. SEQUENCER (Fencing Token)                                          â”‚
â”‚      - Lock acquisition returns sequencer                               â”‚
â”‚      - Clients pass sequencer to resources                              â”‚
â”‚      - Resources verify sequencer is valid and current                  â”‚
â”‚                                                                         â”‚
â”‚   4. CACHING                                                            â”‚
â”‚      - Aggressive client-side caching                                   â”‚
â”‚      - Chubby sends invalidations on changes                            â”‚
â”‚      - Reduces read load on Chubby masters                              â”‚
â”‚                                                                         â”‚
â”‚   5. CELL DESIGN                                                        â”‚
â”‚      - Each Chubby cell: 5 replicas using Paxos                         â”‚
â”‚      - One cell per datacenter                                          â”‚
â”‚      - Cross-datacenter uses proxy                                      â”‚
â”‚                                                                         â”‚
â”‚   DESIGN CHOICES (Trade-offs):                                          â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                         â”‚
â”‚                                                                         â”‚
â”‚   Why files/directories (not pure KV)?                                  â”‚
â”‚   â†’ Familiar API, natural hierarchy, ACLs                               â”‚
â”‚                                                                         â”‚
â”‚   Why coarse-grained locks?                                             â”‚
â”‚   â†’ Simple to reason about, fewer lock operations                       â”‚
â”‚                                                                         â”‚
â”‚   Why lock delay?                                                       â”‚
â”‚   â†’ Prevents thundering herd, gives holder time to finish               â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 9.5.5 Choosing a Coordination Service

| Use Case | Recommendation | Why |
|----------|----------------|-----|
| **Kubernetes deployments** | etcd | Native integration, well-tested |
| **Hadoop/Kafka ecosystem** | ZooKeeper | Mature, ecosystem integration |
| **Service mesh/discovery** | Consul | Built-in service discovery, health checks |
| **Simple leader election** | etcd or Consul | Simpler API than ZooKeeper |
| **Complex hierarchical data** | ZooKeeper | Native tree structure |
| **Need for watches** | etcd | Streaming watches, no one-shot |
| **Multi-datacenter** | Consul | Built-in WAN federation |

### 9.6 Multi-Region Coordination Patterns

Multi-region coordination is one of the hardest problems in distributed systems. Cross-region latency (50-200ms) makes traditional coordination approaches impractical.

#### 9.6.1 The Multi-Region Challenge

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MULTI-REGION LATENCY REALITY                         â”‚
â”‚                                                                         â”‚
â”‚   Intra-region RTT:  0.5 - 2ms                                          â”‚
â”‚   Cross-region RTT:  50 - 200ms (US-East â†” EU, etc.)                    â”‚
â”‚   Cross-continent:   150 - 300ms (US â†” Asia)                            â”‚
â”‚                                                                         â”‚
â”‚   IMPACT ON COORDINATION:                                               â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                               â”‚
â”‚                                                                         â”‚
â”‚   Single global leader (Raft/Paxos):                                    â”‚
â”‚   - Write latency = cross-region RTT Ã— 2 (propose + commit)             â”‚
â”‚   - 5 regions â†’ some writes take 300-600ms                              â”‚
â”‚                                                                         â”‚
â”‚   Distributed lock across regions:                                      â”‚
â”‚   - Acquire: 50-200ms (best case)                                       â”‚
â”‚   - Lease renewal must account for cross-region latency                 â”‚
â”‚                                                                         â”‚
â”‚   Leader election across regions:                                       â”‚
â”‚   - Election timeout must be >> cross-region RTT                        â”‚
â”‚   - Longer timeout = longer unavailability during failover              â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 9.6.2 Pattern 1: Regional Leaders with Global Coordination

```
REGIONAL LEADER PATTERN (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Used by: Spanner, CockroachDB

local_read(key):   â†’ regional leader serves (fast!)
local_write(key):  â†’ regional consensus only (fast!)

cross_region_write(key):
  if key not owned by this region:
    forward to owning region

global_transaction(operations):
  // Phase 1: Prepare
  for each region in operations:
    prepare_results[region] = region.prepare(ops)
  
  if all prepared:
    // Phase 2: Commit with synchronized timestamp
    commit_ts = get_global_timestamp()
    for each region: region.commit(commit_ts)
    commit_wait(commit_ts)  // TrueTime wait
    return committed
  else:
    for each region: region.abort()
    return aborted
```

#### 9.6.3 Pattern 2: Witness Replicas

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WITNESS REPLICA PATTERN                              â”‚
â”‚                                                                         â”‚
â”‚   Problem: 3 replicas across 3 regions = cross-region RTT for quorum    â”‚
â”‚   Solution: Use lightweight "witness" replicas to reduce latency        â”‚
â”‚                                                                         â”‚
â”‚   Traditional 3-region setup:                                           â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                            â”‚
â”‚                                                                         â”‚
â”‚      US-East          EU-West          US-West                          â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”                          â”‚
â”‚      â”‚Full â”‚          â”‚Full â”‚          â”‚Full â”‚                          â”‚
â”‚      â”‚ Rep â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Rep â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Rep â”‚                          â”‚
â”‚      â””â”€â”€â”€â”€â”€â”˜   100ms  â””â”€â”€â”€â”€â”€â”˜   150ms  â””â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                                                                         â”‚
â”‚   Quorum needs 2/3 â†’ minimum 100ms write latency                        â”‚
â”‚                                                                         â”‚
â”‚   With witness:                                                         â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                        â”‚
â”‚                                                                         â”‚
â”‚      US-East                           US-West                          â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”                          â”Œâ”€â”€â”€â”€â”€â”                           â”‚
â”‚      â”‚Full â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚Full â”‚                           â”‚
â”‚      â”‚ Rep â”‚            40ms          â”‚ Rep â”‚                           â”‚
â”‚      â””â”€â”€â”€â”€â”€â”˜                          â””â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚         â”‚                                â”‚                              â”‚
â”‚         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”         â”‚                              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚Witnessâ”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                        â”‚(logs  â”‚                                        â”‚
â”‚                        â”‚ only) â”‚                                        â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚                        US-Central                                       â”‚
â”‚                                                                         â”‚
â”‚   Witness only stores Raft log, not full data.                          â”‚
â”‚   Can vote but not serve reads.                                         â”‚
â”‚   Placed to minimize quorum latency.                                    â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```
WITNESS REPLICA (Concept)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Purpose: Lightweight replica that votes but doesn't store data.

What it stores:
  âœ“ Raft log (for voting)
  âœ“ Current term, voted_for
  âœ— State machine (no data)

What it does:
  âœ“ handle_append_entries â†’ acknowledge
  âœ“ handle_vote_request â†’ vote
  âœ— handle_read â†’ CANNOT serve reads

Benefit: Reduces quorum latency without storing full data.
```

#### 9.6.4 Pattern 3: Hierarchical Coordination

```
HIERARCHICAL COORDINATION (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Two levels: Regional (fast) vs Global (slow)

local_lock(resource):     â†’ regional coordinator only (fast)
global_lock(resource):    â†’ regional intent lock, then global lock

regional_leader_election: â†’ within-region consensus (fast)
global_leader_election:   â†’ cross-region consensus (slow, avoid if possible)
```

#### 9.6.5 Multi-Region Consensus Options

| Approach | Write Latency | Consistency | Use Case |
|----------|---------------|-------------|----------|
| **Single global leader** | High (cross-region) | Strong | Config store, metadata |
| **Regional leaders (CRDTs)** | Low (local) | Eventual | Counters, sets |
| **Regional leaders (2PC)** | Medium | Strong for cross-region txn | Databases |
| **Spanner (TrueTime)** | Medium + commit-wait | External | Financial, critical |
| **Leaderless (EPaxos)** | Low (nearest replica) | Strong | Geo-distributed KV |

#### 9.6.6 Handling Region Failures

```
MULTI-REGION FAILOVER (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

handle_region_failure(failed_region):
  1. Confirm failure (require 2+ signals: heartbeat, health, network)
  2. Remove failed nodes from cluster membership
  3. Check quorum (if lost â†’ manual intervention)
  4. Force election if leader was in failed region
  5. Update routing to exclude failed region
  6. Schedule data recovery

is_region_failure_confirmed():
  checks = [heartbeats, health_endpoints, network_reachability]
  failures = count(check for check in checks if failed)
  return failures >= 2  // avoid false positives
```

---

<a name="anti-patterns"></a>
## 10. Anti-Patterns: How Good Intentions Go Wrong

### Anti-Pattern 1: The God Lock

```
BAD:  with god_lock:         // One lock for everything
        do_anything()

GOOD: with lock(f"user:{user_id}"):   // Lock per resource
        update_user()
      with lock(f"order:{order_id}"): // Different resource = different lock
        update_order()
```

**Why it's bad:** Serializes all operations, SPOF, any slow op blocks everything

### Anti-Pattern 2: The Chatty Coordinator

```
BAD:  handle_request():
        am_i_leader()        // network
        acquire_lock()       // network
        get_config()         // network
        get_peers()          // network
        do_work()
        release_lock()       // network
        // 5 coordination calls for 1 operation!

GOOD: handle_request():
        if cache_stale: refresh_cached_state()  // rare
        do_work(cached_config)  // no coordination on hot path
```

**Why it's bad:** 5Ã— latency, coordination service = bottleneck

### Anti-Pattern 3: Unbounded Lock Hold Time

```
BAD:  with lock(job_id):
        download_10gb_file()    // minutes
        ml_inference()          // hours
        save_result()           // Lock held entire time!

GOOD: with lock(job_id, ttl=5s):
        claim_job()             // fast
      
      download_10gb_file()      // NO LOCK
      ml_inference()            // NO LOCK
      
      with lock(job_id, ttl=5s):
        save_result()           // fast
```

**Why it's bad:** TTL expires, others starve, throughput tanks

### Anti-Pattern 4: Ignoring Lock Timeout

```
BAD:  lock.acquire(ttl=10s)
      do_slow_work()           // takes 30s, lock expired at 10s!
      write_critical_data()    // DANGEROUS: lock expired!
      lock.release()           // releasing lock we don't own!

GOOD: token = lock.acquire(ttl=10s)
      do_slow_work()
      if not lock.is_still_valid():
        raise LockExpired()
      write_critical_data(fencing_token=token)  // storage rejects stale token
            lock.release()
```

### Anti-Pattern 5: Coordination for Read-Only Operations

```
BAD:  get_user():
        with lock(user_id):        // WHY? Reads don't need locks!
          return db.query(...)

GOOD: get_user():
        return db.query(...)       // No lock for reads
      
      update_user():
        with lock(user_id):        // Lock only for writes
          db.execute(...)
```

**Why it's bad:** Reads don't need mutual exclusion â†’ massive perf penalty for nothing

### Anti-Pattern 6: Human Failure Modes in Coordination

Coordination systems amplify human errors. Staff engineers must anticipate and prevent common mistakes that lead to production incidents.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HUMAN FAILURE MODES IN COORDINATION                  â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ Error                        â”‚ Impact                           â”‚  â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚   â”‚ Forgetting fencing tokens    â”‚ Most common, causes silent data  â”‚  â”‚
â”‚   â”‚ in new service               â”‚ corruption                        â”‚  â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚   â”‚ Setting TTL too short       â”‚ Locks expire during normal       â”‚  â”‚
â”‚   â”‚                              â”‚ operation, causing duplicate     â”‚  â”‚
â”‚   â”‚                              â”‚ processing                       â”‚  â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚   â”‚ Setting TTL too long        â”‚ Dead lock holders block          â”‚  â”‚
â”‚   â”‚                              â”‚ resources for minutes            â”‚  â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚   â”‚ Manual lock release during  â”‚ Can violate safety guarantees,   â”‚  â”‚
â”‚   â”‚ incident                     â”‚ cause split-brain                â”‚  â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚   â”‚ Not testing coordination    â”‚ "It works when ZK is healthy"    â”‚  â”‚
â”‚   â”‚ failure in staging           â”‚ is not a test                    â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                         â”‚
â”‚   Prevention Strategies:                                               â”‚
â”‚   - Lint rules that require fencing tokens in lock-acquiring code     â”‚
â”‚   - TTL templates per use case (prevent arbitrary TTL values)          â”‚
â”‚   - Lock release requires 2-person approval during incidents           â”‚
â”‚   - Chaos engineering: regularly inject ZK failures in staging        â”‚
â”‚                                                                         â”‚
â”‚   On-Call Reality:                                                     â”‚
â”‚   "Coordination system issues are the hardest to debug at 3 AM         â”‚
â”‚   because symptoms manifest in dependent services, not in the          â”‚
â”‚   coordination system itself."                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Common Human Errors:**

1. **Forgetting fencing tokens in new service**
   - Most common error
   - Causes silent data corruption (two processes think they hold the lock)
   - Detection: Only discovered when duplicate operations occur

2. **Setting TTL too short**
   - Locks expire during normal operation
   - Causes duplicate processing
   - Example: 30-second TTL for a 45-second operation

3. **Setting TTL too long**
   - Dead lock holders block resources for minutes
   - Prevents recovery from process crashes
   - Example: 1-hour TTL for a 10-second operation

4. **Manual lock release during incident**
   - Can violate safety guarantees
   - Causes split-brain scenarios
   - Example: Releasing a lock while the original holder is still active

5. **Not testing coordination failure in staging**
   - "It works when ZK is healthy" is not a test
   - Production failures are the first time the system sees coordination outages
   - Results in cascading failures that could have been prevented

**Prevention Strategies:**

- **Lint rules that require fencing tokens** in lock-acquiring code
- **TTL templates per use case** (prevent arbitrary TTL values)
- **Lock release requires 2-person approval** during incidents
- **Chaos engineering**: Regularly inject ZK failures in staging

**On-Call Reality:**

"Coordination system issues are the hardest to debug at 3 AM because symptoms manifest in dependent services, not in the coordination system itself." Engineers spend hours tracing symptoms back to a coordination failure that occurred minutes earlier.

---

<a name="when-not-to-use-locks"></a>
## 11. When NOT to Use Locks

### Rule 1: If You Can Use Idempotent Operations Instead

```
LOCK:   with lock("counter"):
    value = db.get("counter")
    db.set("counter", value + 1)

BETTER: db.increment("counter", 1)  // Atomic, no lock needed
```

### Rule 2: If You Can Partition the Work

```
LOCK:   with lock("job-queue"):
    job = queue.pop()

BETTER: my_partition = hash(worker_id) % num_partitions
        job = queue.pop(partition=my_partition)  // Each worker owns partition
```

### Rule 3: If Eventual Consistency Is Acceptable

```
LOCK:   with lock("page-view-counter"):
    views = db.get("page:123:views")
    db.set("page:123:views", views + 1)

BETTER: local_buffer[page_id] += 1  // fast, in-memory
        
        // Background job every second:
        for page_id, count in local_buffer:
          db.increment(page_id, count)
local_buffer.clear()
```

### Rule 4: If CRDTs Can Model Your Data

```
LOCK:   with lock(cart_id):
          cart = db.get(cart_id)
    cart.add(item)
          db.set(cart_id, cart)

BETTER: Use Add-Wins Set CRDT (no lock needed):
        
        add(item):    adds[item].add((timestamp, replica_id))
        remove(item): removes[item].add((timestamp, replica_id))
        
        get_items(): return items where latest_add > latest_remove
        merge(other): union all adds and removes (conflict-free!)
```

### Rule 5: If You Can Use Optimistic Concurrency Control

```
PESSIMISTIC (Lock):
  with lock(account_id):
    account = db.get(account_id)
    account.balance -= amount
    db.set(account_id, account)

OPTIMISTIC (CAS - no lock):
  for attempt in retries:
    account, version = db.get_with_version(account_id)
    account.balance -= amount
    
    if db.set_if_version(account_id, account, expected=version):
      return success
    
    // Version changed, retry
  raise TooManyConflicts
```

### Decision Matrix: Lock vs. Alternatives

| Situation | Use Lock? | Better Alternative |
|-----------|-----------|-------------------|
| Increment counter | âŒ No | Atomic increment |
| Update user profile | âš ï¸ Maybe | Optimistic concurrency |
| Transfer money between accounts | âœ… Yes | Or Saga pattern |
| Process exactly one job | âœ… Yes | Or claim with CAS |
| Update shopping cart | âŒ No | CRDT |
| Track page views | âŒ No | Eventual consistency |
| Leader election | âœ… Yes | Built-in consensus |
| Distributed cache invalidation | âŒ No | TTL + eventual |

---

<a name="graceful-degradation"></a>
## 12. Graceful Degradation: What Happens When Coordination Fails

### 12.1 The Degradation Spectrum

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DEGRADATION STRATEGIES                              â”‚
â”‚                                                                         â”‚
â”‚   Most Restrictive                              Least Restrictive       â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶       â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â”‚    FAIL    â”‚ â”‚   DEGRADE  â”‚ â”‚   CACHE    â”‚ â”‚    FAIL    â”‚           â”‚
â”‚   â”‚   CLOSED   â”‚ â”‚    MODE    â”‚ â”‚  FALLBACK  â”‚ â”‚    OPEN    â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                                         â”‚
â”‚   Reject all     Reduce         Use cached      Allow all               â”‚
â”‚   requests       functionality   values          requests               â”‚
â”‚                                                                         â”‚
â”‚   Safety: â­â­â­â­â­  â­â­â­â­         â­â­â­           â­                  â”‚
â”‚   Availability: â­   â­â­â­          â­â­â­â­         â­â­â­â­â­            â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 12.2 Building Resilient Coordination Clients

```
RESILIENT COORDINATION CLIENT (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Principles:
  1. Always have a fallback
  2. Prefer availability over strict consistency in degraded mode
  3. Make degradation observable (metrics, logs, alerts)
  4. Auto-recover when coordination becomes available

get_config(key):
  try:
    value = coordination.get(key)
    cache[key] = value
    exit_degraded_mode()
    return value
  catch Unavailable:
    enter_degraded_mode()
    if key in cache: return cache[key]      // Fallback 1
    if default: return default               // Fallback 2
    raise ConfigUnavailable                  // Fail if critical

acquire_lock(resource):
  try:
    return coordination.lock(resource)
  catch Unavailable:
    enter_degraded_mode()
    switch(degraded_strategy):
      "local":   return LocalLock(resource)  // Process-level only
      "fail":    raise LockUnavailable       // Fail closed
      "proceed": return NoOpLock             // Fail open (dangerous!)
```

### 12.3 Circuit Breaker for Coordination

```
CIRCUIT BREAKER (Pseudo-code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

States: CLOSED â†’ OPEN â†’ HALF_OPEN â†’ CLOSED

Constants:
  FAILURE_THRESHOLD = 5    // failures before opening
  RESET_TIMEOUT = 30s      // wait before trying again
  SUCCESS_THRESHOLD = 3    // successes to close

call(operation):
  if state == OPEN:
    if should_attempt_reset(): state = HALF_OPEN
    else: raise CircuitOpen  // fast-fail
  
  try:
    result = operation()
    on_success()
    return result
  catch:
    on_failure()
    raise

on_success():
  if state == HALF_OPEN and success_count >= 3:
    state = CLOSED  // recovered!

on_failure():
  failure_count++
  if failure_count >= 5:
    state = OPEN
```

### 12.4 Degradation Patterns by Service Type

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DEGRADATION PATTERNS BY SERVICE TYPE                       â”‚
â”‚                                                                         â”‚
â”‚   SERVICE TYPE          COORDINATION FAILURE â†’ DEGRADATION STRATEGY     â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”‚
â”‚                                                                         â”‚
â”‚   Job Scheduler         Leader dies â†’ Followers buffer jobs locally     â”‚
â”‚                         Resume when new leader elected                  â”‚
â”‚                         Risk: Duplicate execution if not idempotent     â”‚
â”‚                                                                         â”‚
â”‚   Rate Limiter          Redis down â†’ Local rate limiting only           â”‚
â”‚                         Effective limit = global_limit / server_count   â”‚
â”‚                         Risk: Over-limit by factor of server_count      â”‚
â”‚                                                                         â”‚
â”‚   Feature Flags         etcd down â†’ Use cached flags                    â”‚
â”‚                         Cache TTL = 5 minutes (configurable)            â”‚
â”‚                         Risk: Delayed flag updates during outage        â”‚
â”‚                                                                         â”‚
â”‚   Distributed Lock      Lock service down â†’ ???                         â”‚
â”‚                         Option A: Fail closed (reject operations)       â”‚
â”‚                         Option B: Proceed (risk duplicates)             â”‚
â”‚                         Decision depends on cost of duplicates          â”‚
â”‚                                                                         â”‚
â”‚   Configuration         Metadata unavailable â†’ Use last known config    â”‚
â”‚                         Alert if config age > threshold                 â”‚
â”‚                         Risk: Operating with stale config               â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 12.5 Testing Degraded Mode

```
DEGRADED MODE TESTS (What to Test)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

test_coordination_unavailable:
  with mock_failure():
    response = service.handle_request()
    assert response.status != 500
    assert response.headers["X-Degraded-Mode"] == "true"

test_cache_fallback:
  service.get_config("flag_x")              // prime cache
  with mock_failure():
    value = service.get_config("flag_x")    // should use cache
    assert value is not None

test_degraded_mode_metrics:
  with mock_failure(): service.handle_request()
  assert metrics["degraded_mode.entered"] == 1
  
  // Restore
  service.handle_request()
  assert metrics["degraded_mode.duration"] > 0

test_circuit_breaker_opens:
  with mock_failure():
    for 10 times: service.coordinate()  // trigger failures
  assert circuit_breaker.state == "OPEN"
  assert_no_network_calls { service.coordinate() }  // fast-fail
```

---

<a name="interview-explanations"></a>
## 13. Interview Explanations

### 13.1 "Explain distributed locks and when you'd use them"

**Strong Answer:**

> "A distributed lock provides mutual exclusion across multiple machinesâ€”ensuring only one process can access a shared resource at a time.
>
> **When to use them:**
> - Exactly-once job execution (like processing a payment)
> - Preventing concurrent modifications to the same entity
> - Coordinating access to external resources with no built-in concurrency control
>
> **Key implementation concerns:**
> 1. **Lock expiration:** Use TTLs to prevent deadlocks from crashed holders
> 2. **Unique identifiers:** Prevent accidentally releasing someone else's lock
> 3. **Fencing tokens:** Monotonically increasing tokens to detect stale lock holders
>
> **When NOT to use them:**
> - If atomic operations exist (use INCR instead of lock â†’ read â†’ write â†’ unlock)
> - If work can be partitioned (each worker handles its own subset)
> - If eventual consistency is acceptable (use CRDTs)
>
> **The fundamental problem** is that distributed locks aren't foolproofâ€”a process can pause after acquiring the lock (GC, network delay), the lock expires, another process acquires it, and now you have two holders. Fencing tokens protect against this by having downstream resources reject operations from stale holders."

### 13.2 "How does leader election work in distributed systems?"

**Strong Answer:**

> "Leader election ensures exactly one node acts as the authoritative coordinator at any time, with automatic failover when the leader fails.
>
> **Two main approaches:**
>
> 1. **Lease-based:** Leader holds a time-limited lease. Must renew before expiry. If the leader is partitioned from the lease store, the lease expires and someone else can take over.
>
> 2. **Quorum-based:** Leader must maintain support from a majority of nodes through heartbeats. If it can't reach quorum, it steps down.
>
> **The critical safety property** is that at any time, at most one node believes it's the leader. This is achieved through:
> - Lease TTLs (old leader's lease expires before new leader can acquire)
> - Epoch/term numbers (operations include term number; stale terms are rejected)
> - Quorum overlap (any two majorities share at least one node)
>
> **What happens during leader failure:**
> 1. Leader stops sending heartbeats (or lease expires)
> 2. Followers detect missing heartbeats after timeout
> 3. Election triggersâ€”nodes vote for new leader
> 4. Winner starts acting as leader
>
> **This creates unavailability during election (typically 1-10 seconds)**. Systems must be designed to buffer operations during this window or fail gracefully."

### 13.3 "What is split-brain and how do you prevent it?"

**Strong Answer:**

> "Split-brain occurs when a network partition causes two groups of nodes to independently elect their own leaders, resulting in two nodes both believing they're the authoritative leader.
>
> **Why it's dangerous:** Both leaders accept writes, data diverges, and when the partition heals, you have conflicting states that may be impossible to reconcile.
>
> **Prevention mechanisms:**
>
> 1. **Quorum requirement:** Leader must maintain support from majority of nodes. In a 5-node cluster, each partition needs 3 nodes to elect a leader. Since there's only 5 total, only one partition can have 3.
>
> 2. **Fencing:** When a new leader is elected, it 'fences' the old leaderâ€”prevents it from making changes. This can be done through:
>    - Revoking storage access (STONITH - 'Shoot The Other Node In The Head')
>    - Epoch numbers where resources reject old epochs
>
> 3. **Leader step-down:** Leaders that lose quorum must stop accepting writes immediately, even if they don't know a new leader exists.
>
> **The key insight** is that it's safe for there to be NO leader temporarily, but never safe to have TWO leaders. Systems prefer unavailability over inconsistency during partitions."

### 13.4 "How would you design a distributed job scheduler?"

**Strong Answer:**

> "I'd design it with these components:
>
> **1. Job Storage:** A database holding jobs with status (pending, running, completed, failed), scheduled time, and worker assignment. Partitioned by job_id for scale.
>
> **2. Leader/Coordinator:** Single leader (elected via etcd/ZooKeeper lease) that:
> - Scans for ready jobs
> - Assigns jobs to workers
> - Monitors job progress
> - Reclaims jobs from dead workers
>
> **3. Workers:** Register with leader, receive assignments, process jobs.
>
> **Key mechanisms for exactly-once execution:**
>
> 1. **Claim with lock:** Worker acquires distributed lock before processing. Prevents two workers from processing the same job.
>
> 2. **Idempotency keys:** Each job execution has unique ID. If job already completed with that ID, skip it.
>
> 3. **Fencing tokens:** Include token when writing results. Database rejects writes with stale tokens.
>
> **Handling failures:**
>
> - **Worker dies:** Job stays 'running' too long. Reclaimer process detects this, moves job back to 'pending'.
> 
> - **Leader dies:** Workers buffer jobs locally. New leader elected in seconds.
>
> - **Network partition:** Workers can't reach leader. They pause (if strict) or continue processing local queue (if available).
>
> **Trade-off:** Strictly exactly-once adds latency (lock acquisition). At-least-once is simpler and fine if jobs are idempotent."

### 13.5 "When would you NOT use coordination?"

**Strong Answer:**

> "I'd avoid coordination whenever possible because it adds latency, creates bottlenecks, and introduces failure modes. Specifically:
>
> **1. When atomic operations exist:**
> - Don't lock to increment counterâ€”use atomic INCREMENT
> - Don't lock for append-only operationsâ€”just append
>
> **2. When work is naturally partitioned:**
> - Each worker handles specific shardâ€”no contention
> - Message queue partitions assigned to consumersâ€”no shared queue lock
>
> **3. When eventual consistency is acceptable:**
> - Analytics countersâ€”approximate is fine
> - Page view trackingâ€”don't need real-time accuracy
> - Session storageâ€”rarely contested
>
> **4. When CRDTs can model the data:**
> - Shopping carts (Add-Wins Set)
> - Counters (G-Counter, PN-Counter)
> - Sets with concurrent adds/removes (OR-Set)
>
> **5. When optimistic concurrency works:**
> - Low-contention updatesâ€”version checks are cheaper than locks
> - Read-heavy workloadsâ€”no need to lock reads
>
> **The decision framework:**
> 1. What happens if two processes do this simultaneously?
> 2. Can we make the operation commutative?
> 3. Can we detect and retry conflicts?
> 4. Is 'last write wins' acceptable?
>
> If any of these work, avoid distributed locks."

---

<a name="brainstorming-questions"></a>
## 14. Brainstorming Questions

### Architecture Design Questions

1. **You're building a payment processing system. Each payment must be processed exactly once. How do you ensure this without making the lock service a single point of failure?**

2. **Your distributed cache invalidation is causing thundering herd problemsâ€”when a popular key expires, hundreds of requests simultaneously try to rebuild it. How do you coordinate this?**

3. **You have 1000 workers processing jobs from a queue. Using a single lock on the queue would be a bottleneck. How do you scale this?**

4. **Your leader election is using a 10-second lease TTL. During a 5-second network blip, the leader loses its lease and a new leader is elected. Now both think they're leader. How do you prevent data corruption?**

5. **You're designing a distributed rate limiter for 100 million users. Coordinating every request is too expensive. What's your approach?**

### Trade-off Analysis Questions

6. **Compare lease-based vs. quorum-based leader election. When would you prefer each?**

7. **Your coordination service (etcd) is down. You have three options: fail all requests, proceed without coordination, or use cached state. Walk through the trade-offs for a job scheduler.**

8. **You're seeing frequent election stormsâ€”leaders getting elected and deposed rapidly. What could cause this and how would you diagnose/fix it?**

9. **Your distributed lock implementation uses Redis. Someone suggests using a "safer" Redlock algorithm with 5 Redis instances. What are the trade-offs?**

10. **You have the choice between using ZooKeeper (strong consistency, lower throughput) vs. Redis (higher throughput, weaker guarantees) for your distributed locks. How do you decide?**

### Debugging and Operations Questions

11. **Your job scheduler is occasionally processing jobs twice. The lock implementation looks correct. What could be happening?**

12. **After a network partition healed, you discovered that some configuration changes were lost. Your metadata service uses Raft for consensus. How is this possible?**

13. **Workers report that lock acquisition is taking 10+ seconds, up from the usual milliseconds. Debugging shows the lock service is healthy. What's happening?**

14. **Your 5-node consensus cluster lost quorum when 2 nodes died. You need to restore service immediately but can't recover the dead nodes. What are your options?**

15. **You're seeing "stale fencing token" errors in production, but your lock service shows only one active holder. How do you investigate this?**

### System Evolution Questions

16. **Your service currently uses leader election for coordination. Traffic has grown 100x and the leader is a bottleneck. How do you evolve the architecture?**

17. **You started with Redis for distributed locks. Now you need stronger consistency guarantees. What's your migration strategy?**

18. **Your multi-region deployment needs a global leader. Cross-region latency makes lease renewal slow and unreliable. How do you adapt your design?**

19. **Your coordination system is causing cascading failuresâ€”when it goes down, all services fail. How do you add resilience?**

20. **You're moving from a monolith to microservices. The monolith used database locks for coordination. How do you handle coordination in the distributed version?**

---

<a name="homework"></a>
## 15. Homework: Remove Coordination and Re-Architect

### The Challenge

You've inherited a system with excessive coordination. Your mission: **remove or reduce coordination while maintaining correctness.**

### The Existing System

```python
class OverlyCoordinatedSystem:
    """
    A system that uses distributed locks for EVERYTHING.
    Your job: identify what can be removed or optimized.
    """
    
    def __init__(self, redis_client):
        self.redis = redis_client
    
    def increment_page_views(self, page_id):
        """Lock on page to increment view counter."""
        with DistributedLock(self.redis, f"page:{page_id}"):
            views = self.redis.get(f"page:{page_id}:views") or 0
            self.redis.set(f"page:{page_id}:views", int(views) + 1)
    
    def add_item_to_cart(self, user_id, item_id, quantity):
        """Lock on cart to add item."""
        with DistributedLock(self.redis, f"cart:{user_id}"):
            cart = json.loads(self.redis.get(f"cart:{user_id}") or "{}")
            cart[item_id] = cart.get(item_id, 0) + quantity
            self.redis.set(f"cart:{user_id}", json.dumps(cart))
    
    def get_user_profile(self, user_id):
        """Lock on user to read profile (!)."""
        with DistributedLock(self.redis, f"user:{user_id}"):
            return db.query("SELECT * FROM users WHERE id = ?", user_id)
    
    def update_user_profile(self, user_id, updates):
        """Lock on user to update profile."""
        with DistributedLock(self.redis, f"user:{user_id}"):
            db.execute(
                "UPDATE users SET name=?, email=? WHERE id = ?",
                [updates['name'], updates['email'], user_id]
            )
    
    def process_order(self, order_id):
        """Global lock to process any order (!!)."""
        with DistributedLock(self.redis, "order-processing"):
            order = db.query("SELECT * FROM orders WHERE id = ?", order_id)
            self.charge_payment(order)
            self.update_inventory(order)
            self.send_confirmation(order)
            db.execute("UPDATE orders SET status='completed' WHERE id = ?", order_id)
    
    def get_feature_flag(self, flag_name):
        """Lock to read feature flag (!!)."""
        with DistributedLock(self.redis, f"flag:{flag_name}"):
            return self.redis.get(f"feature:{flag_name}") == "true"
    
    def submit_job(self, job_data):
        """Global lock on job queue (!!!)."""
        with DistributedLock(self.redis, "job-queue"):
            job_id = str(uuid.uuid4())
            self.redis.lpush("jobs", json.dumps({
                "id": job_id,
                **job_data
            }))
            return job_id
    
    def claim_job(self):
        """Global lock on job queue to claim job."""
        with DistributedLock(self.redis, "job-queue"):
            job_data = self.redis.rpop("jobs")
            if job_data:
                job = json.loads(job_data)
                return job
            return None
```

### Part 1: Identify the Problems (Analysis)

For each method, answer:
1. Is coordination necessary at all?
2. If yes, is this the right level of granularity?
3. What's the performance impact?
4. What's a better alternative?

**Fill in this table:**

| Method | Necessary? | Problem | Better Alternative |
|--------|------------|---------|-------------------|
| increment_page_views | | | |
| add_item_to_cart | | | |
| get_user_profile | | | |
| update_user_profile | | | |
| process_order | | | |
| get_feature_flag | | | |
| submit_job | | | |
| claim_job | | | |

### Part 2: Re-Architecture (Implementation)

Rewrite the system with minimal coordination. For each method:
- Remove lock if not needed
- Use finer-grained lock if needed
- Use alternative pattern (atomic ops, CRDTs, optimistic concurrency)

**Starter template:**

```python
class OptimizedSystem:
    """
    Refactored system with minimal coordination.
    """
    
    def __init__(self, redis_client):
        self.redis = redis_client
        self.feature_cache = TTLCache(maxsize=1000, ttl=60)
    
    def increment_page_views(self, page_id):
        # TODO: Remove lock, use atomic increment
        pass
    
    def add_item_to_cart(self, user_id, item_id, quantity):
        # TODO: Use Redis HINCRBY for atomic hash increment
        # Or: Use CRDT pattern for concurrent carts
        pass
    
    def get_user_profile(self, user_id):
        # TODO: Remove lock entirely
        pass
    
    def update_user_profile(self, user_id, updates):
        # TODO: Use optimistic concurrency with version check
        pass
    
    def process_order(self, order_id):
        # TODO: Use order-level lock (not global)
        # TODO: Consider saga pattern for multi-step process
        pass
    
    def get_feature_flag(self, flag_name):
        # TODO: Use local cache with background refresh
        pass
    
    def submit_job(self, job_data):
        # TODO: Remove lock, use atomic LPUSH
        pass
    
    def claim_job(self, worker_id):
        # TODO: Use BRPOPLPUSH for atomic claim
        # Or: Partition queue by worker
        pass
```

### Part 3: Failure Mode Analysis

For your refactored system, document:

1. **What happens if Redis is unavailable?**
   - Which operations fail?
   - Which can proceed with degraded functionality?

2. **What happens if a worker crashes mid-operation?**
   - Order processing halfway done
   - Job claimed but not completed

3. **What happens under high contention?**
   - Many concurrent cart updates
   - Many workers claiming jobs

### Part 4: Metrics and Observability

Design monitoring for your coordination:

1. **What metrics would you collect?**
   - Lock acquisition time
   - Lock contention rate
   - Optimistic concurrency retry rate

2. **What alerts would you set?**
   - Lock acquisition p99 > 100ms
   - Retry rate > 10%

3. **How would you trace a "slow request" caused by coordination?**

### Deliverables

1. **Completed analysis table** (Part 1)
2. **Refactored code** with comments explaining decisions (Part 2)
3. **Failure mode documentation** (Part 3)
4. **Monitoring design** (Part 4)

### Bonus Challenges

1. **Multi-region:** How would your design change for a system spanning 3 regions?

2. **Hybrid Consistency:** Some operations need strong consistency (order processing), others don't (page views). Design a system that handles both efficiently.

3. **Coordination-Free Claims:** Design a job processing system where workers claim jobs without any distributed coordination. (Hint: consistent hashing, deterministic assignment)

---

## 16. Operational Excellence: Running Coordination Services in Production

### 16.1 Capacity Planning for Coordination Services

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  COORDINATION SERVICE SIZING GUIDE                      â”‚
â”‚                                                                         â”‚
â”‚   ETCD SIZING:                                                          â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                          â”‚
â”‚                                                                         â”‚
â”‚   Cluster Size:                                                         â”‚
â”‚   - 3 nodes: Survives 1 failure (most common)                           â”‚
â”‚   - 5 nodes: Survives 2 failures (high availability)                    â”‚
â”‚   - 7 nodes: Survives 3 failures (rarely needed)                        â”‚
â”‚                                                                         â”‚
â”‚   Hardware per node:                                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚   â”‚ Load           â”‚ CPU      â”‚ Memory   â”‚ Disk         â”‚               â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤               â”‚
â”‚   â”‚ Light (<500 QPS)â”‚ 2 cores â”‚ 8 GB     â”‚ 50 GB SSD    â”‚               â”‚
â”‚   â”‚ Medium (5K QPS) â”‚ 4 cores â”‚ 16 GB    â”‚ 100 GB SSD   â”‚               â”‚
â”‚   â”‚ Heavy (15K QPS) â”‚ 8 cores â”‚ 32 GB    â”‚ 200 GB NVMe  â”‚               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                                         â”‚
â”‚   ZOOKEEPER SIZING:                                                     â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                     â”‚
â”‚                                                                         â”‚
â”‚   Key metrics to monitor:                                               â”‚
â”‚   - Outstanding requests (should be < 10)                               â”‚
â”‚   - Average latency (should be < 10ms)                                  â”‚
â”‚   - znode count (impacts snapshot time)                                 â”‚
â”‚   - Watch count (impacts notification overhead)                         â”‚
â”‚                                                                         â”‚
â”‚   Warning signs:                                                        â”‚
â”‚   - Snapshot taking > 30 seconds                                        â”‚
â”‚   - JVM heap > 80% utilized                                             â”‚
â”‚   - Log directory filling up                                            â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 16.1B Cost Reality: What Coordination Infrastructure Actually Costs

Coordination services are expensiveâ€”not just in compute, but in operational toil. Staff engineers must understand the true cost before committing to a coordination architecture.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              COORDINATION INFRASTRUCTURE COST COMPARISON                â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚ Solution         â”‚ 3-node Setup â”‚ 5-node Setup â”‚ Managed Service â”‚ â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚   â”‚ ZooKeeper        â”‚ $1.5K/month  â”‚ $3K/month    â”‚ N/A             â”‚ â”‚
â”‚   â”‚ (self-hosted)    â”‚ + $8K/year   â”‚ + $12K/year  â”‚                 â”‚ â”‚
â”‚   â”‚                  â”‚ (ops toil)   â”‚ (ops toil)   â”‚                 â”‚ â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚   â”‚ etcd             â”‚ $1K/month    â”‚ $2K/month    â”‚ N/A             â”‚ â”‚
â”‚   â”‚ (self-hosted)    â”‚ + $5K/year   â”‚ + $8K/year   â”‚                 â”‚ â”‚
â”‚   â”‚                  â”‚ (ops toil)   â”‚ (ops toil)   â”‚                 â”‚ â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚   â”‚ Managed          â”‚ N/A          â”‚ N/A          â”‚ $5K-8K/month    â”‚ â”‚
â”‚   â”‚ (Cloud-native)   â”‚              â”‚              â”‚ + $2K/year      â”‚ â”‚
â”‚   â”‚                  â”‚              â”‚              â”‚ (ops toil)      â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                         â”‚
â”‚   Cost breakdown (5-node ZooKeeper example):                           â”‚
â”‚   - Compute: 5 Ã— (8-core, 32GB RAM) = $2.5K/month                       â”‚
â”‚   - Storage: 5 Ã— 200GB NVMe = $500/month                               â”‚
â”‚   - Network: Inter-zone traffic = $200/month                            â”‚
â”‚   - Operational toil: $10K/year (debugging, upgrades, monitoring)      â”‚
â”‚                                                                         â”‚
â”‚   Total: ~$3.2K/month + $10K/year operational burden                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Top 2 Cost Drivers:**

1. **Dedicated Coordination Nodes That Can't Be Shared**
   - Coordination services require dedicated resourcesâ€”you cannot co-locate other workloads
   - CPU and memory must be reserved even during idle periods
   - A 5-node ZooKeeper cluster costs ~$2K-5K/month in compute alone
   - Adding $10K/year in operational toil (debugging sessions, upgrades, monitoring)
   - Managed alternatives cost 2-3Ã— more ($5K-8K/month) but reduce operational burden by 80%

2. **Operational Expertise Required**
   - ZooKeeper and etcd are notorious for subtle failure modes
   - GC pauses, session storms, and split-brain scenarios require deep expertise
   - On-call engineers spend 2-3 hours per incident debugging coordination issues
   - Training and documentation overhead: ~$5K-8K/year per team

**What Staff Engineers Do NOT Build:**

- **Custom consensus implementations** â†’ Use battle-tested Raft libraries (etcd, Consul)
- **Coordination for operations that can be made idempotent** â†’ Idempotency keys eliminate need for locks
- **Distributed locks for operations that can use optimistic concurrency** â†’ Version numbers and CAS operations are cheaper

**Cost of Coordination Failure:**

If ZooKeeper is down for 10 minutes, all lock-dependent services are blocked. For an e-commerce platform:
- Estimated $50K in lost revenue (assuming $5K/minute transaction volume)
- Customer trust degradation (estimated $20K in churn risk)
- Engineering time to diagnose and resolve: 4-6 hours Ã— $200/hour = $800-1.2K

**Total blast radius: $70K-71K per 10-minute outage**

### 16.1C Scale Thresholds: When Coordination Becomes the Bottleneck

Coordination services have hard limits. Staff engineers must recognize early warning signs and plan architectural changes before hitting these limits.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              COORDINATION SCALING THRESHOLDS & ACTIONS                   â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ Lock Acq/sec  â”‚ ZK Latency   â”‚ Action Required                 â”‚  â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚   â”‚ 100/sec       â”‚ < 5ms        â”‚ Single cluster fine              â”‚  â”‚
â”‚   â”‚               â”‚              â”‚ Monitor leader CPU               â”‚  â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚   â”‚ 1K/sec        â”‚ 5-20ms       â”‚ Watch leader CPU                â”‚  â”‚
â”‚   â”‚               â”‚              â”‚ Consider read-only follower      â”‚  â”‚
â”‚   â”‚               â”‚              â”‚ offload                          â”‚  â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚   â”‚ 10K/sec       â”‚ 20-50ms      â”‚ ZK session count limit           â”‚  â”‚
â”‚   â”‚               â”‚              â”‚ approaching                      â”‚  â”‚
â”‚   â”‚               â”‚              â”‚ Need to partition coordination   â”‚  â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚   â”‚ 100K/sec      â”‚ > 100ms      â”‚ Cannot use centralized           â”‚  â”‚
â”‚   â”‚               â”‚              â”‚ coordination                     â”‚  â”‚
â”‚   â”‚               â”‚              â”‚ Must redesign for coordination   â”‚  â”‚
â”‚   â”‚               â”‚              â”‚ avoidance (CRDTs, partitioned    â”‚  â”‚
â”‚   â”‚               â”‚              â”‚ ownership)                       â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                         â”‚
â”‚   Most Dangerous Scaling Assumption:                                   â”‚
â”‚   "ZooKeeper scales horizontally" â†’ FALSE                              â”‚
â”‚                                                                         â”‚
â”‚   Reality: ZK writes go through leader only. Adding followers helps    â”‚
â”‚   reads but NOT write throughput. Write capacity is fixed by leader   â”‚
â”‚   node performance.                                                    â”‚
â”‚                                                                         â”‚
â”‚   Early Warning Signs:                                                â”‚
â”‚   - ZK leader CPU > 60%                                                â”‚
â”‚   - Session count > 10K                                                â”‚
â”‚   - Write latency p99 > 100ms                                          â”‚
â”‚                                                                         â”‚
â”‚   What Breaks First:                                                   â”‚
â”‚   Session establishment rate (ZK has hard limits on concurrent new      â”‚
â”‚   sessions). When this limit is hit, new clients cannot connect,       â”‚
â”‚   causing cascading failures in dependent services.                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Critical Thresholds:**

- **100/sec**: ZooKeeper handles easily, single cluster is fine
- **1K/sec**: Watch leader CPU, consider read-only follower offload
- **10K/sec**: ZK session count limit approaching, need to partition coordination
- **100K/sec**: Cannot use centralized coordinationâ€”must redesign for coordination avoidance (CRDTs, partitioned ownership)

**Most Dangerous Scaling Assumption:**

"ZooKeeper scales horizontally" â€” **FALSE**. ZK writes go through leader only; adding followers helps reads but not write throughput. Write capacity is fixed by leader node performance.

**Early Warning Signs:**

- ZK leader CPU > 60%
- Session count > 10K
- Write latency p99 > 100ms

**What Breaks First:**

Session establishment rate (ZK has hard limits on concurrent new sessions). When this limit is hit, new clients cannot connect, causing cascading failures in dependent services.

### 16.2 Production Runbooks

#### Runbook 1: Leader Election Storm

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RUNBOOK: LEADER ELECTION STORM                       â”‚
â”‚                                                                         â”‚
â”‚   SYMPTOMS:                                                             â”‚
â”‚   - Frequent leader changes (> 1/minute)                                â”‚
â”‚   - High CPU on consensus nodes                                         â”‚
â”‚   - Increased latency for coordination operations                       â”‚
â”‚   - "leadership transfer" or "new leader elected" log spam              â”‚
â”‚                                                                         â”‚
â”‚   DIAGNOSIS:                                                            â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                            â”‚
â”‚   1. Check network latency between nodes                                â”‚
â”‚      $ ping <peer_ip>                                                   â”‚
â”‚      Expected: < 2ms within datacenter                                  â”‚
â”‚                                                                         â”‚
â”‚   2. Check for CPU throttling                                           â”‚
â”‚      $ cat /sys/fs/cgroup/cpu/cpu.stat                                  â”‚
â”‚      nr_throttled should be 0                                           â”‚
â”‚                                                                         â”‚
â”‚   3. Check disk latency                                                 â”‚
â”‚      $ iostat -x 1                                                      â”‚
â”‚      await should be < 5ms for SSD                                      â”‚
â”‚                                                                         â”‚
â”‚   4. Check for clock skew                                               â”‚
â”‚      $ ntpstat or chronyc tracking                                      â”‚
â”‚      Offset should be < 100ms                                           â”‚
â”‚                                                                         â”‚
â”‚   RESOLUTION:                                                           â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                           â”‚
â”‚   1. If network issues: Fix network, consider increasing heartbeat      â”‚
â”‚      interval temporarily                                               â”‚
â”‚                                                                         â”‚
â”‚   2. If CPU throttling: Increase CPU limits or move to dedicated host   â”‚
â”‚                                                                         â”‚
â”‚   3. If disk latency: Move to faster storage (NVMe)                     â”‚
â”‚                                                                         â”‚
â”‚   4. If clock skew: Restart NTP, check for VM time drift                â”‚
â”‚                                                                         â”‚
â”‚   5. Temporary mitigation: Increase election timeout                    â”‚
â”‚      etcd: --election-timeout=5000 (5 seconds)                          â”‚
â”‚      ZK: tickTime * initLimit                                           â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Runbook 2: etcd Database Size Growing

```
ETCD DATABASE SIZE RUNBOOK
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DIAGNOSE:
  etcdctl endpoint status --write-out=table    // check size
  etcdctl get '' --prefix --keys-only | ...    // count keys by prefix

COMPACT + DEFRAG (one node at a time!):
  REVISION=$(etcdctl endpoint status ... | jq '.revision')
  etcdctl compact $((REVISION - 10000))        // keep last 10K revisions
  etcdctl defrag --endpoints=$endpoint         // âš ï¸ brief unavailability

AUTO-COMPACTION (etcd config):
  auto-compaction-mode: periodic
  auto-compaction-retention: "1h"
```

#### Runbook 3: ZooKeeper Session Expiration Storm

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                RUNBOOK: ZOOKEEPER SESSION EXPIRATION STORM              â”‚
â”‚                                                                         â”‚
â”‚   SYMPTOMS:                                                             â”‚
â”‚   - Mass client disconnections                                          â”‚
â”‚   - "Session expired" errors across many services                       â”‚
â”‚   - Ephemeral nodes disappearing unexpectedly                           â”‚
â”‚   - Leader election churn in dependent services                         â”‚
â”‚                                                                         â”‚
â”‚   LIKELY CAUSES:                                                        â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                       â”‚
â”‚   1. ZooKeeper overloaded (long GC pauses)                              â”‚
â”‚   2. Network partition between clients and ZK                           â”‚
â”‚   3. Session timeout too aggressive                                     â”‚
â”‚   4. Too many watches or ephemeral nodes                                â”‚
â”‚                                                                         â”‚
â”‚   DIAGNOSIS:                                                            â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                            â”‚
â”‚   1. Check ZK server logs for GC pauses:                                â”‚
â”‚      grep "long gc" /var/log/zookeeper/zookeeper.log                    â”‚
â”‚                                                                         â”‚
â”‚   2. Check outstanding requests:                                        â”‚
â”‚      echo "stat" | nc localhost 2181 | grep Outstanding                 â”‚
â”‚                                                                         â”‚
â”‚   3. Check watch count:                                                 â”‚
â”‚      echo "wchs" | nc localhost 2181                                    â”‚
â”‚                                                                         â”‚
â”‚   4. Check ephemeral node count:                                        â”‚
â”‚      echo "stat" | nc localhost 2181 | grep Ephemeral                   â”‚
â”‚                                                                         â”‚
â”‚   RESOLUTION:                                                           â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                           â”‚
â”‚   1. If GC pauses:                                                      â”‚
â”‚      - Increase heap size                                               â”‚
â”‚      - Tune GC settings (use G1GC)                                      â”‚
â”‚      - Reduce znode data size                                           â”‚
â”‚                                                                         â”‚
â”‚   2. If too many watches:                                               â”‚
â”‚      - Clients should use single watch per path                         â”‚
â”‚      - Consider moving to etcd (streaming watches)                      â”‚
â”‚                                                                         â”‚
â”‚   3. If session timeout too aggressive:                                 â”‚
â”‚      - Increase client session timeout                                  â”‚
â”‚      - Default 30s is often too short for production                    â”‚
â”‚                                                                         â”‚
â”‚   4. Emergency: Rolling restart of ZK ensemble                          â”‚
â”‚      - Restart followers first, leader last                             â”‚
â”‚      - Wait for full sync between restarts                              â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 16.3 Disaster Recovery for Coordination Services

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ETCD BACKUP PROCEDURE                               â”‚
â”‚                     (Run every 1-6 hours)                               â”‚
â”‚                                                                         â”‚
â”‚   1. CREATE SNAPSHOT:                                                   â”‚
â”‚      etcdctl snapshot save /var/backups/etcd/snapshot_$(date).db        â”‚
â”‚                                                                         â”‚
â”‚   2. VERIFY SNAPSHOT:                                                   â”‚
â”‚      etcdctl snapshot status <snapshot_file>                            â”‚
â”‚                                                                         â”‚
â”‚   3. UPLOAD TO REMOTE STORAGE:                                          â”‚
â”‚      aws s3 cp <snapshot_file> s3://my-backups/etcd/                    â”‚
â”‚                                                                         â”‚
â”‚   4. CLEANUP:                                                           â”‚
â”‚      Keep last 24 local backups, delete older ones                      â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ETCD RESTORE FROM BACKUP                               â”‚
â”‚         âš ï¸  WARNING: Creates new cluster. Old data is lost!             â”‚
â”‚                                                                         â”‚
â”‚   STEP 1: Stop all etcd members                                         â”‚
â”‚           systemctl stop etcd (on each host)                            â”‚
â”‚                                                                         â”‚
â”‚   STEP 2: Download backup from remote storage                           â”‚
â”‚           aws s3 cp s3://my-backups/etcd/snapshot.db /tmp/              â”‚
â”‚                                                                         â”‚
â”‚   STEP 3: On each member, restore with new cluster config               â”‚
â”‚           rm -rf /var/lib/etcd/*                                        â”‚
â”‚           etcdctl snapshot restore /tmp/snapshot.db \                   â”‚
â”‚               --name=$HOSTNAME \                                        â”‚
â”‚               --data-dir=/var/lib/etcd \                                â”‚
â”‚               --initial-cluster=$NEW_CLUSTER_CONFIG                     â”‚
â”‚                                                                         â”‚
â”‚   STEP 4: Start all members                                             â”‚
â”‚           systemctl start etcd (on each host)                           â”‚
â”‚                                                                         â”‚
â”‚   STEP 5: Verify cluster health                                         â”‚
â”‚           etcdctl endpoint health --cluster                             â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     QUORUM LOSS RECOVERY                                â”‚
â”‚              âš ï¸  LAST RESORT - May result in data loss!                 â”‚
â”‚                                                                         â”‚
â”‚   OPTION 1: Force new cluster from surviving member                     â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚
â”‚   1. Stop all etcd processes                                            â”‚
â”‚   2. On surviving member:                                               â”‚
â”‚      etcd --force-new-cluster --data-dir=/var/lib/etcd                  â”‚
â”‚   3. This creates single-node cluster with existing data                â”‚
â”‚   4. Add new members normally                                           â”‚
â”‚                                                                         â”‚
â”‚   OPTION 2: Restore from backup                                         â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                         â”‚
â”‚   1. Follow restore procedure above                                     â”‚
â”‚   2. Accept that data since last backup is lost                         â”‚
â”‚                                                                         â”‚
â”‚   PREVENTION:                                                           â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                           â”‚
â”‚   â€¢ Use 5 nodes instead of 3 for critical services                      â”‚
â”‚   â€¢ Spread across failure domains (racks, AZs)                          â”‚
â”‚   â€¢ Regular backup testing                                              â”‚
â”‚   â€¢ Monitoring for member health                                        â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 16.4 Performance Tuning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              COORDINATION SERVICE PERFORMANCE TUNING                    â”‚
â”‚                                                                         â”‚
â”‚   ETCD TUNING:                                                          â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                          â”‚
â”‚                                                                         â”‚
â”‚   # Key performance settings                                            â”‚
â”‚   heartbeat-interval: 100ms    # Default, increase for WAN              â”‚
â”‚   election-timeout: 1000ms     # Must be > 5x heartbeat                 â”‚
â”‚   snapshot-count: 100000       # Increase if write-heavy                â”‚
â”‚   max-wals: 5                  # WAL file retention                     â”‚
â”‚                                                                         â”‚
â”‚   # Client-side tuning                                                  â”‚
â”‚   - Use connection pooling                                              â”‚
â”‚   - Batch reads with txn                                                â”‚
â”‚   - Use lease for multiple ephemeral keys                               â”‚
â”‚   - Avoid hot keys (shard if needed)                                    â”‚
â”‚                                                                         â”‚
â”‚   ZOOKEEPER TUNING:                                                     â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                     â”‚
â”‚                                                                         â”‚
â”‚   # zoo.cfg key settings                                                â”‚
â”‚   tickTime=2000              # Base time unit (ms)                      â”‚
â”‚   initLimit=10               # Ticks to initial sync                    â”‚
â”‚   syncLimit=5                # Ticks for sync                           â”‚
â”‚   maxClientCnxns=60          # Per-IP connection limit                  â”‚
â”‚   autopurge.snapRetainCount=3                                           â”‚
â”‚   autopurge.purgeInterval=1                                             â”‚
â”‚                                                                         â”‚
â”‚   # JVM settings for ZK                                                 â”‚
â”‚   -Xms4g -Xmx4g              # Fixed heap size                          â”‚
â”‚   -XX:+UseG1GC               # G1 for lower pause times                 â”‚
â”‚   -XX:MaxGCPauseMillis=50    # Target GC pause                          â”‚
â”‚                                                                         â”‚
â”‚   DISTRIBUTED LOCK TUNING:                                              â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚                                                                         â”‚
â”‚   - TTL: Balance between                                                â”‚
â”‚     - Too short: False lock expiration during GC                        â”‚
â”‚     - Too long: Slow recovery from crashes                              â”‚
â”‚     - Recommendation: 30-60 seconds for most use cases                  â”‚
â”‚                                                                         â”‚
â”‚   - Renewal interval: TTL / 3                                           â”‚
â”‚     - Renew well before expiry                                          â”‚
â”‚     - Account for network latency                                       â”‚
â”‚                                                                         â”‚
â”‚   - Retry backoff: Exponential with jitter                              â”‚
â”‚     - Prevents thundering herd                                          â”‚
â”‚     - Max backoff: 1-5 seconds                                          â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 16.5 Monitoring Dashboards

**Essential Metrics to Monitor:**

| Service | Metric | What It Tells You |
|---------|--------|-------------------|
| **etcd** | `etcd_server_has_leader` | 1 = healthy, 0 = no leader |
| | `etcd_server_leader_changes_seen_total` | Election frequency |
| | `etcd_disk_wal_fsync_duration_seconds` | Write latency |
| | `etcd_network_peer_round_trip_time_seconds` | Cluster communication |
| | `etcd_mvcc_db_total_size_in_bytes` | Database size |
| | `etcd_server_proposals_failed_total` | Consensus failures |
| **ZooKeeper** | `zk_outstanding_requests` | Queued requests |
| | `zk_avg_latency` | Average request latency |
| | `zk_num_alive_connections` | Active clients |
| | `zk_ephemerals_count` | Ephemeral nodes |
| | `zk_watch_count` | Active watches |
| | `jvm_gc_pause_seconds` | GC pause duration |

**Critical Alerts:**

| Alert | Condition | Severity | Action |
|-------|-----------|----------|--------|
| **NoLeader** | `has_leader == 0` for 30s | ğŸ”´ Critical | Cluster can't accept writes. Check node health. |
| **HighLatency** | `wal_fsync_p99 > 100ms` | ğŸŸ¡ Warning | Disk latency high. Check for noisy neighbors. |
| **DatabaseFull** | `db_size > 6GB` | ğŸ”´ Critical | etcd limit is 8GB. Compact and defrag now. |
| **FrequentElections** | `elections > 0.1/min` | ğŸŸ¡ Warning | Check network stability between nodes. |
| **SessionExpiration** | `expirations > 1/min` | ğŸŸ¡ Warning | Clients losing sessions. Check ZK load. |

**Essential Dashboard Panels:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   COORDINATION DASHBOARD LAYOUT                         â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚   â”‚ Leader Statusâ”‚  â”‚ Elections/hr â”‚  â”‚ Error Rate   â”‚                  â”‚
â”‚   â”‚   [STAT]     â”‚  â”‚   [GRAPH]    â”‚  â”‚   [GRAPH]    â”‚                  â”‚
â”‚   â”‚    âœ“ / âœ—     â”‚  â”‚   ~~~~~~~~   â”‚  â”‚   ~~~~~~~~   â”‚                  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚   â”‚ Latency p99  â”‚  â”‚ Database Sizeâ”‚  â”‚ Connections  â”‚                  â”‚
â”‚   â”‚   [GRAPH]    â”‚  â”‚   [GAUGE]    â”‚  â”‚   [STAT]     â”‚                  â”‚
â”‚   â”‚   ~~~~~~~~   â”‚  â”‚   [####--]   â”‚  â”‚    1,234     â”‚                  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Self-Check: Did I Cover Coordination Properly?

| Signal | Weak | Strong | âœ“ |
|--------|------|--------|---|
| **Need for coordination** | Assumed it's needed | "First, can we avoid coordination entirely?" | â˜ |
| **Leader election** | "We need a leader" | "Leader with lease-based election, 30s failover, degraded mode when no leader" | â˜ |
| **Distributed locks** | "Lock before write" | "Lock with TTL, fencing tokens, what if lock service fails?" | â˜ |
| **Failure handling** | Not addressed | "If ZooKeeper down, we use local counts and sync later" | â˜ |
| **Clock assumptions** | "Use timestamps" | "Can't trust clocks; use logical clocks or fencing tokens" | â˜ |
| **Trade-offs** | Correctness always | "Accepting approximate limits for availability" | â˜ |

---

## Common Interview Questions & Staff-Level Answers

| Question | Senior Answer | Staff Answer |
|----------|--------------|--------------|
| **"How would you prevent duplicate job execution?"** | "Use a distributed lock" | "First, can we make jobs idempotent? If not, use leader election for the scheduler with a claims table for at-most-once semantics" |
| **"How do you handle the rate limiter's Redis going down?"** | "Retry connection" | "Fail open with local limits. Accept over-limit requests temporarily. Log for analysis. Alert on-call." |
| **"What happens if two nodes both think they're leader?"** | "Shouldn't happen" | "Split-brain is possible. Use fencing tokens. New leader's token > old leader's. Resources reject stale tokens." |
| **"Why not just use timestamps?"** | "That works" | "Clock skew can be 100ms+. Use logical clocks for ordering. Never use timestamps alone for coordination." |

---

## Quick Reference Card

### When to Use Each Coordination Pattern

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COORDINATION PATTERN DECISION TREE                   â”‚
â”‚                                                                         â”‚
â”‚   Need mutual exclusion?                                                â”‚
â”‚   â”‚                                                                     â”‚
â”‚   â”œâ”€â”€ YES: Can you use atomic operations?                               â”‚
â”‚   â”‚   â”œâ”€â”€ YES â†’ Use atomic ops (INCR, CAS)                              â”‚
â”‚   â”‚   â””â”€â”€ NO: Is contention low?                                        â”‚
â”‚   â”‚       â”œâ”€â”€ YES â†’ Use optimistic concurrency                          â”‚
â”‚   â”‚       â””â”€â”€ NO: Is the critical section short?                        â”‚
â”‚   â”‚           â”œâ”€â”€ YES â†’ Use distributed lock                            â”‚
â”‚   â”‚           â””â”€â”€ NO â†’ Redesign to minimize lock scope                  â”‚
â”‚   â”‚                                                                     â”‚
â”‚   â””â”€â”€ NO: Need a single coordinator?                                    â”‚
â”‚       â”‚                                                                 â”‚
â”‚       â”œâ”€â”€ YES â†’ Use leader election                                     â”‚
â”‚       â””â”€â”€ NO: Need agreed-upon value?                                   â”‚
â”‚           â”‚                                                             â”‚
â”‚           â”œâ”€â”€ YES â†’ Use consensus (etcd, ZK)                            â”‚
â”‚           â””â”€â”€ NO â†’ No coordination needed!                              â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Lock Implementation Checklist

```
â–¡ Unique lock identifier (prevent releasing others' locks)
â–¡ TTL to prevent deadlocks
â–¡ Fencing tokens for downstream protection
â–¡ Graceful handling of lock service failure
â–¡ Metrics for acquisition time and contention
â–¡ Timeout for acquisition attempts
â–¡ Safe release (only if still owner)
â–¡ Renewal mechanism for long operations (if needed)
```

### Leader Election Checklist

```
â–¡ Single leader guarantee (quorum-based or lease-based)
â–¡ Fast failover (< 10 seconds typically)
â–¡ Leader step-down on quorum loss
â–¡ Epoch/term numbers to fence old leaders
â–¡ Heartbeat mechanism
â–¡ Client redirection when leader changes
â–¡ Degraded mode when no leader
â–¡ Metrics for election frequency and duration
```

### Failure Response Matrix

| Failure | Rate Limiter | Job Scheduler | Config Service |
|---------|-------------|---------------|----------------|
| Leader down | Local counts | Queue jobs | Use cache |
| Lock service down | Fail open | Pause processing | Stale config |
| Network partition | Per-partition limits | Risk duplicates | Stale reads |
| Clock skew | Inaccurate windows | Lease issues | TTL problems |

### Key Metrics to Monitor

| Metric | Warning Threshold | Critical Threshold |
|--------|-------------------|-------------------|
| Lock acquisition p99 | > 50ms | > 200ms |
| Lock contention rate | > 10% | > 50% |
| Election frequency | > 1/hour | > 1/minute |
| Election duration | > 5s | > 30s |
| Coordination errors | > 0.1% | > 1% |
| Fencing token rejections | > 0 | > 0.01% |

---

## Further Reading

1. **"How to do distributed locking"** - Martin Kleppmann
   - The famous Redlock analysis
   - https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html

2. **"Distributed Systems for Fun and Profit"** - Mikito Takada
   - Free online book covering consensus fundamentals
   - http://book.mixu.net/distsys/

3. **"The Raft Consensus Algorithm"** - Diego Ongaro
   - Understandable consensus
   - https://raft.github.io/

4. **"Designing Data-Intensive Applications"** - Martin Kleppmann
   - Chapters 8 (Distributed Systems) and 9 (Consistency & Consensus)

5. **"Time, Clocks, and the Ordering of Events"** - Leslie Lamport
   - The foundational paper on logical clocks
   - https://lamport.azurewebsites.net/pubs/time-clocks.pdf

---

*"The first rule of distributed systems: Don't distribute. The second rule: If you must distribute, don't coordinate. The third rule: If you must coordinate, make it as rare as possible."*

---

# Part 17: Interview Calibration for Coordination Topics

## What Interviewers Are Evaluating

When a candidate discusses coordination in system design, interviewers assess:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INTERVIEWER'S MENTAL RUBRIC                              â”‚
â”‚                                                                             â”‚
â”‚   QUESTION IN INTERVIEWER'S MIND          L5 SIGNAL           L6 SIGNAL     â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                             â”‚
â”‚   "Did they question whether                                                â”‚
â”‚    coordination is needed?"             Assumed needed    "Can we avoid it?"â”‚
â”‚                                                                             â”‚
â”‚   "Do they understand the costs?"       Lists benefits    Discusses costs   â”‚
â”‚                                                           AND benefits      â”‚
â”‚                                                                             â”‚
â”‚   "Do they know failure modes?"         "It should work"  Split-brain,      â”‚
â”‚                                                           election storms   â”‚
â”‚                                                                             â”‚
â”‚   "Can they size timeouts?"             Uses defaults     Calculates based  â”‚
â”‚                                                           on latency/skew   â”‚
â”‚                                                                             â”‚
â”‚   "Do they consider operations?"        Not mentioned     Backup, restore,  â”‚
â”‚                                                           runbooks          â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## L5 vs L6 Interview Phrases

| Topic | L5 Answer (Competent) | L6 Answer (Staff-Level) |
|-------|----------------------|------------------------|
| **Need for coordination** | "We'll use a distributed lock" | "First, can we avoid coordination? Can we partition the work or use idempotency instead?" |
| **Leader election** | "We'll use ZooKeeper for leader election" | "Leader election with 30s lease, stepping down on quorum loss, and degraded mode when no leader available" |
| **Lock implementation** | "Use Redis SETNX with TTL" | "SETNX with TTL, fencing tokens passed to downstream, and fallback behavior when Redis is unavailable" |
| **Split-brain** | "We prevent it with proper design" | "Split-brain is possible. We use epoch numbers; resources reject stale epochs. If it happens, we have reconciliation procedures." |
| **Failure detection** | "Heartbeat timeout" | "Phi accrual detector that adapts to network conditions. 30s timeout balances false positives against detection speed." |
| **Clock assumptions** | "We use timestamps" | "Clocks can drift 100ms+. We use logical clocks for ordering and never rely on timestamps alone for coordination decisions." |
| **Degradation** | Not discussed | "When coordination is unavailable, we fail closed for writes but allow cached reads for 5 minutes." |

## Common L5 Mistakes That Cost the Level

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    L5 MISTAKES IN COORDINATION DISCUSSIONS                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   MISTAKE 1: "We'll use Kafka for coordination"                             â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                             â”‚
â”‚   Kafka is a log, not a coordination service. Using it for leader           â”‚
â”‚   election or distributed locks requires building consensus on top,         â”‚
â”‚   which is complex and error-prone.                                         â”‚
â”‚                                                                             â”‚
â”‚   L6 CORRECTION: "Kafka is for event streaming. For coordination, I'd       â”‚
â”‚   use etcd or ZooKeeper which provide consensus primitives."                â”‚
â”‚                                                                             â”‚
â”‚   MISTAKE 2: "The lock prevents duplicate processing"                       â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚
â”‚   Locks expire. GC pauses can cause a process to continue after losing      â”‚
â”‚   its lock. Without fencing tokens, duplicates are still possible.          â”‚
â”‚                                                                             â”‚
â”‚   L6 CORRECTION: "The lock provides mutual exclusion, but we also           â”‚
â”‚   need fencing tokens. The downstream resource checks the token and         â”‚
â”‚   rejects stale holders."                                                   â”‚
â”‚                                                                             â”‚
â”‚   MISTAKE 3: "We'll set timeout to 5 seconds"                               â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                              â”‚
â”‚   No justification. Timeouts should be calculated based on network          â”‚
â”‚   latency, clock skew, and acceptable detection delay.                      â”‚
â”‚                                                                             â”‚
â”‚   L6 CORRECTION: "Given cross-AZ latency of 2ms P99 and 50ms clock          â”‚
â”‚   skew worst case, I'd set heartbeat at 500ms, timeout at 2 seconds,        â”‚
â”‚   and lease TTL at 10 seconds to account for GC pauses."                    â”‚
â”‚                                                                             â”‚
â”‚   MISTAKE 4: "We use 3-node cluster for high availability"                  â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚
â”‚   3 nodes survives 1 failure. But what if you need to do rolling            â”‚
â”‚   updates? Or if 2 nodes are in the same failure domain?                    â”‚
â”‚                                                                             â”‚
â”‚   L6 CORRECTION: "3 nodes survives 1 failure. For a critical service,       â”‚
â”‚   I'd use 5 nodes across 3 AZs. This allows 2 failures and enables          â”‚
â”‚   rolling updates without risking quorum."                                  â”‚
â”‚                                                                             â”‚
â”‚   MISTAKE 5: Not mentioning what happens when coordination fails            â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚
â”‚   This is the Staff-level differentiator. L5s design for the happy          â”‚
â”‚   path. L6s design for failure.                                             â”‚
â”‚                                                                             â”‚
â”‚   L6 CORRECTION: "When etcd is unavailable, the job scheduler buffers       â”‚
â”‚   jobs locally and stops leader election. Jobs continue processing at       â”‚
â”‚   reduced capacity until coordination recovers."                            â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Example Interview Exchange

```
INTERVIEWER: "How would you coordinate job scheduling across multiple workers?"

L5 ANSWER:
"I'd use a distributed lock. Before processing a job, the worker acquires 
a lock on the job ID. This prevents duplicates. I'd use Redis for the lock 
service."

L6 ANSWER:
"Let me first check if we need coordination at all. 

If jobs can be partitioned by ID, each worker handles a specific partition 
and we avoid coordination entirely. That's the best approach.

If we can't partition, I'd use leader election rather than per-job locks. 
The leader assigns jobs to workers. Benefits: one coordination point, not 
one per job. I'd implement this with etcd leases.

For failure handling:
- Leader dies: 10-30 second election window. Workers buffer jobs locally.
- Worker dies mid-job: Heartbeat-based detection. Leader reassigns after 
  timeout. Job must be idempotent or we use fencing tokens.
- etcd cluster down: Workers continue processing assigned jobs. No new 
  assignments until etcd recovers. We accept reduced throughput.

I'd also add:
- Retry budget: max 10% of requests as retries to prevent storms
- Circuit breaker on etcd calls: fail fast if etcd is struggling
- Metrics on election frequency, lock acquisition latency, queue depth"
```

## Staff-Level Reasoning Visibility

When discussing coordination, make your reasoning visible:

```
"I'm choosing leader election over per-job locks because..."
   â””â”€â”€â”€ Shows you considered alternatives

"The timeout needs to be longer than network P99 plus clock skew..."
   â””â”€â”€â”€ Shows you understand the mathematics

"When the lock service fails, we..."
   â””â”€â”€â”€ Shows you plan for failure

"The trade-off is availability for consistency during the election window..."
   â””â”€â”€â”€ Shows you understand trade-offs
```

---

# Part 18: Final Verification

## Does This Section Meet L6 Expectations?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    L6 COVERAGE CHECKLIST                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   JUDGMENT & DECISION-MAKING                                                â”‚
â”‚   â˜‘ When to use coordination vs. alternatives (idempotency, partitioning)   â”‚
â”‚   â˜‘ Choosing between leader election, locks, and consensus                  â”‚
â”‚   â˜‘ Timeout and TTL sizing with justification                               â”‚
â”‚   â˜‘ Trade-off between availability and consistency during failures          â”‚
â”‚                                                                             â”‚
â”‚   FAILURE & DEGRADATION THINKING                                            â”‚
â”‚   â˜‘ Split-brain prevention and detection                                    â”‚
â”‚   â˜‘ Election storms: causes and mitigations                                 â”‚
â”‚   â˜‘ Graceful degradation when coordination unavailable                      â”‚
â”‚   â˜‘ Fencing tokens for stale lock holders                                   â”‚
â”‚   â˜‘ Clock skew handling                                                     â”‚
â”‚                                                                             â”‚
â”‚   SCALE & EVOLUTION                                                         â”‚
â”‚   â˜‘ Multi-region coordination patterns                                      â”‚
â”‚   â˜‘ Scaling beyond leader bottleneck                                        â”‚
â”‚   â˜‘ Migration strategies (Redis to etcd, etc.)                              â”‚
â”‚                                                                             â”‚
â”‚   STAFF-LEVEL SIGNALS                                                       â”‚
â”‚   â˜‘ Questions coordination necessity first                                  â”‚
â”‚   â˜‘ Understands operational costs (runbooks, backup, restore)               â”‚
â”‚   â˜‘ Makes reasoning visible                                                 â”‚
â”‚   â˜‘ Acknowledges uncertainty and trade-offs                                 â”‚
â”‚                                                                             â”‚
â”‚   REAL-WORLD APPLICATION                                                    â”‚
â”‚   â˜‘ Job scheduler case study                                                â”‚
â”‚   â˜‘ Rate limiter coordination                                               â”‚
â”‚   â˜‘ Metadata service architecture                                           â”‚
â”‚                                                                             â”‚
â”‚   INTERVIEW CALIBRATION                                                     â”‚
â”‚   â˜‘ L5 vs L6 phrase comparisons                                             â”‚
â”‚   â˜‘ Common mistakes that cost the level                                     â”‚
â”‚   â˜‘ Interviewer evaluation criteria                                         â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Remaining Gaps (Acceptable for Scope)

| Gap | Reason Acceptable |
|-----|-------------------|
| Byzantine fault tolerance | Rarely needed in practice; covered conceptually |
| Paxos algorithm details | Raft is preferred; Paxos covered at intuition level |
| Vendor-specific tuning | General principles apply; ops teams handle specifics |

## Self-Check Questions Before Interview

Use these to verify your understanding:

```
â–¡ Can I explain why coordination should be avoided when possible?
â–¡ Can I differentiate leader election, locks, and consensus use cases?
â–¡ Can I design a system that degrades gracefully when coordination fails?
â–¡ Can I size timeouts based on network and clock characteristics?
â–¡ Can I explain fencing tokens and why locks alone aren't sufficient?
â–¡ Can I discuss multi-region coordination trade-offs?
â–¡ Can I identify the failure modes of my coordination design?
```

---

*"The best coordination is no coordination. The second best is coordination that fails gracefully."*
---

# Brainstorming Questions

## Understanding Coordination

1. Think of a system that uses distributed coordination. Could it be redesigned to avoid coordination? What would be the trade-offs?

2. When have you seen leader election cause problems? What was the failure mode?

3. How do you size timeouts for distributed locks? What's your mental model?

4. What's the difference between a lock and a lease? When would you choose one over the other?

5. How do you explain fencing tokens to someone who hasn't heard of them?

## Failure Modes

6. You have a distributed lock using Redis. What happens if Redis restarts during lock hold?

7. Design a system where leader election failure causes minimal impact. What patterns do you use?

8. How do you detect and recover from split-brain in a leader-based system?

9. What's an election storm? How do you prevent it?

10. Your lock service is experiencing high latency. What are the implications for lock holders?

## Applied Scenarios

11. Design leader election for a job scheduler. What's your availability vs. consistency trade-off?

12. You need a rate limiter across 100 servers. Do you need coordination? What are the alternatives?

13. How would you implement a lease-based cache invalidation system?

14. Design a metadata service where consistency is critical but availability is also important.

15. What's your go-to technology for coordination? When would you choose something different?

---

# Reflection Prompts

Set aside 15-20 minutes for each of these reflection exercises.

## Reflection 1: Your Coordination Instincts

Think about how you approach problems that seem to need coordination.

- Do you reach for locks and leader election by default?
- When was the last time you avoided coordination by redesigning the problem?
- Can you list three alternatives to distributed locks for a given problem?
- Do you consider the operational cost of coordination infrastructure?

For a system you've built that uses coordination, redesign it to minimize coordination.

## Reflection 2: Your Failure Mode Coverage

Consider how you think about coordination failures.

- Do you design for the case where the lock service itself fails?
- Have you ever debugged a fencing token issue?
- Can you explain what happens during leader election to a non-expert?
- Do you test coordination failure scenarios in your systems?

Write a failure mode analysis for coordination in a system you know well.

## Reflection 3: Your Technology Choices

Examine how you choose coordination technologies.

- Why do you choose one coordination technology over another?
- Do you understand the consistency guarantees of your chosen tools?
- Have you ever migrated between coordination technologies? What triggered it?
- Can you explain the trade-offs of ZooKeeper vs. etcd vs. Redis for locks?

Research a coordination technology you haven't used and compare it to your default choice.

---

# Homework Exercises

## Exercise 1: Coordination Avoidance

Take these problems that seem to require coordination. For each, design a solution that avoids centralized coordination:

1. **Sequential ID generation** across 10 services
2. **Rate limiting** across 50 servers
3. **Cache invalidation** across multiple regions
4. **Task assignment** to workers without double-processing
5. **Configuration updates** that must be atomic across services

For each:
- Describe the no-coordination approach
- What's the trade-off compared to coordinated approach?
- When would coordination still be necessary?

## Exercise 2: Leader Election Design

Design a leader election system for:

**Scenario: Multi-region job scheduler**
- 3 regions, one active leader needed
- Jobs must not be duplicated or lost
- Switchover time < 30 seconds

Include:
- Technology choice with justification
- Timeout values with reasoning
- Fencing mechanism
- Fallback behavior during election
- Monitoring and alerting

## Exercise 3: Failure Scenario Runbooks

Create runbooks for these coordination failure scenarios:

1. **Lock service completely unavailable**
   - Detection, immediate response, recovery

2. **Leader election taking > 5 minutes**
   - Investigation steps, manual intervention options

3. **Split-brain detected** (two leaders active)
   - Immediate actions, damage assessment, resolution

4. **Lock holder crashed without releasing**
   - Detection, automatic vs. manual resolution

5. **Clock skew causing lock issues**
   - Detection, mitigation, prevention

## Exercise 4: Technology Comparison

Compare these coordination approaches for a distributed cache invalidation system:

1. **Redis-based**: SETNX for locks
2. **ZooKeeper/etcd**: Proper consensus-based locks
3. **Kafka-based**: Event-driven invalidation
4. **No coordination**: Version-based invalidation

Create a comparison matrix with:
- Consistency guarantees
- Latency characteristics
- Failure modes
- Operational complexity
- Scalability limits

## Exercise 5: Interview Practice

Practice explaining these concepts (3 minutes each):

1. "Why shouldn't you use distributed locks in most cases?"
2. "Explain fencing tokens and why they're necessary"
3. "How does leader election work and what are its failure modes?"
4. "When would you choose ZooKeeper vs. Redis for coordination?"
5. "Design a job scheduler that's resilient to coordination failures"

Record yourself and evaluate for clarity and trade-off acknowledgment.

---

# Conclusion

Coordination is one of the hardest problems in distributed systems. The key insights from this section:

1. **Avoid coordination when possible.** Redesign problems to use idempotency, partitioning, or CRDTs instead.

2. **When coordination is needed, understand the failure modes.** Leader election can stall, locks can deadlock, consensus can partition.

3. **Timeouts and TTLs require careful tuning.** Too short causes false positives; too long causes availability issues.

4. **Fencing tokens are essential for correctness.** Locks alone are not sufficient in distributed systems.

5. **Graceful degradation matters.** What happens when coordination is unavailable? Design for this.

6. **Operational complexity is high.** Coordination infrastructure (ZooKeeper, etcd) requires expertise to run well.

In interviews, demonstrate that you think about coordination critically. Don't reach for it by defaultâ€”question whether it's necessary. When it is, address failure modes proactively. That's Staff-level thinking.

---
